{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/storage/MLND_Capstone_Project_2019/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1227 images belonging to 7 classes.\n",
      "Found 335 images belonging to 7 classes.\n",
      "Found 331 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Image Data Generator w/ no augmentation\n",
    "#Scaling for pixels\n",
    "piece_train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "piece_test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "piece_valid_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "\n",
    "\n",
    "#Flow data from directory\n",
    "\n",
    "piece_train_iter = piece_train_datagen.flow_from_directory(\n",
    "    directory = '../data/piece_data/train',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "piece_test_iter = piece_test_datagen.flow_from_directory(\n",
    "    directory = '../data/piece_data/test',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "piece_valid_iter = piece_valid_datagen.flow_from_directory(\n",
    "    directory = '../data/piece_data/valid',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 135, 135, 16)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 67, 67, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 67, 67, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 67, 67, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 33, 33, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_1 (Ba (None, 33, 33, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 33, 33, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_2 (Ba (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       32896     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_3 (Ba (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_4 (Ba (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 64)          16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_5 (Ba (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              133120    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 14343     \n",
      "=================================================================\n",
      "Total params: 4,506,439\n",
      "Trainable params: 4,505,703\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define NN architecture\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "piece_model = Sequential()\n",
    "piece_model.add(Conv2D(filters=16, kernel_size=5, padding='same', activation='relu', \n",
    "                        input_shape=(135, 135, 1)))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Dropout(0.3))\n",
    "piece_model.add(GlobalAveragePooling2D())\n",
    "piece_model.add(Dense(2048, activation='relu'))\n",
    "piece_model.add(Dense(2048, activation='relu'))\n",
    "piece_model.add(Dropout(0.4))\n",
    "piece_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "piece_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "piece_model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.00001), \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "STEP_SIZE_TRAIN = piece_train_iter.n/piece_train_iter.batch_size\n",
    "STEP_SIZE_VALID = piece_valid_iter.n/piece_valid_iter.batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.06090, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.8741 - accuracy: 0.2567 - val_loss: 2.0609 - val_accuracy: 0.1511\n",
      "Epoch 2/1500\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.06090\n",
      "39/38 - 9s - loss: 1.7854 - accuracy: 0.3170 - val_loss: 2.0750 - val_accuracy: 0.1511\n",
      "Epoch 3/1500\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.7355 - accuracy: 0.3146 - val_loss: 2.1458 - val_accuracy: 0.1511\n",
      "Epoch 4/1500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.6775 - accuracy: 0.3521 - val_loss: 2.3064 - val_accuracy: 0.1511\n",
      "Epoch 5/1500\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.6371 - accuracy: 0.3643 - val_loss: 2.5697 - val_accuracy: 0.1511\n",
      "Epoch 6/1500\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.5866 - accuracy: 0.4010 - val_loss: 2.8977 - val_accuracy: 0.1511\n",
      "Epoch 7/1500\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.5626 - accuracy: 0.4108 - val_loss: 3.2088 - val_accuracy: 0.1511\n",
      "Epoch 8/1500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.5342 - accuracy: 0.4165 - val_loss: 3.4515 - val_accuracy: 0.1511\n",
      "Epoch 9/1500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.5041 - accuracy: 0.4295 - val_loss: 3.4871 - val_accuracy: 0.1511\n",
      "Epoch 10/1500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.4772 - accuracy: 0.4556 - val_loss: 3.3740 - val_accuracy: 0.1511\n",
      "Epoch 11/1500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.4520 - accuracy: 0.4605 - val_loss: 3.0584 - val_accuracy: 0.1511\n",
      "Epoch 12/1500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.4405 - accuracy: 0.4662 - val_loss: 2.7683 - val_accuracy: 0.1571\n",
      "Epoch 13/1500\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.4147 - accuracy: 0.4580 - val_loss: 2.5565 - val_accuracy: 0.1752\n",
      "Epoch 14/1500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.3823 - accuracy: 0.4939 - val_loss: 2.3039 - val_accuracy: 0.2054\n",
      "Epoch 15/1500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.06090\n",
      "39/38 - 8s - loss: 1.3548 - accuracy: 0.4996 - val_loss: 2.1173 - val_accuracy: 0.2266\n",
      "Epoch 16/1500\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.06090 to 1.98423, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 1.3503 - accuracy: 0.5134 - val_loss: 1.9842 - val_accuracy: 0.2508\n",
      "Epoch 17/1500\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.98423 to 1.85646, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 1.3254 - accuracy: 0.5175 - val_loss: 1.8565 - val_accuracy: 0.2840\n",
      "Epoch 18/1500\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.85646 to 1.79829, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.3142 - accuracy: 0.5224 - val_loss: 1.7983 - val_accuracy: 0.3021\n",
      "Epoch 19/1500\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.79829 to 1.72141, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.2859 - accuracy: 0.5379 - val_loss: 1.7214 - val_accuracy: 0.3172\n",
      "Epoch 20/1500\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.72141 to 1.67144, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.2833 - accuracy: 0.5412 - val_loss: 1.6714 - val_accuracy: 0.3323\n",
      "Epoch 21/1500\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.67144 to 1.63462, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 1.2576 - accuracy: 0.5526 - val_loss: 1.6346 - val_accuracy: 0.3414\n",
      "Epoch 22/1500\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.63462 to 1.60949, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 1.2371 - accuracy: 0.5632 - val_loss: 1.6095 - val_accuracy: 0.3746\n",
      "Epoch 23/1500\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.60949 to 1.59448, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.2368 - accuracy: 0.5403 - val_loss: 1.5945 - val_accuracy: 0.3535\n",
      "Epoch 24/1500\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.59448 to 1.57744, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.2021 - accuracy: 0.5607 - val_loss: 1.5774 - val_accuracy: 0.3505\n",
      "Epoch 25/1500\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.57744 to 1.57222, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 1.1965 - accuracy: 0.5713 - val_loss: 1.5722 - val_accuracy: 0.3686\n",
      "Epoch 26/1500\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.57222 to 1.56220, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 1.1777 - accuracy: 0.5721 - val_loss: 1.5622 - val_accuracy: 0.3716\n",
      "Epoch 27/1500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.56220\n",
      "39/38 - 8s - loss: 1.1639 - accuracy: 0.5672 - val_loss: 1.5641 - val_accuracy: 0.3686\n",
      "Epoch 28/1500\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.56220 to 1.54887, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.1638 - accuracy: 0.5558 - val_loss: 1.5489 - val_accuracy: 0.3686\n",
      "Epoch 29/1500\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.54887 to 1.54245, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.1311 - accuracy: 0.6015 - val_loss: 1.5424 - val_accuracy: 0.3897\n",
      "Epoch 30/1500\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.54245 to 1.53536, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.1182 - accuracy: 0.5803 - val_loss: 1.5354 - val_accuracy: 0.3958\n",
      "Epoch 31/1500\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.53536 to 1.52708, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.1056 - accuracy: 0.5958 - val_loss: 1.5271 - val_accuracy: 0.3927\n",
      "Epoch 32/1500\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.52708 to 1.51663, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 1.0846 - accuracy: 0.6137 - val_loss: 1.5166 - val_accuracy: 0.4139\n",
      "Epoch 33/1500\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.51663 to 1.50789, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 1.0730 - accuracy: 0.6023 - val_loss: 1.5079 - val_accuracy: 0.4290\n",
      "Epoch 34/1500\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.50789 to 1.50367, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.0681 - accuracy: 0.6300 - val_loss: 1.5037 - val_accuracy: 0.4139\n",
      "Epoch 35/1500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.50367\n",
      "39/38 - 8s - loss: 1.0367 - accuracy: 0.6438 - val_loss: 1.5079 - val_accuracy: 0.4109\n",
      "Epoch 36/1500\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.50367 to 1.49830, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.0200 - accuracy: 0.6455 - val_loss: 1.4983 - val_accuracy: 0.4139\n",
      "Epoch 37/1500\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.49830 to 1.49554, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 1.0197 - accuracy: 0.6390 - val_loss: 1.4955 - val_accuracy: 0.4079\n",
      "Epoch 38/1500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.49554\n",
      "39/38 - 8s - loss: 1.0018 - accuracy: 0.6479 - val_loss: 1.5028 - val_accuracy: 0.4079\n",
      "Epoch 39/1500\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.49554 to 1.48167, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 0.9767 - accuracy: 0.6406 - val_loss: 1.4817 - val_accuracy: 0.4290\n",
      "Epoch 40/1500\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.48167 to 1.48162, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 0.9668 - accuracy: 0.6659 - val_loss: 1.4816 - val_accuracy: 0.4350\n",
      "Epoch 41/1500\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.48162\n",
      "39/38 - 8s - loss: 0.9535 - accuracy: 0.6789 - val_loss: 1.4827 - val_accuracy: 0.4471\n",
      "Epoch 42/1500\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.48162 to 1.46852, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 0.9364 - accuracy: 0.6716 - val_loss: 1.4685 - val_accuracy: 0.4350\n",
      "Epoch 43/1500\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.46852 to 1.46263, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 0.9295 - accuracy: 0.6781 - val_loss: 1.4626 - val_accuracy: 0.4502\n",
      "Epoch 44/1500\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.46263 to 1.45687, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 0.9119 - accuracy: 0.6862 - val_loss: 1.4569 - val_accuracy: 0.4562\n",
      "Epoch 45/1500\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.45687 to 1.45556, saving model to piece_model_1.weights.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/38 - 9s - loss: 0.9159 - accuracy: 0.6813 - val_loss: 1.4556 - val_accuracy: 0.4441\n",
      "Epoch 46/1500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.45556\n",
      "39/38 - 9s - loss: 0.8950 - accuracy: 0.6813 - val_loss: 1.4576 - val_accuracy: 0.4471\n",
      "Epoch 47/1500\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.45556 to 1.45540, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 0.8606 - accuracy: 0.6870 - val_loss: 1.4554 - val_accuracy: 0.4411\n",
      "Epoch 48/1500\n",
      "\n",
      "Epoch 00048: val_loss improved from 1.45540 to 1.45311, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 0.8632 - accuracy: 0.7001 - val_loss: 1.4531 - val_accuracy: 0.4653\n",
      "Epoch 49/1500\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.45311 to 1.44495, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 0.8323 - accuracy: 0.7156 - val_loss: 1.4449 - val_accuracy: 0.4502\n",
      "Epoch 50/1500\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.44495 to 1.44473, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 0.8321 - accuracy: 0.7148 - val_loss: 1.4447 - val_accuracy: 0.4653\n",
      "Epoch 51/1500\n",
      "\n",
      "Epoch 00051: val_loss improved from 1.44473 to 1.44418, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 0.8059 - accuracy: 0.7196 - val_loss: 1.4442 - val_accuracy: 0.4592\n",
      "Epoch 52/1500\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.44418\n",
      "39/38 - 8s - loss: 0.8098 - accuracy: 0.7123 - val_loss: 1.4507 - val_accuracy: 0.4622\n",
      "Epoch 53/1500\n",
      "\n",
      "Epoch 00053: val_loss improved from 1.44418 to 1.43273, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 0.7782 - accuracy: 0.7319 - val_loss: 1.4327 - val_accuracy: 0.4743\n",
      "Epoch 54/1500\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.43273\n",
      "39/38 - 8s - loss: 0.7783 - accuracy: 0.7335 - val_loss: 1.4367 - val_accuracy: 0.4713\n",
      "Epoch 55/1500\n",
      "\n",
      "Epoch 00055: val_loss improved from 1.43273 to 1.42546, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 9s - loss: 0.7974 - accuracy: 0.7229 - val_loss: 1.4255 - val_accuracy: 0.4743\n",
      "Epoch 56/1500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.42546\n",
      "39/38 - 8s - loss: 0.7613 - accuracy: 0.7400 - val_loss: 1.4399 - val_accuracy: 0.4532\n",
      "Epoch 57/1500\n",
      "\n",
      "Epoch 00057: val_loss improved from 1.42546 to 1.42410, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 0.7769 - accuracy: 0.7392 - val_loss: 1.4241 - val_accuracy: 0.4894\n",
      "Epoch 58/1500\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.42410\n",
      "39/38 - 8s - loss: 0.7384 - accuracy: 0.7400 - val_loss: 1.4264 - val_accuracy: 0.4622\n",
      "Epoch 59/1500\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.42410\n",
      "39/38 - 8s - loss: 0.7117 - accuracy: 0.7653 - val_loss: 1.4291 - val_accuracy: 0.4683\n",
      "Epoch 60/1500\n",
      "\n",
      "Epoch 00060: val_loss improved from 1.42410 to 1.41917, saving model to piece_model_1.weights.best.hdf5\n",
      "39/38 - 8s - loss: 0.7038 - accuracy: 0.7498 - val_loss: 1.4192 - val_accuracy: 0.4743\n",
      "Epoch 61/1500\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6808 - accuracy: 0.7767 - val_loss: 1.4265 - val_accuracy: 0.4653\n",
      "Epoch 62/1500\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6900 - accuracy: 0.7604 - val_loss: 1.4225 - val_accuracy: 0.4773\n",
      "Epoch 63/1500\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6336 - accuracy: 0.7873 - val_loss: 1.4409 - val_accuracy: 0.4683\n",
      "Epoch 64/1500\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6473 - accuracy: 0.7840 - val_loss: 1.4287 - val_accuracy: 0.4743\n",
      "Epoch 65/1500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6510 - accuracy: 0.7840 - val_loss: 1.4413 - val_accuracy: 0.4773\n",
      "Epoch 66/1500\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6298 - accuracy: 0.7824 - val_loss: 1.4279 - val_accuracy: 0.4743\n",
      "Epoch 67/1500\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6450 - accuracy: 0.7865 - val_loss: 1.4364 - val_accuracy: 0.4653\n",
      "Epoch 68/1500\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6367 - accuracy: 0.7840 - val_loss: 1.4323 - val_accuracy: 0.4804\n",
      "Epoch 69/1500\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6014 - accuracy: 0.7922 - val_loss: 1.4279 - val_accuracy: 0.4894\n",
      "Epoch 70/1500\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6233 - accuracy: 0.7832 - val_loss: 1.4298 - val_accuracy: 0.4713\n",
      "Epoch 71/1500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.6088 - accuracy: 0.7963 - val_loss: 1.4409 - val_accuracy: 0.4713\n",
      "Epoch 72/1500\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5780 - accuracy: 0.8077 - val_loss: 1.4305 - val_accuracy: 0.4653\n",
      "Epoch 73/1500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5701 - accuracy: 0.8191 - val_loss: 1.4492 - val_accuracy: 0.4683\n",
      "Epoch 74/1500\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5406 - accuracy: 0.8126 - val_loss: 1.4301 - val_accuracy: 0.4894\n",
      "Epoch 75/1500\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5729 - accuracy: 0.8117 - val_loss: 1.4382 - val_accuracy: 0.4773\n",
      "Epoch 76/1500\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5049 - accuracy: 0.8452 - val_loss: 1.4490 - val_accuracy: 0.4955\n",
      "Epoch 77/1500\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5505 - accuracy: 0.8060 - val_loss: 1.4461 - val_accuracy: 0.4894\n",
      "Epoch 78/1500\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5143 - accuracy: 0.8362 - val_loss: 1.4443 - val_accuracy: 0.4955\n",
      "Epoch 79/1500\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5005 - accuracy: 0.8394 - val_loss: 1.4540 - val_accuracy: 0.4864\n",
      "Epoch 80/1500\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.4910 - accuracy: 0.8346 - val_loss: 1.4689 - val_accuracy: 0.4894\n",
      "Epoch 81/1500\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5310 - accuracy: 0.8231 - val_loss: 1.4671 - val_accuracy: 0.4924\n",
      "Epoch 82/1500\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5138 - accuracy: 0.8321 - val_loss: 1.4698 - val_accuracy: 0.4864\n",
      "Epoch 83/1500\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5239 - accuracy: 0.8207 - val_loss: 1.4519 - val_accuracy: 0.5015\n",
      "Epoch 84/1500\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.5219 - accuracy: 0.8256 - val_loss: 1.4548 - val_accuracy: 0.4894\n",
      "Epoch 85/1500\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.4600 - accuracy: 0.8517 - val_loss: 1.4626 - val_accuracy: 0.4955\n",
      "Epoch 86/1500\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.4526 - accuracy: 0.8541 - val_loss: 1.4666 - val_accuracy: 0.4924\n",
      "Epoch 87/1500\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.4354 - accuracy: 0.8680 - val_loss: 1.4922 - val_accuracy: 0.4894\n",
      "Epoch 88/1500\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.4706 - accuracy: 0.8378 - val_loss: 1.4767 - val_accuracy: 0.4924\n",
      "Epoch 89/1500\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.4410 - accuracy: 0.8533 - val_loss: 1.4751 - val_accuracy: 0.5015\n",
      "Epoch 90/1500\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.41917\n",
      "39/38 - 8s - loss: 0.4547 - accuracy: 0.8525 - val_loss: 1.4818 - val_accuracy: 0.4924\n",
      "Epoch 91/1500\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.41917\n",
      "39/38 - 9s - loss: 0.4209 - accuracy: 0.8590 - val_loss: 1.5015 - val_accuracy: 0.4834\n",
      "Epoch 92/1500\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.41917\n",
      "39/38 - 10s - loss: 0.4215 - accuracy: 0.8720 - val_loss: 1.4896 - val_accuracy: 0.5076\n",
      "Epoch 93/1500\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.4293 - accuracy: 0.8769 - val_loss: 1.5573 - val_accuracy: 0.4864\n",
      "Epoch 94/1500\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.4161 - accuracy: 0.8672 - val_loss: 1.4975 - val_accuracy: 0.5106\n",
      "Epoch 95/1500\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.4175 - accuracy: 0.8655 - val_loss: 1.5148 - val_accuracy: 0.5076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/1500\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3644 - accuracy: 0.8826 - val_loss: 1.5301 - val_accuracy: 0.5015\n",
      "Epoch 97/1500\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3839 - accuracy: 0.8786 - val_loss: 1.5271 - val_accuracy: 0.5076\n",
      "Epoch 98/1500\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3783 - accuracy: 0.8794 - val_loss: 1.5021 - val_accuracy: 0.5136\n",
      "Epoch 99/1500\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3625 - accuracy: 0.8875 - val_loss: 1.4984 - val_accuracy: 0.5045\n",
      "Epoch 100/1500\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.3764 - accuracy: 0.8941 - val_loss: 1.4891 - val_accuracy: 0.5076\n",
      "Epoch 101/1500\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3467 - accuracy: 0.8867 - val_loss: 1.5361 - val_accuracy: 0.5045\n",
      "Epoch 102/1500\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.3571 - accuracy: 0.8932 - val_loss: 1.5086 - val_accuracy: 0.5045\n",
      "Epoch 103/1500\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.3170 - accuracy: 0.9071 - val_loss: 1.5375 - val_accuracy: 0.4955\n",
      "Epoch 104/1500\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3213 - accuracy: 0.9046 - val_loss: 1.5613 - val_accuracy: 0.5015\n",
      "Epoch 105/1500\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3433 - accuracy: 0.8859 - val_loss: 1.5390 - val_accuracy: 0.5106\n",
      "Epoch 106/1500\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3154 - accuracy: 0.8989 - val_loss: 1.5517 - val_accuracy: 0.4773\n",
      "Epoch 107/1500\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3210 - accuracy: 0.8892 - val_loss: 1.5602 - val_accuracy: 0.4924\n",
      "Epoch 108/1500\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3602 - accuracy: 0.8973 - val_loss: 1.5614 - val_accuracy: 0.5015\n",
      "Epoch 109/1500\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3042 - accuracy: 0.9030 - val_loss: 1.5533 - val_accuracy: 0.5045\n",
      "Epoch 110/1500\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2943 - accuracy: 0.9022 - val_loss: 1.5580 - val_accuracy: 0.4924\n",
      "Epoch 111/1500\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2800 - accuracy: 0.9169 - val_loss: 1.5544 - val_accuracy: 0.5106\n",
      "Epoch 112/1500\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3089 - accuracy: 0.9063 - val_loss: 1.5639 - val_accuracy: 0.5257\n",
      "Epoch 113/1500\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2642 - accuracy: 0.9152 - val_loss: 1.5583 - val_accuracy: 0.5227\n",
      "Epoch 114/1500\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2752 - accuracy: 0.9055 - val_loss: 1.5913 - val_accuracy: 0.5106\n",
      "Epoch 115/1500\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2636 - accuracy: 0.9209 - val_loss: 1.6108 - val_accuracy: 0.5227\n",
      "Epoch 116/1500\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2814 - accuracy: 0.9104 - val_loss: 1.5875 - val_accuracy: 0.5196\n",
      "Epoch 117/1500\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.3270 - accuracy: 0.8941 - val_loss: 1.6182 - val_accuracy: 0.5106\n",
      "Epoch 118/1500\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2740 - accuracy: 0.9095 - val_loss: 1.5843 - val_accuracy: 0.5196\n",
      "Epoch 119/1500\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2565 - accuracy: 0.9226 - val_loss: 1.5274 - val_accuracy: 0.5106\n",
      "Epoch 120/1500\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2594 - accuracy: 0.9169 - val_loss: 1.5811 - val_accuracy: 0.5076\n",
      "Epoch 121/1500\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2662 - accuracy: 0.9193 - val_loss: 1.5869 - val_accuracy: 0.5166\n",
      "Epoch 122/1500\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2559 - accuracy: 0.9226 - val_loss: 1.6182 - val_accuracy: 0.5196\n",
      "Epoch 123/1500\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2236 - accuracy: 0.9307 - val_loss: 1.7110 - val_accuracy: 0.5196\n",
      "Epoch 124/1500\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.2213 - accuracy: 0.9332 - val_loss: 1.7057 - val_accuracy: 0.5136\n",
      "Epoch 125/1500\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2457 - accuracy: 0.9258 - val_loss: 1.7274 - val_accuracy: 0.5136\n",
      "Epoch 126/1500\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2062 - accuracy: 0.9462 - val_loss: 1.7671 - val_accuracy: 0.4955\n",
      "Epoch 127/1500\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.2019 - accuracy: 0.9430 - val_loss: 1.7286 - val_accuracy: 0.5196\n",
      "Epoch 128/1500\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.2319 - accuracy: 0.9250 - val_loss: 1.7400 - val_accuracy: 0.5196\n",
      "Epoch 129/1500\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2610 - accuracy: 0.9267 - val_loss: 1.7006 - val_accuracy: 0.5227\n",
      "Epoch 130/1500\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2134 - accuracy: 0.9421 - val_loss: 1.6587 - val_accuracy: 0.5227\n",
      "Epoch 131/1500\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2303 - accuracy: 0.9307 - val_loss: 1.6336 - val_accuracy: 0.5227\n",
      "Epoch 132/1500\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1923 - accuracy: 0.9413 - val_loss: 1.6477 - val_accuracy: 0.5347\n",
      "Epoch 133/1500\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.2045 - accuracy: 0.9438 - val_loss: 1.6539 - val_accuracy: 0.5196\n",
      "Epoch 134/1500\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2113 - accuracy: 0.9389 - val_loss: 1.7244 - val_accuracy: 0.5257\n",
      "Epoch 135/1500\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2160 - accuracy: 0.9381 - val_loss: 1.7783 - val_accuracy: 0.5076\n",
      "Epoch 136/1500\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2016 - accuracy: 0.9372 - val_loss: 1.7767 - val_accuracy: 0.5287\n",
      "Epoch 137/1500\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2123 - accuracy: 0.9307 - val_loss: 1.8121 - val_accuracy: 0.5227\n",
      "Epoch 138/1500\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1756 - accuracy: 0.9421 - val_loss: 1.7997 - val_accuracy: 0.5287\n",
      "Epoch 139/1500\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1943 - accuracy: 0.9275 - val_loss: 1.8010 - val_accuracy: 0.5317\n",
      "Epoch 140/1500\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2018 - accuracy: 0.9381 - val_loss: 1.8192 - val_accuracy: 0.5287\n",
      "Epoch 141/1500\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2013 - accuracy: 0.9332 - val_loss: 1.8141 - val_accuracy: 0.5317\n",
      "Epoch 142/1500\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1695 - accuracy: 0.9438 - val_loss: 1.8265 - val_accuracy: 0.5257\n",
      "Epoch 143/1500\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1938 - accuracy: 0.9470 - val_loss: 1.8286 - val_accuracy: 0.5227\n",
      "Epoch 144/1500\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1751 - accuracy: 0.9421 - val_loss: 1.8434 - val_accuracy: 0.5317\n",
      "Epoch 145/1500\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1630 - accuracy: 0.9511 - val_loss: 1.7363 - val_accuracy: 0.5257\n",
      "Epoch 146/1500\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1880 - accuracy: 0.9454 - val_loss: 1.7621 - val_accuracy: 0.5196\n",
      "Epoch 147/1500\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.2012 - accuracy: 0.9552 - val_loss: 1.7892 - val_accuracy: 0.5227\n",
      "Epoch 148/1500\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1675 - accuracy: 0.9495 - val_loss: 1.7996 - val_accuracy: 0.5196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/1500\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.1570 - accuracy: 0.9584 - val_loss: 1.8187 - val_accuracy: 0.5136\n",
      "Epoch 150/1500\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1739 - accuracy: 0.9478 - val_loss: 1.7839 - val_accuracy: 0.5317\n",
      "Epoch 151/1500\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1451 - accuracy: 0.9593 - val_loss: 1.8228 - val_accuracy: 0.5257\n",
      "Epoch 152/1500\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1624 - accuracy: 0.9560 - val_loss: 1.8400 - val_accuracy: 0.5257\n",
      "Epoch 153/1500\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1823 - accuracy: 0.9405 - val_loss: 1.8700 - val_accuracy: 0.5136\n",
      "Epoch 154/1500\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1356 - accuracy: 0.9593 - val_loss: 1.8424 - val_accuracy: 0.5287\n",
      "Epoch 155/1500\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1474 - accuracy: 0.9593 - val_loss: 1.8016 - val_accuracy: 0.5257\n",
      "Epoch 156/1500\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1955 - accuracy: 0.9544 - val_loss: 1.8728 - val_accuracy: 0.5136\n",
      "Epoch 157/1500\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1397 - accuracy: 0.9625 - val_loss: 1.8285 - val_accuracy: 0.5227\n",
      "Epoch 158/1500\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1395 - accuracy: 0.9593 - val_loss: 1.7972 - val_accuracy: 0.5136\n",
      "Epoch 159/1500\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1535 - accuracy: 0.9544 - val_loss: 1.8082 - val_accuracy: 0.5166\n",
      "Epoch 160/1500\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.1637 - accuracy: 0.9527 - val_loss: 1.7814 - val_accuracy: 0.5257\n",
      "Epoch 161/1500\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1232 - accuracy: 0.9650 - val_loss: 1.8383 - val_accuracy: 0.5438\n",
      "Epoch 162/1500\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.1254 - accuracy: 0.9641 - val_loss: 1.8967 - val_accuracy: 0.5196\n",
      "Epoch 163/1500\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.1292 - accuracy: 0.9633 - val_loss: 1.8602 - val_accuracy: 0.5287\n",
      "Epoch 164/1500\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.1235 - accuracy: 0.9584 - val_loss: 1.8430 - val_accuracy: 0.5227\n",
      "Epoch 165/1500\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.1653 - accuracy: 0.9617 - val_loss: 1.8599 - val_accuracy: 0.5257\n",
      "Epoch 166/1500\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1076 - accuracy: 0.9682 - val_loss: 1.9058 - val_accuracy: 0.5227\n",
      "Epoch 167/1500\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1469 - accuracy: 0.9544 - val_loss: 1.8973 - val_accuracy: 0.5257\n",
      "Epoch 168/1500\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.1592 - accuracy: 0.9560 - val_loss: 1.9723 - val_accuracy: 0.5317\n",
      "Epoch 169/1500\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1063 - accuracy: 0.9690 - val_loss: 1.8894 - val_accuracy: 0.5408\n",
      "Epoch 170/1500\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1216 - accuracy: 0.9601 - val_loss: 1.8654 - val_accuracy: 0.5287\n",
      "Epoch 171/1500\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1159 - accuracy: 0.9690 - val_loss: 1.9119 - val_accuracy: 0.5317\n",
      "Epoch 172/1500\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.1223 - accuracy: 0.9666 - val_loss: 1.9091 - val_accuracy: 0.5378\n",
      "Epoch 173/1500\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.1340 - accuracy: 0.9584 - val_loss: 1.8855 - val_accuracy: 0.5257\n",
      "Epoch 174/1500\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1209 - accuracy: 0.9666 - val_loss: 1.9823 - val_accuracy: 0.5166\n",
      "Epoch 175/1500\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1428 - accuracy: 0.9503 - val_loss: 1.9382 - val_accuracy: 0.5287\n",
      "Epoch 176/1500\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0929 - accuracy: 0.9788 - val_loss: 1.9239 - val_accuracy: 0.5378\n",
      "Epoch 177/1500\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1346 - accuracy: 0.9568 - val_loss: 1.9171 - val_accuracy: 0.5347\n",
      "Epoch 178/1500\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1200 - accuracy: 0.9698 - val_loss: 1.9398 - val_accuracy: 0.5136\n",
      "Epoch 179/1500\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1212 - accuracy: 0.9593 - val_loss: 2.0152 - val_accuracy: 0.5076\n",
      "Epoch 180/1500\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0983 - accuracy: 0.9731 - val_loss: 1.9174 - val_accuracy: 0.5287\n",
      "Epoch 181/1500\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1132 - accuracy: 0.9658 - val_loss: 2.0024 - val_accuracy: 0.5196\n",
      "Epoch 182/1500\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0901 - accuracy: 0.9747 - val_loss: 1.9640 - val_accuracy: 0.5287\n",
      "Epoch 183/1500\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1222 - accuracy: 0.9617 - val_loss: 1.9455 - val_accuracy: 0.5257\n",
      "Epoch 184/1500\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1054 - accuracy: 0.9682 - val_loss: 1.9546 - val_accuracy: 0.5227\n",
      "Epoch 185/1500\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1194 - accuracy: 0.9625 - val_loss: 1.9669 - val_accuracy: 0.5196\n",
      "Epoch 186/1500\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0992 - accuracy: 0.9731 - val_loss: 1.9789 - val_accuracy: 0.5106\n",
      "Epoch 187/1500\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1248 - accuracy: 0.9576 - val_loss: 2.0013 - val_accuracy: 0.5287\n",
      "Epoch 188/1500\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1035 - accuracy: 0.9666 - val_loss: 1.9268 - val_accuracy: 0.5468\n",
      "Epoch 189/1500\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1021 - accuracy: 0.9682 - val_loss: 1.9387 - val_accuracy: 0.5408\n",
      "Epoch 190/1500\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1132 - accuracy: 0.9690 - val_loss: 2.0256 - val_accuracy: 0.5076\n",
      "Epoch 191/1500\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0846 - accuracy: 0.9756 - val_loss: 2.0358 - val_accuracy: 0.5287\n",
      "Epoch 192/1500\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0930 - accuracy: 0.9804 - val_loss: 1.9396 - val_accuracy: 0.5136\n",
      "Epoch 193/1500\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0989 - accuracy: 0.9666 - val_loss: 1.9703 - val_accuracy: 0.5287\n",
      "Epoch 194/1500\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0832 - accuracy: 0.9756 - val_loss: 2.0782 - val_accuracy: 0.5076\n",
      "Epoch 195/1500\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0897 - accuracy: 0.9707 - val_loss: 2.0559 - val_accuracy: 0.5347\n",
      "Epoch 196/1500\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0845 - accuracy: 0.9715 - val_loss: 1.9894 - val_accuracy: 0.5408\n",
      "Epoch 197/1500\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0833 - accuracy: 0.9698 - val_loss: 1.9780 - val_accuracy: 0.5408\n",
      "Epoch 198/1500\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0909 - accuracy: 0.9747 - val_loss: 2.0443 - val_accuracy: 0.5227\n",
      "Epoch 199/1500\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1010 - accuracy: 0.9731 - val_loss: 1.9685 - val_accuracy: 0.5317\n",
      "Epoch 200/1500\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0882 - accuracy: 0.9747 - val_loss: 2.0130 - val_accuracy: 0.5257\n",
      "Epoch 201/1500\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0903 - accuracy: 0.9731 - val_loss: 2.0396 - val_accuracy: 0.5438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/1500\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0656 - accuracy: 0.9845 - val_loss: 2.0787 - val_accuracy: 0.5287\n",
      "Epoch 203/1500\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0862 - accuracy: 0.9739 - val_loss: 2.0463 - val_accuracy: 0.5196\n",
      "Epoch 204/1500\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0886 - accuracy: 0.9690 - val_loss: 2.0153 - val_accuracy: 0.5438\n",
      "Epoch 205/1500\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0686 - accuracy: 0.9796 - val_loss: 1.9678 - val_accuracy: 0.5287\n",
      "Epoch 206/1500\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0890 - accuracy: 0.9731 - val_loss: 2.1127 - val_accuracy: 0.5287\n",
      "Epoch 207/1500\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0721 - accuracy: 0.9739 - val_loss: 2.1464 - val_accuracy: 0.5347\n",
      "Epoch 208/1500\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0942 - accuracy: 0.9739 - val_loss: 2.1573 - val_accuracy: 0.5438\n",
      "Epoch 209/1500\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0880 - accuracy: 0.9780 - val_loss: 2.0897 - val_accuracy: 0.5378\n",
      "Epoch 210/1500\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1129 - accuracy: 0.9682 - val_loss: 2.1477 - val_accuracy: 0.5166\n",
      "Epoch 211/1500\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0706 - accuracy: 0.9813 - val_loss: 2.0686 - val_accuracy: 0.5438\n",
      "Epoch 212/1500\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0775 - accuracy: 0.9821 - val_loss: 2.1312 - val_accuracy: 0.5257\n",
      "Epoch 213/1500\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0683 - accuracy: 0.9837 - val_loss: 2.0520 - val_accuracy: 0.5408\n",
      "Epoch 214/1500\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0671 - accuracy: 0.9780 - val_loss: 2.0358 - val_accuracy: 0.5498\n",
      "Epoch 215/1500\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0795 - accuracy: 0.9747 - val_loss: 2.1177 - val_accuracy: 0.5317\n",
      "Epoch 216/1500\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0578 - accuracy: 0.9829 - val_loss: 2.0523 - val_accuracy: 0.5347\n",
      "Epoch 217/1500\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0782 - accuracy: 0.9796 - val_loss: 2.1505 - val_accuracy: 0.5408\n",
      "Epoch 218/1500\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0816 - accuracy: 0.9764 - val_loss: 2.0991 - val_accuracy: 0.5498\n",
      "Epoch 219/1500\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0647 - accuracy: 0.9764 - val_loss: 2.1572 - val_accuracy: 0.5317\n",
      "Epoch 220/1500\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0822 - accuracy: 0.9788 - val_loss: 2.1174 - val_accuracy: 0.5378\n",
      "Epoch 221/1500\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0624 - accuracy: 0.9796 - val_loss: 2.0820 - val_accuracy: 0.5287\n",
      "Epoch 222/1500\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0587 - accuracy: 0.9853 - val_loss: 2.1257 - val_accuracy: 0.5408\n",
      "Epoch 223/1500\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0740 - accuracy: 0.9829 - val_loss: 2.0683 - val_accuracy: 0.5347\n",
      "Epoch 224/1500\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0792 - accuracy: 0.9731 - val_loss: 2.1068 - val_accuracy: 0.5378\n",
      "Epoch 225/1500\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0592 - accuracy: 0.9870 - val_loss: 2.1771 - val_accuracy: 0.5438\n",
      "Epoch 226/1500\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0736 - accuracy: 0.9739 - val_loss: 2.2955 - val_accuracy: 0.5468\n",
      "Epoch 227/1500\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0649 - accuracy: 0.9821 - val_loss: 2.2912 - val_accuracy: 0.5468\n",
      "Epoch 228/1500\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0607 - accuracy: 0.9796 - val_loss: 2.1624 - val_accuracy: 0.5378\n",
      "Epoch 229/1500\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0574 - accuracy: 0.9821 - val_loss: 2.1993 - val_accuracy: 0.5317\n",
      "Epoch 230/1500\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0700 - accuracy: 0.9813 - val_loss: 2.1561 - val_accuracy: 0.5438\n",
      "Epoch 231/1500\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0652 - accuracy: 0.9788 - val_loss: 2.1763 - val_accuracy: 0.5196\n",
      "Epoch 232/1500\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0669 - accuracy: 0.9829 - val_loss: 2.2796 - val_accuracy: 0.5227\n",
      "Epoch 233/1500\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0652 - accuracy: 0.9788 - val_loss: 2.2505 - val_accuracy: 0.5347\n",
      "Epoch 234/1500\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0612 - accuracy: 0.9813 - val_loss: 2.1267 - val_accuracy: 0.5378\n",
      "Epoch 235/1500\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0476 - accuracy: 0.9861 - val_loss: 2.0685 - val_accuracy: 0.5438\n",
      "Epoch 236/1500\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0704 - accuracy: 0.9780 - val_loss: 2.1033 - val_accuracy: 0.5438\n",
      "Epoch 237/1500\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0622 - accuracy: 0.9837 - val_loss: 2.2271 - val_accuracy: 0.5378\n",
      "Epoch 238/1500\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0490 - accuracy: 0.9829 - val_loss: 2.2031 - val_accuracy: 0.5468\n",
      "Epoch 239/1500\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0525 - accuracy: 0.9861 - val_loss: 2.2693 - val_accuracy: 0.5378\n",
      "Epoch 240/1500\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0398 - accuracy: 0.9886 - val_loss: 2.2785 - val_accuracy: 0.5378\n",
      "Epoch 241/1500\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0493 - accuracy: 0.9870 - val_loss: 2.1904 - val_accuracy: 0.5287\n",
      "Epoch 242/1500\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0500 - accuracy: 0.9845 - val_loss: 2.2314 - val_accuracy: 0.5196\n",
      "Epoch 243/1500\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0568 - accuracy: 0.9788 - val_loss: 2.2546 - val_accuracy: 0.5257\n",
      "Epoch 244/1500\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0552 - accuracy: 0.9837 - val_loss: 2.1310 - val_accuracy: 0.5408\n",
      "Epoch 245/1500\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0662 - accuracy: 0.9772 - val_loss: 2.1360 - val_accuracy: 0.5347\n",
      "Epoch 246/1500\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0562 - accuracy: 0.9829 - val_loss: 2.1524 - val_accuracy: 0.5347\n",
      "Epoch 247/1500\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0577 - accuracy: 0.9845 - val_loss: 2.1512 - val_accuracy: 0.5347\n",
      "Epoch 248/1500\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0762 - accuracy: 0.9764 - val_loss: 2.2566 - val_accuracy: 0.5468\n",
      "Epoch 249/1500\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1005 - accuracy: 0.9804 - val_loss: 2.2916 - val_accuracy: 0.5317\n",
      "Epoch 250/1500\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0538 - accuracy: 0.9813 - val_loss: 2.2430 - val_accuracy: 0.5468\n",
      "Epoch 251/1500\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0525 - accuracy: 0.9861 - val_loss: 2.2877 - val_accuracy: 0.5317\n",
      "Epoch 252/1500\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0418 - accuracy: 0.9870 - val_loss: 2.4108 - val_accuracy: 0.5408\n",
      "Epoch 253/1500\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0757 - accuracy: 0.9870 - val_loss: 2.2020 - val_accuracy: 0.5257\n",
      "Epoch 254/1500\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0599 - accuracy: 0.9845 - val_loss: 2.1331 - val_accuracy: 0.5378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255/1500\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0382 - accuracy: 0.9910 - val_loss: 2.1192 - val_accuracy: 0.5468\n",
      "Epoch 256/1500\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0555 - accuracy: 0.9853 - val_loss: 2.2309 - val_accuracy: 0.5378\n",
      "Epoch 257/1500\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0373 - accuracy: 0.9902 - val_loss: 2.2491 - val_accuracy: 0.5347\n",
      "Epoch 258/1500\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0373 - accuracy: 0.9902 - val_loss: 2.2183 - val_accuracy: 0.5498\n",
      "Epoch 259/1500\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0789 - accuracy: 0.9731 - val_loss: 2.2061 - val_accuracy: 0.5498\n",
      "Epoch 261/1500\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0438 - accuracy: 0.9902 - val_loss: 2.2010 - val_accuracy: 0.5378\n",
      "Epoch 262/1500\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0681 - accuracy: 0.9829 - val_loss: 2.1946 - val_accuracy: 0.5589\n",
      "Epoch 263/1500\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0496 - accuracy: 0.9845 - val_loss: 2.2186 - val_accuracy: 0.5378\n",
      "Epoch 264/1500\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0348 - accuracy: 0.9902 - val_loss: 2.2542 - val_accuracy: 0.5468\n",
      "Epoch 265/1500\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0460 - accuracy: 0.9861 - val_loss: 2.3090 - val_accuracy: 0.5257\n",
      "Epoch 266/1500\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0775 - accuracy: 0.9804 - val_loss: 2.2356 - val_accuracy: 0.5287\n",
      "Epoch 267/1500\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0526 - accuracy: 0.9788 - val_loss: 2.2705 - val_accuracy: 0.5317\n",
      "Epoch 268/1500\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0577 - accuracy: 0.9821 - val_loss: 2.2737 - val_accuracy: 0.5257\n",
      "Epoch 269/1500\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0312 - accuracy: 0.9910 - val_loss: 2.2746 - val_accuracy: 0.5347\n",
      "Epoch 270/1500\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0434 - accuracy: 0.9870 - val_loss: 2.2535 - val_accuracy: 0.5347\n",
      "Epoch 271/1500\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0661 - accuracy: 0.9747 - val_loss: 2.3264 - val_accuracy: 0.5257\n",
      "Epoch 272/1500\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0428 - accuracy: 0.9878 - val_loss: 2.3081 - val_accuracy: 0.5438\n",
      "Epoch 273/1500\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0564 - accuracy: 0.9821 - val_loss: 2.3140 - val_accuracy: 0.5438\n",
      "Epoch 274/1500\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0326 - accuracy: 0.9935 - val_loss: 2.2649 - val_accuracy: 0.5227\n",
      "Epoch 275/1500\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0559 - accuracy: 0.9796 - val_loss: 2.2698 - val_accuracy: 0.5408\n",
      "Epoch 276/1500\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0435 - accuracy: 0.9845 - val_loss: 2.2902 - val_accuracy: 0.5438\n",
      "Epoch 277/1500\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0265 - accuracy: 0.9927 - val_loss: 2.2551 - val_accuracy: 0.5378\n",
      "Epoch 278/1500\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0450 - accuracy: 0.9845 - val_loss: 2.3161 - val_accuracy: 0.5378\n",
      "Epoch 279/1500\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0394 - accuracy: 0.9853 - val_loss: 2.3308 - val_accuracy: 0.5408\n",
      "Epoch 280/1500\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0303 - accuracy: 0.9894 - val_loss: 2.4164 - val_accuracy: 0.5438\n",
      "Epoch 281/1500\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0475 - accuracy: 0.9861 - val_loss: 2.3540 - val_accuracy: 0.5498\n",
      "Epoch 282/1500\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0350 - accuracy: 0.9870 - val_loss: 2.3072 - val_accuracy: 0.5378\n",
      "Epoch 283/1500\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0638 - accuracy: 0.9837 - val_loss: 2.3252 - val_accuracy: 0.5196\n",
      "Epoch 284/1500\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0370 - accuracy: 0.9902 - val_loss: 2.3143 - val_accuracy: 0.5468\n",
      "Epoch 285/1500\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0584 - accuracy: 0.9837 - val_loss: 2.2935 - val_accuracy: 0.5378\n",
      "Epoch 286/1500\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0503 - accuracy: 0.9821 - val_loss: 2.5159 - val_accuracy: 0.5106\n",
      "Epoch 287/1500\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0590 - accuracy: 0.9886 - val_loss: 2.4468 - val_accuracy: 0.5257\n",
      "Epoch 288/1500\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0451 - accuracy: 0.9853 - val_loss: 2.2654 - val_accuracy: 0.5378\n",
      "Epoch 289/1500\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0545 - accuracy: 0.9845 - val_loss: 2.2737 - val_accuracy: 0.5498\n",
      "Epoch 290/1500\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0568 - accuracy: 0.9837 - val_loss: 2.2638 - val_accuracy: 0.5317\n",
      "Epoch 291/1500\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0504 - accuracy: 0.9870 - val_loss: 2.3225 - val_accuracy: 0.5287\n",
      "Epoch 292/1500\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0764 - accuracy: 0.9804 - val_loss: 2.3351 - val_accuracy: 0.5317\n",
      "Epoch 293/1500\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0425 - accuracy: 0.9845 - val_loss: 2.5007 - val_accuracy: 0.5287\n",
      "Epoch 294/1500\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0370 - accuracy: 0.9894 - val_loss: 2.4309 - val_accuracy: 0.5347\n",
      "Epoch 295/1500\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0344 - accuracy: 0.9894 - val_loss: 2.4744 - val_accuracy: 0.5257\n",
      "Epoch 296/1500\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0320 - accuracy: 0.9919 - val_loss: 2.5302 - val_accuracy: 0.5529\n",
      "Epoch 297/1500\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0410 - accuracy: 0.9886 - val_loss: 2.5014 - val_accuracy: 0.5498\n",
      "Epoch 298/1500\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0523 - accuracy: 0.9845 - val_loss: 2.5519 - val_accuracy: 0.5498\n",
      "Epoch 299/1500\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0582 - accuracy: 0.9886 - val_loss: 2.4678 - val_accuracy: 0.5529\n",
      "Epoch 300/1500\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0503 - accuracy: 0.9845 - val_loss: 2.5048 - val_accuracy: 0.5559\n",
      "Epoch 301/1500\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0412 - accuracy: 0.9870 - val_loss: 2.5567 - val_accuracy: 0.5438\n",
      "Epoch 302/1500\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0509 - accuracy: 0.9886 - val_loss: 2.5638 - val_accuracy: 0.5378\n",
      "Epoch 303/1500\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0350 - accuracy: 0.9919 - val_loss: 2.3747 - val_accuracy: 0.5498\n",
      "Epoch 304/1500\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0552 - accuracy: 0.9829 - val_loss: 2.4967 - val_accuracy: 0.5498\n",
      "Epoch 305/1500\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0312 - accuracy: 0.9902 - val_loss: 2.4854 - val_accuracy: 0.5347\n",
      "Epoch 306/1500\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0399 - accuracy: 0.9861 - val_loss: 2.4118 - val_accuracy: 0.5438\n",
      "Epoch 307/1500\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0520 - accuracy: 0.9813 - val_loss: 2.4388 - val_accuracy: 0.5347\n",
      "Epoch 308/1500\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0272 - accuracy: 0.9935 - val_loss: 2.3783 - val_accuracy: 0.5438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309/1500\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0292 - accuracy: 0.9902 - val_loss: 2.4711 - val_accuracy: 0.5378\n",
      "Epoch 310/1500\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0351 - accuracy: 0.9910 - val_loss: 2.4469 - val_accuracy: 0.5408\n",
      "Epoch 311/1500\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0342 - accuracy: 0.9910 - val_loss: 2.4123 - val_accuracy: 0.5468\n",
      "Epoch 312/1500\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0261 - accuracy: 0.9910 - val_loss: 2.3631 - val_accuracy: 0.5498\n",
      "Epoch 313/1500\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0433 - accuracy: 0.9870 - val_loss: 2.4636 - val_accuracy: 0.5438\n",
      "Epoch 314/1500\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0298 - accuracy: 0.9919 - val_loss: 2.4618 - val_accuracy: 0.5378\n",
      "Epoch 315/1500\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0379 - accuracy: 0.9878 - val_loss: 2.5626 - val_accuracy: 0.5468\n",
      "Epoch 316/1500\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0449 - accuracy: 0.9870 - val_loss: 2.5059 - val_accuracy: 0.5378\n",
      "Epoch 317/1500\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0306 - accuracy: 0.9894 - val_loss: 2.5521 - val_accuracy: 0.5438\n",
      "Epoch 318/1500\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0369 - accuracy: 0.9919 - val_loss: 2.5177 - val_accuracy: 0.5347\n",
      "Epoch 319/1500\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0394 - accuracy: 0.9902 - val_loss: 2.5008 - val_accuracy: 0.5287\n",
      "Epoch 320/1500\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0490 - accuracy: 0.9845 - val_loss: 2.4727 - val_accuracy: 0.5468\n",
      "Epoch 321/1500\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0366 - accuracy: 0.9878 - val_loss: 2.5442 - val_accuracy: 0.5347\n",
      "Epoch 323/1500\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0283 - accuracy: 0.9894 - val_loss: 2.5690 - val_accuracy: 0.5378\n",
      "Epoch 324/1500\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0490 - accuracy: 0.9853 - val_loss: 2.4317 - val_accuracy: 0.5347\n",
      "Epoch 326/1500\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0431 - accuracy: 0.9878 - val_loss: 2.4112 - val_accuracy: 0.5529\n",
      "Epoch 327/1500\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0266 - accuracy: 0.9919 - val_loss: 2.4701 - val_accuracy: 0.5408\n",
      "Epoch 328/1500\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0354 - accuracy: 0.9870 - val_loss: 2.7959 - val_accuracy: 0.5378\n",
      "Epoch 329/1500\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0367 - accuracy: 0.9861 - val_loss: 2.5138 - val_accuracy: 0.5468\n",
      "Epoch 330/1500\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0319 - accuracy: 0.9902 - val_loss: 2.4600 - val_accuracy: 0.5408\n",
      "Epoch 331/1500\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0434 - accuracy: 0.9829 - val_loss: 2.4721 - val_accuracy: 0.5438\n",
      "Epoch 333/1500\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0272 - accuracy: 0.9910 - val_loss: 2.4582 - val_accuracy: 0.5408\n",
      "Epoch 334/1500\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0118 - accuracy: 0.9984 - val_loss: 2.5197 - val_accuracy: 0.5408\n",
      "Epoch 335/1500\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0263 - accuracy: 0.9927 - val_loss: 2.4563 - val_accuracy: 0.5317\n",
      "Epoch 336/1500\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0370 - accuracy: 0.9894 - val_loss: 2.5416 - val_accuracy: 0.5498\n",
      "Epoch 337/1500\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0342 - accuracy: 0.9919 - val_loss: 2.5233 - val_accuracy: 0.5347\n",
      "Epoch 338/1500\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0317 - accuracy: 0.9902 - val_loss: 2.5307 - val_accuracy: 0.5287\n",
      "Epoch 339/1500\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0236 - accuracy: 0.9919 - val_loss: 2.6948 - val_accuracy: 0.5378\n",
      "Epoch 340/1500\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0245 - accuracy: 0.9927 - val_loss: 2.6408 - val_accuracy: 0.5287\n",
      "Epoch 343/1500\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0403 - accuracy: 0.9886 - val_loss: 2.5131 - val_accuracy: 0.5378\n",
      "Epoch 344/1500\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0238 - accuracy: 0.9927 - val_loss: 2.5766 - val_accuracy: 0.5347\n",
      "Epoch 346/1500\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0345 - accuracy: 0.9853 - val_loss: 2.5886 - val_accuracy: 0.5408\n",
      "Epoch 347/1500\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0189 - accuracy: 0.9951 - val_loss: 2.6579 - val_accuracy: 0.5317\n",
      "Epoch 348/1500\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0276 - accuracy: 0.9919 - val_loss: 2.6769 - val_accuracy: 0.5257\n",
      "Epoch 349/1500\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0530 - accuracy: 0.9878 - val_loss: 2.5912 - val_accuracy: 0.5408\n",
      "Epoch 350/1500\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0251 - accuracy: 0.9935 - val_loss: 2.6214 - val_accuracy: 0.5408\n",
      "Epoch 351/1500\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0291 - accuracy: 0.9919 - val_loss: 2.6445 - val_accuracy: 0.5378\n",
      "Epoch 352/1500\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0324 - accuracy: 0.9910 - val_loss: 2.6584 - val_accuracy: 0.5317\n",
      "Epoch 353/1500\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0283 - accuracy: 0.9927 - val_loss: 2.6815 - val_accuracy: 0.5196\n",
      "Epoch 354/1500\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0205 - accuracy: 0.9951 - val_loss: 2.6783 - val_accuracy: 0.5317\n",
      "Epoch 355/1500\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0193 - accuracy: 0.9951 - val_loss: 2.5764 - val_accuracy: 0.5498\n",
      "Epoch 356/1500\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0255 - accuracy: 0.9902 - val_loss: 2.6727 - val_accuracy: 0.5287\n",
      "Epoch 357/1500\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0149 - accuracy: 0.9951 - val_loss: 2.6197 - val_accuracy: 0.5347\n",
      "Epoch 358/1500\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0234 - accuracy: 0.9919 - val_loss: 2.8045 - val_accuracy: 0.5106\n",
      "Epoch 359/1500\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0217 - accuracy: 0.9910 - val_loss: 2.5991 - val_accuracy: 0.5378\n",
      "Epoch 360/1500\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0302 - accuracy: 0.9870 - val_loss: 2.6561 - val_accuracy: 0.5408\n",
      "Epoch 361/1500\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0153 - accuracy: 0.9984 - val_loss: 2.6142 - val_accuracy: 0.5347\n",
      "Epoch 362/1500\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0260 - accuracy: 0.9894 - val_loss: 2.6154 - val_accuracy: 0.5227\n",
      "Epoch 363/1500\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0277 - accuracy: 0.9935 - val_loss: 2.5409 - val_accuracy: 0.5378\n",
      "Epoch 364/1500\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0243 - accuracy: 0.9943 - val_loss: 2.5716 - val_accuracy: 0.5287\n",
      "Epoch 365/1500\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0196 - accuracy: 0.9919 - val_loss: 2.5744 - val_accuracy: 0.5438\n",
      "Epoch 366/1500\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0345 - accuracy: 0.9935 - val_loss: 2.5935 - val_accuracy: 0.5438\n",
      "Epoch 367/1500\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0245 - accuracy: 0.9935 - val_loss: 2.6267 - val_accuracy: 0.5287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/1500\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0287 - accuracy: 0.9927 - val_loss: 2.5912 - val_accuracy: 0.5408\n",
      "Epoch 369/1500\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0192 - accuracy: 0.9943 - val_loss: 2.7541 - val_accuracy: 0.5317\n",
      "Epoch 370/1500\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0187 - accuracy: 0.9935 - val_loss: 2.6987 - val_accuracy: 0.5257\n",
      "Epoch 371/1500\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0232 - accuracy: 0.9927 - val_loss: 2.6965 - val_accuracy: 0.5287\n",
      "Epoch 372/1500\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0191 - accuracy: 0.9943 - val_loss: 2.7093 - val_accuracy: 0.5257\n",
      "Epoch 373/1500\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0224 - accuracy: 0.9951 - val_loss: 3.0086 - val_accuracy: 0.5227\n",
      "Epoch 374/1500\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0357 - accuracy: 0.9886 - val_loss: 2.6892 - val_accuracy: 0.5347\n",
      "Epoch 375/1500\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0375 - accuracy: 0.9910 - val_loss: 2.7081 - val_accuracy: 0.5408\n",
      "Epoch 376/1500\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0252 - accuracy: 0.9910 - val_loss: 2.8344 - val_accuracy: 0.5227\n",
      "Epoch 377/1500\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0349 - accuracy: 0.9910 - val_loss: 2.7805 - val_accuracy: 0.5468\n",
      "Epoch 378/1500\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0290 - accuracy: 0.9943 - val_loss: 2.6946 - val_accuracy: 0.5347\n",
      "Epoch 379/1500\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0217 - accuracy: 0.9935 - val_loss: 2.6911 - val_accuracy: 0.5287\n",
      "Epoch 380/1500\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0267 - accuracy: 0.9943 - val_loss: 2.7127 - val_accuracy: 0.5408\n",
      "Epoch 381/1500\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0247 - accuracy: 0.9951 - val_loss: 2.6701 - val_accuracy: 0.5378\n",
      "Epoch 382/1500\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0651 - accuracy: 0.9837 - val_loss: 2.7056 - val_accuracy: 0.5468\n",
      "Epoch 383/1500\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0186 - accuracy: 0.9951 - val_loss: 2.6861 - val_accuracy: 0.5378\n",
      "Epoch 384/1500\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0305 - accuracy: 0.9919 - val_loss: 2.8676 - val_accuracy: 0.5468\n",
      "Epoch 385/1500\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0217 - accuracy: 0.9935 - val_loss: 2.9693 - val_accuracy: 0.5196\n",
      "Epoch 386/1500\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0400 - accuracy: 0.9886 - val_loss: 2.8910 - val_accuracy: 0.5257\n",
      "Epoch 387/1500\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0153 - accuracy: 0.9959 - val_loss: 2.8275 - val_accuracy: 0.5347\n",
      "Epoch 388/1500\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0904 - accuracy: 0.9878 - val_loss: 2.7975 - val_accuracy: 0.5438\n",
      "Epoch 389/1500\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0245 - accuracy: 0.9935 - val_loss: 2.7009 - val_accuracy: 0.5347\n",
      "Epoch 390/1500\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0254 - accuracy: 0.9910 - val_loss: 2.6568 - val_accuracy: 0.5408\n",
      "Epoch 391/1500\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0317 - accuracy: 0.9878 - val_loss: 2.7821 - val_accuracy: 0.5378\n",
      "Epoch 392/1500\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0205 - accuracy: 0.9943 - val_loss: 2.8149 - val_accuracy: 0.5347\n",
      "Epoch 393/1500\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0260 - accuracy: 0.9935 - val_loss: 2.8312 - val_accuracy: 0.5317\n",
      "Epoch 394/1500\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0159 - accuracy: 0.9951 - val_loss: 2.7336 - val_accuracy: 0.5438\n",
      "Epoch 395/1500\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0144 - accuracy: 0.9951 - val_loss: 2.7069 - val_accuracy: 0.5498\n",
      "Epoch 396/1500\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0179 - accuracy: 0.9935 - val_loss: 2.8012 - val_accuracy: 0.5408\n",
      "Epoch 397/1500\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0240 - accuracy: 0.9935 - val_loss: 2.7149 - val_accuracy: 0.5347\n",
      "Epoch 398/1500\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0192 - accuracy: 0.9927 - val_loss: 2.7210 - val_accuracy: 0.5408\n",
      "Epoch 399/1500\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0382 - accuracy: 0.9870 - val_loss: 2.6036 - val_accuracy: 0.5529\n",
      "Epoch 400/1500\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0255 - accuracy: 0.9910 - val_loss: 2.6685 - val_accuracy: 0.5287\n",
      "Epoch 401/1500\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0265 - accuracy: 0.9910 - val_loss: 2.8486 - val_accuracy: 0.5257\n",
      "Epoch 402/1500\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0235 - accuracy: 0.9927 - val_loss: 2.8612 - val_accuracy: 0.5317\n",
      "Epoch 403/1500\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0203 - accuracy: 0.9935 - val_loss: 2.8594 - val_accuracy: 0.5257\n",
      "Epoch 404/1500\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0192 - accuracy: 0.9943 - val_loss: 2.8034 - val_accuracy: 0.5408\n",
      "Epoch 405/1500\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0136 - accuracy: 0.9959 - val_loss: 2.9983 - val_accuracy: 0.5287\n",
      "Epoch 406/1500\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0272 - accuracy: 0.9935 - val_loss: 2.9704 - val_accuracy: 0.5196\n",
      "Epoch 407/1500\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0480 - accuracy: 0.9910 - val_loss: 2.9092 - val_accuracy: 0.5317\n",
      "Epoch 408/1500\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0265 - accuracy: 0.9919 - val_loss: 3.0140 - val_accuracy: 0.5408\n",
      "Epoch 409/1500\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0189 - accuracy: 0.9927 - val_loss: 2.7186 - val_accuracy: 0.5619\n",
      "Epoch 410/1500\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0168 - accuracy: 0.9959 - val_loss: 2.8712 - val_accuracy: 0.5287\n",
      "Epoch 411/1500\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0160 - accuracy: 0.9976 - val_loss: 2.6842 - val_accuracy: 0.5498\n",
      "Epoch 412/1500\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0160 - accuracy: 0.9951 - val_loss: 2.9057 - val_accuracy: 0.5196\n",
      "Epoch 413/1500\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0251 - accuracy: 0.9894 - val_loss: 2.7911 - val_accuracy: 0.5347\n",
      "Epoch 414/1500\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0238 - accuracy: 0.9935 - val_loss: 2.7567 - val_accuracy: 0.5227\n",
      "Epoch 415/1500\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0290 - accuracy: 0.9894 - val_loss: 2.7668 - val_accuracy: 0.5378\n",
      "Epoch 416/1500\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0188 - accuracy: 0.9951 - val_loss: 2.7750 - val_accuracy: 0.5378\n",
      "Epoch 417/1500\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0094 - accuracy: 0.9984 - val_loss: 2.9164 - val_accuracy: 0.5106\n",
      "Epoch 418/1500\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0082 - accuracy: 0.9976 - val_loss: 2.8318 - val_accuracy: 0.5438\n",
      "Epoch 419/1500\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0351 - accuracy: 0.9935 - val_loss: 2.9259 - val_accuracy: 0.5227\n",
      "Epoch 420/1500\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0094 - accuracy: 0.9967 - val_loss: 2.8627 - val_accuracy: 0.5347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/1500\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0134 - accuracy: 0.9959 - val_loss: 2.8855 - val_accuracy: 0.5468\n",
      "Epoch 422/1500\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0126 - accuracy: 0.9967 - val_loss: 2.8701 - val_accuracy: 0.5559\n",
      "Epoch 423/1500\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0113 - accuracy: 0.9951 - val_loss: 2.9178 - val_accuracy: 0.5408\n",
      "Epoch 424/1500\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0179 - accuracy: 0.9935 - val_loss: 2.9373 - val_accuracy: 0.5468\n",
      "Epoch 425/1500\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0157 - accuracy: 0.9951 - val_loss: 2.9258 - val_accuracy: 0.5529\n",
      "Epoch 426/1500\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0336 - accuracy: 0.9919 - val_loss: 2.9519 - val_accuracy: 0.5619\n",
      "Epoch 427/1500\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0249 - accuracy: 0.9894 - val_loss: 2.9680 - val_accuracy: 0.5317\n",
      "Epoch 428/1500\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0228 - accuracy: 0.9910 - val_loss: 2.8605 - val_accuracy: 0.5468\n",
      "Epoch 429/1500\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0133 - accuracy: 0.9967 - val_loss: 2.9817 - val_accuracy: 0.5378\n",
      "Epoch 430/1500\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0246 - accuracy: 0.9951 - val_loss: 2.9773 - val_accuracy: 0.5408\n",
      "Epoch 431/1500\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0135 - accuracy: 0.9935 - val_loss: 2.9486 - val_accuracy: 0.5408\n",
      "Epoch 432/1500\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0203 - accuracy: 0.9959 - val_loss: 2.9563 - val_accuracy: 0.5227\n",
      "Epoch 433/1500\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0096 - accuracy: 0.9976 - val_loss: 2.8629 - val_accuracy: 0.5438\n",
      "Epoch 434/1500\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0206 - accuracy: 0.9967 - val_loss: 2.9391 - val_accuracy: 0.5317\n",
      "Epoch 435/1500\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0245 - accuracy: 0.9927 - val_loss: 2.9160 - val_accuracy: 0.5438\n",
      "Epoch 436/1500\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0196 - accuracy: 0.9943 - val_loss: 2.9570 - val_accuracy: 0.5227\n",
      "Epoch 437/1500\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0221 - accuracy: 0.9935 - val_loss: 2.8532 - val_accuracy: 0.5408\n",
      "Epoch 438/1500\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0173 - accuracy: 0.9959 - val_loss: 2.9021 - val_accuracy: 0.5468\n",
      "Epoch 439/1500\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0203 - accuracy: 0.9943 - val_loss: 2.8399 - val_accuracy: 0.5378\n",
      "Epoch 440/1500\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0296 - accuracy: 0.9967 - val_loss: 2.8616 - val_accuracy: 0.5529\n",
      "Epoch 441/1500\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0334 - accuracy: 0.9943 - val_loss: 2.8585 - val_accuracy: 0.5498\n",
      "Epoch 442/1500\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0126 - accuracy: 0.9967 - val_loss: 2.9449 - val_accuracy: 0.5408\n",
      "Epoch 443/1500\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0185 - accuracy: 0.9959 - val_loss: 2.9006 - val_accuracy: 0.5650\n",
      "Epoch 444/1500\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0134 - accuracy: 0.9984 - val_loss: 2.8879 - val_accuracy: 0.5529\n",
      "Epoch 445/1500\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0140 - accuracy: 0.9976 - val_loss: 2.8244 - val_accuracy: 0.5559\n",
      "Epoch 446/1500\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0100 - accuracy: 0.9967 - val_loss: 2.8839 - val_accuracy: 0.5619\n",
      "Epoch 447/1500\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0178 - accuracy: 0.9935 - val_loss: 2.8524 - val_accuracy: 0.5317\n",
      "Epoch 448/1500\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0313 - accuracy: 0.9886 - val_loss: 2.9377 - val_accuracy: 0.5498\n",
      "Epoch 449/1500\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0162 - accuracy: 0.9935 - val_loss: 2.9301 - val_accuracy: 0.5347\n",
      "Epoch 450/1500\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0093 - accuracy: 0.9976 - val_loss: 2.9707 - val_accuracy: 0.5378\n",
      "Epoch 451/1500\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0262 - accuracy: 0.9935 - val_loss: 2.9737 - val_accuracy: 0.5529\n",
      "Epoch 452/1500\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0136 - accuracy: 0.9959 - val_loss: 2.9364 - val_accuracy: 0.5468\n",
      "Epoch 453/1500\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0160 - accuracy: 0.9967 - val_loss: 2.9606 - val_accuracy: 0.5347\n",
      "Epoch 454/1500\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0147 - accuracy: 0.9959 - val_loss: 2.8371 - val_accuracy: 0.5498\n",
      "Epoch 455/1500\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0176 - accuracy: 0.9943 - val_loss: 2.9095 - val_accuracy: 0.5257\n",
      "Epoch 456/1500\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0158 - accuracy: 0.9959 - val_loss: 2.8706 - val_accuracy: 0.5559\n",
      "Epoch 457/1500\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0075 - accuracy: 0.9984 - val_loss: 2.9484 - val_accuracy: 0.5498\n",
      "Epoch 458/1500\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0162 - accuracy: 0.9935 - val_loss: 2.9783 - val_accuracy: 0.5347\n",
      "Epoch 459/1500\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0174 - accuracy: 0.9943 - val_loss: 2.9378 - val_accuracy: 0.5529\n",
      "Epoch 460/1500\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0211 - accuracy: 0.9919 - val_loss: 2.9793 - val_accuracy: 0.5317\n",
      "Epoch 461/1500\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0095 - accuracy: 0.9976 - val_loss: 2.8597 - val_accuracy: 0.5619\n",
      "Epoch 462/1500\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0171 - accuracy: 0.9959 - val_loss: 2.9535 - val_accuracy: 0.5438\n",
      "Epoch 463/1500\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0215 - accuracy: 0.9919 - val_loss: 2.9612 - val_accuracy: 0.5468\n",
      "Epoch 464/1500\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0146 - accuracy: 0.9967 - val_loss: 2.9952 - val_accuracy: 0.5378\n",
      "Epoch 465/1500\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0255 - accuracy: 0.9902 - val_loss: 2.9527 - val_accuracy: 0.5498\n",
      "Epoch 466/1500\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0294 - accuracy: 0.9951 - val_loss: 2.9683 - val_accuracy: 0.5529\n",
      "Epoch 467/1500\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0195 - accuracy: 0.9943 - val_loss: 2.9572 - val_accuracy: 0.5559\n",
      "Epoch 468/1500\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0090 - accuracy: 0.9976 - val_loss: 3.0330 - val_accuracy: 0.5347\n",
      "Epoch 469/1500\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0304 - accuracy: 0.9943 - val_loss: 2.9666 - val_accuracy: 0.5468\n",
      "Epoch 470/1500\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0101 - accuracy: 0.9967 - val_loss: 2.7752 - val_accuracy: 0.5438\n",
      "Epoch 471/1500\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0183 - accuracy: 0.9943 - val_loss: 2.8892 - val_accuracy: 0.5498\n",
      "Epoch 472/1500\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0175 - accuracy: 0.9967 - val_loss: 2.9638 - val_accuracy: 0.5438\n",
      "Epoch 473/1500\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0168 - accuracy: 0.9943 - val_loss: 2.9695 - val_accuracy: 0.5559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474/1500\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0231 - accuracy: 0.9919 - val_loss: 2.8493 - val_accuracy: 0.5559\n",
      "Epoch 475/1500\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0158 - accuracy: 0.9967 - val_loss: 2.9814 - val_accuracy: 0.5438\n",
      "Epoch 476/1500\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0156 - accuracy: 0.9967 - val_loss: 2.9999 - val_accuracy: 0.5529\n",
      "Epoch 477/1500\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0252 - accuracy: 0.9959 - val_loss: 2.9976 - val_accuracy: 0.5529\n",
      "Epoch 478/1500\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0170 - accuracy: 0.9943 - val_loss: 2.8226 - val_accuracy: 0.5589\n",
      "Epoch 479/1500\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0159 - accuracy: 0.9951 - val_loss: 2.9174 - val_accuracy: 0.5498\n",
      "Epoch 480/1500\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0085 - accuracy: 0.9976 - val_loss: 2.7996 - val_accuracy: 0.5347\n",
      "Epoch 481/1500\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0057 - accuracy: 0.9992 - val_loss: 2.8433 - val_accuracy: 0.5529\n",
      "Epoch 482/1500\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0121 - accuracy: 0.9959 - val_loss: 2.8628 - val_accuracy: 0.5529\n",
      "Epoch 483/1500\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0166 - accuracy: 0.9959 - val_loss: 2.9708 - val_accuracy: 0.5589\n",
      "Epoch 484/1500\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0119 - accuracy: 0.9967 - val_loss: 2.9442 - val_accuracy: 0.5468\n",
      "Epoch 485/1500\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0165 - accuracy: 0.9976 - val_loss: 2.9188 - val_accuracy: 0.5559\n",
      "Epoch 486/1500\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0178 - accuracy: 0.9935 - val_loss: 2.9600 - val_accuracy: 0.5589\n",
      "Epoch 487/1500\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0175 - accuracy: 0.9951 - val_loss: 3.0320 - val_accuracy: 0.5559\n",
      "Epoch 488/1500\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0144 - accuracy: 0.9959 - val_loss: 3.0292 - val_accuracy: 0.5347\n",
      "Epoch 489/1500\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0237 - accuracy: 0.9935 - val_loss: 2.9995 - val_accuracy: 0.5317\n",
      "Epoch 490/1500\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0141 - accuracy: 0.9951 - val_loss: 3.0654 - val_accuracy: 0.5438\n",
      "Epoch 491/1500\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0132 - accuracy: 0.9959 - val_loss: 3.2091 - val_accuracy: 0.5408\n",
      "Epoch 492/1500\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0247 - accuracy: 0.9902 - val_loss: 3.1279 - val_accuracy: 0.5378\n",
      "Epoch 493/1500\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0150 - accuracy: 0.9951 - val_loss: 2.9990 - val_accuracy: 0.5347\n",
      "Epoch 494/1500\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0203 - accuracy: 0.9943 - val_loss: 2.9876 - val_accuracy: 0.5287\n",
      "Epoch 495/1500\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0180 - accuracy: 0.9935 - val_loss: 2.9864 - val_accuracy: 0.5650\n",
      "Epoch 496/1500\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0307 - accuracy: 0.9943 - val_loss: 2.8805 - val_accuracy: 0.5529\n",
      "Epoch 497/1500\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0129 - accuracy: 0.9967 - val_loss: 2.9690 - val_accuracy: 0.5529\n",
      "Epoch 498/1500\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0206 - accuracy: 0.9927 - val_loss: 3.1988 - val_accuracy: 0.5498\n",
      "Epoch 499/1500\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0234 - accuracy: 0.9927 - val_loss: 3.1728 - val_accuracy: 0.5438\n",
      "Epoch 500/1500\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0166 - accuracy: 0.9951 - val_loss: 3.1738 - val_accuracy: 0.5468\n",
      "Epoch 501/1500\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0269 - accuracy: 0.9927 - val_loss: 3.1072 - val_accuracy: 0.5529\n",
      "Epoch 502/1500\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0085 - accuracy: 0.9976 - val_loss: 3.1152 - val_accuracy: 0.5680\n",
      "Epoch 503/1500\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0119 - accuracy: 0.9959 - val_loss: 2.9509 - val_accuracy: 0.5468\n",
      "Epoch 504/1500\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0182 - accuracy: 0.9935 - val_loss: 2.8782 - val_accuracy: 0.5498\n",
      "Epoch 505/1500\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0154 - accuracy: 0.9927 - val_loss: 3.1408 - val_accuracy: 0.5408\n",
      "Epoch 506/1500\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0660 - accuracy: 0.9902 - val_loss: 3.2634 - val_accuracy: 0.5498\n",
      "Epoch 507/1500\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0245 - accuracy: 0.9919 - val_loss: 3.2772 - val_accuracy: 0.5468\n",
      "Epoch 508/1500\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0070 - accuracy: 0.9984 - val_loss: 3.1913 - val_accuracy: 0.5559\n",
      "Epoch 509/1500\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0291 - accuracy: 0.9919 - val_loss: 3.2214 - val_accuracy: 0.5529\n",
      "Epoch 510/1500\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0139 - accuracy: 0.9959 - val_loss: 3.1717 - val_accuracy: 0.5529\n",
      "Epoch 511/1500\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0206 - accuracy: 0.9967 - val_loss: 3.1750 - val_accuracy: 0.5468\n",
      "Epoch 512/1500\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0302 - accuracy: 0.9943 - val_loss: 3.1669 - val_accuracy: 0.5529\n",
      "Epoch 513/1500\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0161 - accuracy: 0.9919 - val_loss: 3.1892 - val_accuracy: 0.5559\n",
      "Epoch 514/1500\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0058 - accuracy: 0.9984 - val_loss: 3.1997 - val_accuracy: 0.5710\n",
      "Epoch 515/1500\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0060 - accuracy: 0.9976 - val_loss: 3.1797 - val_accuracy: 0.5257\n",
      "Epoch 516/1500\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0222 - accuracy: 0.9935 - val_loss: 3.2223 - val_accuracy: 0.5650\n",
      "Epoch 517/1500\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0104 - accuracy: 0.9976 - val_loss: 3.2259 - val_accuracy: 0.5498\n",
      "Epoch 518/1500\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0048 - accuracy: 0.9992 - val_loss: 2.9138 - val_accuracy: 0.5619\n",
      "Epoch 519/1500\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0392 - accuracy: 0.9943 - val_loss: 2.9175 - val_accuracy: 0.5438\n",
      "Epoch 520/1500\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9992 - val_loss: 2.9219 - val_accuracy: 0.5619\n",
      "Epoch 521/1500\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.9951 - val_accuracy: 0.5438\n",
      "Epoch 522/1500\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.9581 - val_accuracy: 0.5347\n",
      "Epoch 523/1500\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0149 - accuracy: 0.9935 - val_loss: 2.9376 - val_accuracy: 0.5408\n",
      "Epoch 524/1500\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0146 - accuracy: 0.9943 - val_loss: 2.9243 - val_accuracy: 0.5438\n",
      "Epoch 525/1500\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0159 - accuracy: 0.9943 - val_loss: 2.9535 - val_accuracy: 0.5438\n",
      "Epoch 526/1500\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0243 - accuracy: 0.9951 - val_loss: 2.9478 - val_accuracy: 0.5589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 527/1500\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0166 - accuracy: 0.9951 - val_loss: 2.9306 - val_accuracy: 0.5529\n",
      "Epoch 528/1500\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0282 - accuracy: 0.9943 - val_loss: 2.9040 - val_accuracy: 0.5740\n",
      "Epoch 529/1500\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0252 - accuracy: 0.9935 - val_loss: 2.9776 - val_accuracy: 0.5559\n",
      "Epoch 530/1500\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0091 - accuracy: 0.9976 - val_loss: 2.9490 - val_accuracy: 0.5589\n",
      "Epoch 531/1500\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0517 - accuracy: 0.9935 - val_loss: 2.9697 - val_accuracy: 0.5498\n",
      "Epoch 532/1500\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0283 - accuracy: 0.9902 - val_loss: 3.0144 - val_accuracy: 0.5438\n",
      "Epoch 533/1500\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0142 - accuracy: 0.9927 - val_loss: 2.9275 - val_accuracy: 0.5589\n",
      "Epoch 534/1500\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0082 - accuracy: 0.9976 - val_loss: 2.9814 - val_accuracy: 0.5468\n",
      "Epoch 535/1500\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0062 - accuracy: 0.9984 - val_loss: 3.1053 - val_accuracy: 0.5408\n",
      "Epoch 536/1500\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0152 - accuracy: 0.9935 - val_loss: 2.9563 - val_accuracy: 0.5619\n",
      "Epoch 537/1500\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0138 - accuracy: 0.9959 - val_loss: 3.2678 - val_accuracy: 0.5408\n",
      "Epoch 538/1500\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0125 - accuracy: 0.9951 - val_loss: 3.1613 - val_accuracy: 0.5408\n",
      "Epoch 539/1500\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0383 - accuracy: 0.9919 - val_loss: 3.0850 - val_accuracy: 0.5347\n",
      "Epoch 540/1500\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0084 - accuracy: 0.9967 - val_loss: 2.9513 - val_accuracy: 0.5468\n",
      "Epoch 541/1500\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0214 - accuracy: 0.9943 - val_loss: 3.0161 - val_accuracy: 0.5468\n",
      "Epoch 542/1500\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0130 - accuracy: 0.9959 - val_loss: 2.9604 - val_accuracy: 0.5559\n",
      "Epoch 543/1500\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0142 - accuracy: 0.9967 - val_loss: 2.9789 - val_accuracy: 0.5498\n",
      "Epoch 544/1500\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0101 - accuracy: 0.9976 - val_loss: 2.9966 - val_accuracy: 0.5347\n",
      "Epoch 545/1500\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0150 - accuracy: 0.9959 - val_loss: 3.1195 - val_accuracy: 0.5498\n",
      "Epoch 546/1500\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0208 - accuracy: 0.9919 - val_loss: 2.9814 - val_accuracy: 0.5468\n",
      "Epoch 547/1500\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0217 - accuracy: 0.9919 - val_loss: 2.9865 - val_accuracy: 0.5378\n",
      "Epoch 548/1500\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0091 - accuracy: 0.9967 - val_loss: 3.0453 - val_accuracy: 0.5438\n",
      "Epoch 549/1500\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0090 - accuracy: 0.9967 - val_loss: 3.0118 - val_accuracy: 0.5378\n",
      "Epoch 550/1500\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0357 - accuracy: 0.9935 - val_loss: 2.9372 - val_accuracy: 0.5317\n",
      "Epoch 551/1500\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0140 - accuracy: 0.9943 - val_loss: 3.0350 - val_accuracy: 0.5408\n",
      "Epoch 552/1500\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0140 - accuracy: 0.9951 - val_loss: 3.1021 - val_accuracy: 0.5257\n",
      "Epoch 553/1500\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0608 - accuracy: 0.9927 - val_loss: 3.1413 - val_accuracy: 0.5196\n",
      "Epoch 554/1500\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0120 - accuracy: 0.9984 - val_loss: 3.1633 - val_accuracy: 0.5378\n",
      "Epoch 555/1500\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0091 - accuracy: 0.9976 - val_loss: 3.3250 - val_accuracy: 0.5378\n",
      "Epoch 556/1500\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0115 - accuracy: 0.9959 - val_loss: 3.1410 - val_accuracy: 0.5317\n",
      "Epoch 557/1500\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0057 - accuracy: 0.9992 - val_loss: 3.0813 - val_accuracy: 0.5347\n",
      "Epoch 558/1500\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0088 - accuracy: 0.9959 - val_loss: 3.0085 - val_accuracy: 0.5257\n",
      "Epoch 559/1500\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0155 - accuracy: 0.9967 - val_loss: 2.9775 - val_accuracy: 0.5317\n",
      "Epoch 560/1500\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0109 - accuracy: 0.9959 - val_loss: 3.0487 - val_accuracy: 0.5257\n",
      "Epoch 561/1500\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0069 - accuracy: 0.9992 - val_loss: 3.0143 - val_accuracy: 0.5378\n",
      "Epoch 562/1500\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0151 - accuracy: 0.9959 - val_loss: 3.3186 - val_accuracy: 0.5378\n",
      "Epoch 563/1500\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0098 - accuracy: 0.9967 - val_loss: 3.0206 - val_accuracy: 0.5468\n",
      "Epoch 564/1500\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0523 - accuracy: 0.9919 - val_loss: 3.1551 - val_accuracy: 0.5378\n",
      "Epoch 565/1500\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0139 - accuracy: 0.9935 - val_loss: 3.1362 - val_accuracy: 0.5408\n",
      "Epoch 566/1500\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0095 - accuracy: 0.9967 - val_loss: 3.1822 - val_accuracy: 0.5227\n",
      "Epoch 567/1500\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0138 - accuracy: 0.9959 - val_loss: 3.1541 - val_accuracy: 0.5287\n",
      "Epoch 568/1500\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0113 - accuracy: 0.9984 - val_loss: 3.2182 - val_accuracy: 0.5438\n",
      "Epoch 569/1500\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0128 - accuracy: 0.9959 - val_loss: 3.2153 - val_accuracy: 0.5317\n",
      "Epoch 570/1500\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0132 - accuracy: 0.9951 - val_loss: 3.2664 - val_accuracy: 0.5347\n",
      "Epoch 571/1500\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0133 - accuracy: 0.9951 - val_loss: 3.0351 - val_accuracy: 0.5498\n",
      "Epoch 572/1500\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0093 - accuracy: 0.9984 - val_loss: 3.1315 - val_accuracy: 0.5408\n",
      "Epoch 573/1500\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0140 - accuracy: 0.9959 - val_loss: 3.1277 - val_accuracy: 0.5498\n",
      "Epoch 574/1500\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0160 - accuracy: 0.9943 - val_loss: 3.1890 - val_accuracy: 0.5438\n",
      "Epoch 575/1500\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0078 - accuracy: 0.9976 - val_loss: 2.9720 - val_accuracy: 0.5468\n",
      "Epoch 576/1500\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0086 - accuracy: 0.9976 - val_loss: 3.0129 - val_accuracy: 0.5529\n",
      "Epoch 577/1500\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.0380 - val_accuracy: 0.5468\n",
      "Epoch 578/1500\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0089 - accuracy: 0.9959 - val_loss: 3.0541 - val_accuracy: 0.5529\n",
      "Epoch 579/1500\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0064 - accuracy: 0.9967 - val_loss: 3.3691 - val_accuracy: 0.5257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 580/1500\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0151 - accuracy: 0.9951 - val_loss: 3.2274 - val_accuracy: 0.5166\n",
      "Epoch 581/1500\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0083 - accuracy: 0.9967 - val_loss: 3.1505 - val_accuracy: 0.5498\n",
      "Epoch 582/1500\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0159 - accuracy: 0.9951 - val_loss: 3.1309 - val_accuracy: 0.5529\n",
      "Epoch 583/1500\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0034 - accuracy: 0.9992 - val_loss: 3.0261 - val_accuracy: 0.5589\n",
      "Epoch 584/1500\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0037 - accuracy: 0.9984 - val_loss: 3.0572 - val_accuracy: 0.5559\n",
      "Epoch 585/1500\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0181 - accuracy: 0.9959 - val_loss: 3.1100 - val_accuracy: 0.5559\n",
      "Epoch 586/1500\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0325 - accuracy: 0.9910 - val_loss: 3.1899 - val_accuracy: 0.5468\n",
      "Epoch 587/1500\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0128 - accuracy: 0.9951 - val_loss: 3.1141 - val_accuracy: 0.5257\n",
      "Epoch 588/1500\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0121 - accuracy: 0.9976 - val_loss: 3.0632 - val_accuracy: 0.5468\n",
      "Epoch 589/1500\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0185 - accuracy: 0.9943 - val_loss: 3.1627 - val_accuracy: 0.5498\n",
      "Epoch 590/1500\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0156 - accuracy: 0.9951 - val_loss: 3.0553 - val_accuracy: 0.5438\n",
      "Epoch 591/1500\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0057 - accuracy: 0.9976 - val_loss: 3.0942 - val_accuracy: 0.5529\n",
      "Epoch 592/1500\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0176 - accuracy: 0.9943 - val_loss: 3.1397 - val_accuracy: 0.5317\n",
      "Epoch 593/1500\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0119 - accuracy: 0.9967 - val_loss: 3.1786 - val_accuracy: 0.5468\n",
      "Epoch 594/1500\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0176 - accuracy: 0.9951 - val_loss: 3.3871 - val_accuracy: 0.5408\n",
      "Epoch 595/1500\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 3.1623 - val_accuracy: 0.5529\n",
      "Epoch 596/1500\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0134 - accuracy: 0.9967 - val_loss: 3.1503 - val_accuracy: 0.5529\n",
      "Epoch 597/1500\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0115 - accuracy: 0.9967 - val_loss: 3.1707 - val_accuracy: 0.5378\n",
      "Epoch 598/1500\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0284 - accuracy: 0.9935 - val_loss: 3.0704 - val_accuracy: 0.5498\n",
      "Epoch 599/1500\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0180 - accuracy: 0.9935 - val_loss: 3.0799 - val_accuracy: 0.5468\n",
      "Epoch 600/1500\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0121 - accuracy: 0.9967 - val_loss: 3.0726 - val_accuracy: 0.5650\n",
      "Epoch 601/1500\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0061 - accuracy: 0.9976 - val_loss: 3.1661 - val_accuracy: 0.5589\n",
      "Epoch 602/1500\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0171 - accuracy: 0.9935 - val_loss: 3.1031 - val_accuracy: 0.5438\n",
      "Epoch 603/1500\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0132 - accuracy: 0.9943 - val_loss: 3.1889 - val_accuracy: 0.5559\n",
      "Epoch 604/1500\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0081 - accuracy: 0.9984 - val_loss: 3.3214 - val_accuracy: 0.5529\n",
      "Epoch 605/1500\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0162 - accuracy: 0.9935 - val_loss: 3.1941 - val_accuracy: 0.5559\n",
      "Epoch 606/1500\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0374 - accuracy: 0.9967 - val_loss: 3.1209 - val_accuracy: 0.5378\n",
      "Epoch 607/1500\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0144 - accuracy: 0.9967 - val_loss: 2.9920 - val_accuracy: 0.5589\n",
      "Epoch 608/1500\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.0270 - val_accuracy: 0.5619\n",
      "Epoch 609/1500\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0164 - accuracy: 0.9943 - val_loss: 3.1574 - val_accuracy: 0.5650\n",
      "Epoch 610/1500\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0171 - accuracy: 0.9943 - val_loss: 3.1783 - val_accuracy: 0.5589\n",
      "Epoch 611/1500\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0061 - accuracy: 0.9984 - val_loss: 3.3629 - val_accuracy: 0.5408\n",
      "Epoch 612/1500\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0085 - accuracy: 0.9976 - val_loss: 3.3144 - val_accuracy: 0.5529\n",
      "Epoch 613/1500\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0057 - accuracy: 0.9967 - val_loss: 3.2033 - val_accuracy: 0.5619\n",
      "Epoch 614/1500\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0169 - accuracy: 0.9951 - val_loss: 3.4360 - val_accuracy: 0.5408\n",
      "Epoch 615/1500\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0239 - accuracy: 0.9927 - val_loss: 3.3237 - val_accuracy: 0.5650\n",
      "Epoch 616/1500\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0251 - accuracy: 0.9951 - val_loss: 3.3335 - val_accuracy: 0.5347\n",
      "Epoch 617/1500\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0239 - accuracy: 0.9943 - val_loss: 3.3518 - val_accuracy: 0.5498\n",
      "Epoch 618/1500\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0127 - accuracy: 0.9967 - val_loss: 3.3034 - val_accuracy: 0.5529\n",
      "Epoch 619/1500\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0084 - accuracy: 0.9984 - val_loss: 3.3073 - val_accuracy: 0.5498\n",
      "Epoch 620/1500\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0195 - accuracy: 0.9902 - val_loss: 3.1727 - val_accuracy: 0.5498\n",
      "Epoch 621/1500\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0071 - accuracy: 0.9976 - val_loss: 3.2902 - val_accuracy: 0.5680\n",
      "Epoch 622/1500\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0220 - accuracy: 0.9935 - val_loss: 3.0800 - val_accuracy: 0.5589\n",
      "Epoch 623/1500\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0107 - accuracy: 0.9967 - val_loss: 3.2224 - val_accuracy: 0.5378\n",
      "Epoch 624/1500\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0145 - accuracy: 0.9959 - val_loss: 3.3530 - val_accuracy: 0.5468\n",
      "Epoch 625/1500\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0105 - accuracy: 0.9967 - val_loss: 3.3758 - val_accuracy: 0.5529\n",
      "Epoch 626/1500\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0136 - accuracy: 0.9943 - val_loss: 3.2891 - val_accuracy: 0.5468\n",
      "Epoch 627/1500\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0046 - accuracy: 0.9992 - val_loss: 3.3019 - val_accuracy: 0.5589\n",
      "Epoch 628/1500\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0136 - accuracy: 0.9976 - val_loss: 3.3893 - val_accuracy: 0.5468\n",
      "Epoch 629/1500\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0183 - accuracy: 0.9951 - val_loss: 3.4148 - val_accuracy: 0.5468\n",
      "Epoch 630/1500\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0090 - accuracy: 0.9984 - val_loss: 3.3421 - val_accuracy: 0.5559\n",
      "Epoch 631/1500\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0133 - accuracy: 0.9959 - val_loss: 3.1111 - val_accuracy: 0.5559\n",
      "Epoch 632/1500\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0092 - accuracy: 0.9976 - val_loss: 3.2388 - val_accuracy: 0.5559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 633/1500\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0164 - accuracy: 0.9951 - val_loss: 3.2375 - val_accuracy: 0.5589\n",
      "Epoch 634/1500\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0141 - accuracy: 0.9976 - val_loss: 3.2573 - val_accuracy: 0.5498\n",
      "Epoch 635/1500\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0225 - accuracy: 0.9967 - val_loss: 3.2313 - val_accuracy: 0.5498\n",
      "Epoch 636/1500\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0056 - accuracy: 0.9992 - val_loss: 3.2403 - val_accuracy: 0.5498\n",
      "Epoch 637/1500\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0057 - accuracy: 0.9967 - val_loss: 3.2172 - val_accuracy: 0.5589\n",
      "Epoch 638/1500\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0073 - accuracy: 0.9984 - val_loss: 3.2628 - val_accuracy: 0.5498\n",
      "Epoch 639/1500\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0103 - accuracy: 0.9951 - val_loss: 3.2723 - val_accuracy: 0.5589\n",
      "Epoch 640/1500\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0110 - accuracy: 0.9967 - val_loss: 3.5614 - val_accuracy: 0.5378\n",
      "Epoch 641/1500\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0059 - accuracy: 0.9984 - val_loss: 3.5804 - val_accuracy: 0.5347\n",
      "Epoch 642/1500\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0215 - accuracy: 0.9943 - val_loss: 3.4779 - val_accuracy: 0.5498\n",
      "Epoch 643/1500\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0090 - accuracy: 0.9976 - val_loss: 3.4927 - val_accuracy: 0.5529\n",
      "Epoch 644/1500\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0096 - accuracy: 0.9967 - val_loss: 3.3619 - val_accuracy: 0.5529\n",
      "Epoch 645/1500\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0123 - accuracy: 0.9951 - val_loss: 3.4760 - val_accuracy: 0.5468\n",
      "Epoch 646/1500\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0081 - accuracy: 0.9959 - val_loss: 3.3983 - val_accuracy: 0.5529\n",
      "Epoch 647/1500\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0086 - accuracy: 0.9976 - val_loss: 3.5261 - val_accuracy: 0.5529\n",
      "Epoch 648/1500\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0046 - accuracy: 0.9992 - val_loss: 3.4432 - val_accuracy: 0.5559\n",
      "Epoch 649/1500\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0204 - accuracy: 0.9935 - val_loss: 3.3755 - val_accuracy: 0.5619\n",
      "Epoch 650/1500\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0116 - accuracy: 0.9959 - val_loss: 3.3105 - val_accuracy: 0.5589\n",
      "Epoch 651/1500\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0060 - accuracy: 0.9976 - val_loss: 3.1978 - val_accuracy: 0.5498\n",
      "Epoch 652/1500\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0116 - accuracy: 0.9984 - val_loss: 3.2773 - val_accuracy: 0.5559\n",
      "Epoch 653/1500\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0084 - accuracy: 0.9967 - val_loss: 3.2535 - val_accuracy: 0.5408\n",
      "Epoch 654/1500\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0222 - accuracy: 0.9976 - val_loss: 3.2773 - val_accuracy: 0.5529\n",
      "Epoch 655/1500\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0073 - accuracy: 0.9992 - val_loss: 3.3070 - val_accuracy: 0.5559\n",
      "Epoch 656/1500\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1315 - accuracy: 0.9919 - val_loss: 3.2722 - val_accuracy: 0.5408\n",
      "Epoch 657/1500\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0326 - accuracy: 0.9951 - val_loss: 3.3986 - val_accuracy: 0.5468\n",
      "Epoch 658/1500\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0112 - accuracy: 0.9959 - val_loss: 3.5229 - val_accuracy: 0.5619\n",
      "Epoch 659/1500\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0097 - accuracy: 0.9976 - val_loss: 3.6307 - val_accuracy: 0.5438\n",
      "Epoch 660/1500\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0123 - accuracy: 0.9943 - val_loss: 3.4159 - val_accuracy: 0.5408\n",
      "Epoch 661/1500\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0143 - accuracy: 0.9959 - val_loss: 3.4829 - val_accuracy: 0.5498\n",
      "Epoch 662/1500\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0121 - accuracy: 0.9943 - val_loss: 3.5045 - val_accuracy: 0.5498\n",
      "Epoch 663/1500\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0178 - accuracy: 0.9943 - val_loss: 3.4379 - val_accuracy: 0.5347\n",
      "Epoch 664/1500\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0148 - accuracy: 0.9951 - val_loss: 3.4269 - val_accuracy: 0.5408\n",
      "Epoch 665/1500\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0161 - accuracy: 0.9943 - val_loss: 3.4866 - val_accuracy: 0.5257\n",
      "Epoch 666/1500\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0141 - accuracy: 0.9959 - val_loss: 3.3200 - val_accuracy: 0.5529\n",
      "Epoch 667/1500\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0070 - accuracy: 0.9976 - val_loss: 3.3487 - val_accuracy: 0.5589\n",
      "Epoch 668/1500\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.3413 - val_accuracy: 0.5468\n",
      "Epoch 669/1500\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0076 - accuracy: 0.9984 - val_loss: 3.2477 - val_accuracy: 0.5498\n",
      "Epoch 670/1500\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0160 - accuracy: 0.9967 - val_loss: 3.2018 - val_accuracy: 0.5438\n",
      "Epoch 671/1500\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0080 - accuracy: 0.9967 - val_loss: 3.3756 - val_accuracy: 0.5166\n",
      "Epoch 672/1500\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0087 - accuracy: 0.9967 - val_loss: 3.2427 - val_accuracy: 0.5559\n",
      "Epoch 673/1500\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0332 - accuracy: 0.9967 - val_loss: 3.3081 - val_accuracy: 0.5378\n",
      "Epoch 674/1500\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0169 - accuracy: 0.9943 - val_loss: 3.4141 - val_accuracy: 0.5347\n",
      "Epoch 675/1500\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0100 - accuracy: 0.9967 - val_loss: 3.2409 - val_accuracy: 0.5498\n",
      "Epoch 676/1500\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 3.3352 - val_accuracy: 0.5347\n",
      "Epoch 677/1500\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0142 - accuracy: 0.9959 - val_loss: 3.3132 - val_accuracy: 0.5378\n",
      "Epoch 678/1500\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0060 - accuracy: 0.9976 - val_loss: 3.3666 - val_accuracy: 0.5498\n",
      "Epoch 679/1500\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0045 - accuracy: 0.9992 - val_loss: 3.3432 - val_accuracy: 0.5408\n",
      "Epoch 680/1500\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0091 - accuracy: 0.9976 - val_loss: 3.2706 - val_accuracy: 0.5559\n",
      "Epoch 681/1500\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0141 - accuracy: 0.9951 - val_loss: 3.3259 - val_accuracy: 0.5378\n",
      "Epoch 682/1500\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0068 - accuracy: 0.9992 - val_loss: 3.5548 - val_accuracy: 0.5317\n",
      "Epoch 683/1500\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0025 - accuracy: 0.9992 - val_loss: 3.3371 - val_accuracy: 0.5438\n",
      "Epoch 684/1500\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0131 - accuracy: 0.9959 - val_loss: 3.1879 - val_accuracy: 0.5438\n",
      "Epoch 685/1500\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0102 - accuracy: 0.9959 - val_loss: 3.4305 - val_accuracy: 0.5378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 686/1500\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0135 - accuracy: 0.9951 - val_loss: 3.4068 - val_accuracy: 0.5468\n",
      "Epoch 687/1500\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0073 - accuracy: 0.9976 - val_loss: 3.4726 - val_accuracy: 0.5468\n",
      "Epoch 688/1500\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0059 - accuracy: 0.9976 - val_loss: 3.5721 - val_accuracy: 0.5347\n",
      "Epoch 689/1500\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0158 - accuracy: 0.9951 - val_loss: 3.4476 - val_accuracy: 0.5408\n",
      "Epoch 690/1500\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0150 - accuracy: 0.9927 - val_loss: 3.4712 - val_accuracy: 0.5498\n",
      "Epoch 691/1500\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0063 - accuracy: 0.9967 - val_loss: 3.3199 - val_accuracy: 0.5257\n",
      "Epoch 692/1500\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0101 - accuracy: 0.9967 - val_loss: 3.2557 - val_accuracy: 0.5498\n",
      "Epoch 693/1500\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0105 - accuracy: 0.9967 - val_loss: 3.5076 - val_accuracy: 0.5378\n",
      "Epoch 694/1500\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.7885 - val_accuracy: 0.5498\n",
      "Epoch 695/1500\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0031 - accuracy: 0.9984 - val_loss: 3.4769 - val_accuracy: 0.5408\n",
      "Epoch 696/1500\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 3.3232 - val_accuracy: 0.5498\n",
      "Epoch 697/1500\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0103 - accuracy: 0.9967 - val_loss: 3.3349 - val_accuracy: 0.5378\n",
      "Epoch 698/1500\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9992 - val_loss: 3.4103 - val_accuracy: 0.5408\n",
      "Epoch 699/1500\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0117 - accuracy: 0.9951 - val_loss: 3.3091 - val_accuracy: 0.5498\n",
      "Epoch 700/1500\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0115 - accuracy: 0.9976 - val_loss: 3.4498 - val_accuracy: 0.5438\n",
      "Epoch 701/1500\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0222 - accuracy: 0.9951 - val_loss: 3.3688 - val_accuracy: 0.5559\n",
      "Epoch 702/1500\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.1955 - val_accuracy: 0.5529\n",
      "Epoch 703/1500\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0125 - accuracy: 0.9967 - val_loss: 3.2354 - val_accuracy: 0.5559\n",
      "Epoch 704/1500\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0057 - accuracy: 0.9976 - val_loss: 3.2303 - val_accuracy: 0.5468\n",
      "Epoch 705/1500\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0040 - accuracy: 0.9984 - val_loss: 3.3329 - val_accuracy: 0.5498\n",
      "Epoch 706/1500\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0194 - accuracy: 0.9935 - val_loss: 3.4702 - val_accuracy: 0.5529\n",
      "Epoch 707/1500\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0034 - accuracy: 0.9992 - val_loss: 3.4283 - val_accuracy: 0.5468\n",
      "Epoch 708/1500\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0089 - accuracy: 0.9984 - val_loss: 3.3500 - val_accuracy: 0.5498\n",
      "Epoch 709/1500\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0044 - accuracy: 0.9984 - val_loss: 3.3072 - val_accuracy: 0.5438\n",
      "Epoch 710/1500\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0107 - accuracy: 0.9951 - val_loss: 3.2123 - val_accuracy: 0.5498\n",
      "Epoch 711/1500\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0075 - accuracy: 0.9984 - val_loss: 3.5070 - val_accuracy: 0.5498\n",
      "Epoch 712/1500\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0069 - accuracy: 0.9984 - val_loss: 3.2831 - val_accuracy: 0.5378\n",
      "Epoch 713/1500\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0073 - accuracy: 0.9984 - val_loss: 3.2124 - val_accuracy: 0.5468\n",
      "Epoch 714/1500\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.3009 - val_accuracy: 0.5529\n",
      "Epoch 715/1500\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0256 - accuracy: 0.9984 - val_loss: 3.2619 - val_accuracy: 0.5619\n",
      "Epoch 716/1500\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0068 - accuracy: 0.9976 - val_loss: 3.4531 - val_accuracy: 0.5408\n",
      "Epoch 717/1500\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0069 - accuracy: 0.9967 - val_loss: 3.5407 - val_accuracy: 0.5438\n",
      "Epoch 718/1500\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0390 - accuracy: 0.9951 - val_loss: 3.4207 - val_accuracy: 0.5468\n",
      "Epoch 719/1500\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0106 - accuracy: 0.9959 - val_loss: 3.5359 - val_accuracy: 0.5166\n",
      "Epoch 720/1500\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0052 - accuracy: 0.9984 - val_loss: 3.3759 - val_accuracy: 0.5378\n",
      "Epoch 721/1500\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0235 - accuracy: 0.9927 - val_loss: 3.3653 - val_accuracy: 0.5650\n",
      "Epoch 722/1500\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0081 - accuracy: 0.9976 - val_loss: 3.3736 - val_accuracy: 0.5619\n",
      "Epoch 723/1500\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0082 - accuracy: 0.9976 - val_loss: 3.3567 - val_accuracy: 0.5559\n",
      "Epoch 724/1500\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0056 - accuracy: 0.9984 - val_loss: 3.4706 - val_accuracy: 0.5498\n",
      "Epoch 725/1500\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0106 - accuracy: 0.9951 - val_loss: 3.5912 - val_accuracy: 0.5559\n",
      "Epoch 726/1500\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 3.3492 - val_accuracy: 0.5468\n",
      "Epoch 727/1500\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0170 - accuracy: 0.9935 - val_loss: 3.3026 - val_accuracy: 0.5650\n",
      "Epoch 728/1500\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0031 - accuracy: 0.9992 - val_loss: 3.5753 - val_accuracy: 0.5529\n",
      "Epoch 729/1500\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0098 - accuracy: 0.9959 - val_loss: 3.5815 - val_accuracy: 0.5498\n",
      "Epoch 730/1500\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0296 - accuracy: 0.9902 - val_loss: 3.3947 - val_accuracy: 0.5589\n",
      "Epoch 731/1500\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.3784 - val_accuracy: 0.5559\n",
      "Epoch 732/1500\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0046 - accuracy: 0.9992 - val_loss: 3.5165 - val_accuracy: 0.5498\n",
      "Epoch 733/1500\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0117 - accuracy: 0.9976 - val_loss: 3.5795 - val_accuracy: 0.5498\n",
      "Epoch 734/1500\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0278 - accuracy: 0.9959 - val_loss: 3.6593 - val_accuracy: 0.5619\n",
      "Epoch 735/1500\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0079 - accuracy: 0.9943 - val_loss: 3.6320 - val_accuracy: 0.5408\n",
      "Epoch 736/1500\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0074 - accuracy: 0.9959 - val_loss: 3.5451 - val_accuracy: 0.5498\n",
      "Epoch 737/1500\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0057 - accuracy: 0.9992 - val_loss: 3.7291 - val_accuracy: 0.5619\n",
      "Epoch 738/1500\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0440 - accuracy: 0.9943 - val_loss: 3.7227 - val_accuracy: 0.5498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 739/1500\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0076 - accuracy: 0.9967 - val_loss: 3.6121 - val_accuracy: 0.5559\n",
      "Epoch 740/1500\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0340 - accuracy: 0.9976 - val_loss: 3.4539 - val_accuracy: 0.5408\n",
      "Epoch 741/1500\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0042 - accuracy: 0.9976 - val_loss: 3.5565 - val_accuracy: 0.5438\n",
      "Epoch 742/1500\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0427 - accuracy: 0.9919 - val_loss: 3.5874 - val_accuracy: 0.5498\n",
      "Epoch 743/1500\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0118 - accuracy: 0.9967 - val_loss: 3.3571 - val_accuracy: 0.5650\n",
      "Epoch 744/1500\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0097 - accuracy: 0.9959 - val_loss: 3.3649 - val_accuracy: 0.5619\n",
      "Epoch 745/1500\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0084 - accuracy: 0.9976 - val_loss: 3.4056 - val_accuracy: 0.5559\n",
      "Epoch 746/1500\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0082 - accuracy: 0.9959 - val_loss: 3.7623 - val_accuracy: 0.5468\n",
      "Epoch 747/1500\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0099 - accuracy: 0.9951 - val_loss: 3.4174 - val_accuracy: 0.5498\n",
      "Epoch 748/1500\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0090 - accuracy: 0.9951 - val_loss: 3.4982 - val_accuracy: 0.5468\n",
      "Epoch 749/1500\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0060 - accuracy: 0.9959 - val_loss: 3.5145 - val_accuracy: 0.5529\n",
      "Epoch 750/1500\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0094 - accuracy: 0.9951 - val_loss: 3.4812 - val_accuracy: 0.5680\n",
      "Epoch 751/1500\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0202 - accuracy: 0.9943 - val_loss: 3.4305 - val_accuracy: 0.5710\n",
      "Epoch 752/1500\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0147 - accuracy: 0.9951 - val_loss: 3.7553 - val_accuracy: 0.5740\n",
      "Epoch 753/1500\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0058 - accuracy: 0.9984 - val_loss: 3.7255 - val_accuracy: 0.5650\n",
      "Epoch 754/1500\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0087 - accuracy: 0.9976 - val_loss: 3.5392 - val_accuracy: 0.5559\n",
      "Epoch 755/1500\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0061 - accuracy: 0.9976 - val_loss: 3.6221 - val_accuracy: 0.5650\n",
      "Epoch 756/1500\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0122 - accuracy: 0.9959 - val_loss: 3.8930 - val_accuracy: 0.5257\n",
      "Epoch 757/1500\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0060 - accuracy: 0.9984 - val_loss: 3.5244 - val_accuracy: 0.5619\n",
      "Epoch 758/1500\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0072 - accuracy: 0.9976 - val_loss: 3.4996 - val_accuracy: 0.5468\n",
      "Epoch 759/1500\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0140 - accuracy: 0.9951 - val_loss: 3.5148 - val_accuracy: 0.5438\n",
      "Epoch 760/1500\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0046 - accuracy: 0.9992 - val_loss: 3.5056 - val_accuracy: 0.5529\n",
      "Epoch 761/1500\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0072 - accuracy: 0.9984 - val_loss: 3.3712 - val_accuracy: 0.5468\n",
      "Epoch 762/1500\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0222 - accuracy: 0.9951 - val_loss: 3.3903 - val_accuracy: 0.5438\n",
      "Epoch 763/1500\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0040 - accuracy: 0.9984 - val_loss: 3.4544 - val_accuracy: 0.5589\n",
      "Epoch 764/1500\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0091 - accuracy: 0.9976 - val_loss: 3.5481 - val_accuracy: 0.5196\n",
      "Epoch 765/1500\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0042 - accuracy: 0.9976 - val_loss: 3.3696 - val_accuracy: 0.5468\n",
      "Epoch 766/1500\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0276 - accuracy: 0.9967 - val_loss: 3.5423 - val_accuracy: 0.5468\n",
      "Epoch 767/1500\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0124 - accuracy: 0.9967 - val_loss: 3.4619 - val_accuracy: 0.5650\n",
      "Epoch 768/1500\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0064 - accuracy: 0.9984 - val_loss: 3.5122 - val_accuracy: 0.5408\n",
      "Epoch 769/1500\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0305 - accuracy: 0.9951 - val_loss: 3.4673 - val_accuracy: 0.5378\n",
      "Epoch 770/1500\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0121 - accuracy: 0.9967 - val_loss: 3.4718 - val_accuracy: 0.5589\n",
      "Epoch 771/1500\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0037 - accuracy: 0.9976 - val_loss: 3.7159 - val_accuracy: 0.5317\n",
      "Epoch 772/1500\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0078 - accuracy: 0.9967 - val_loss: 3.6698 - val_accuracy: 0.5498\n",
      "Epoch 773/1500\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.6813 - val_accuracy: 0.5468\n",
      "Epoch 774/1500\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0081 - accuracy: 0.9976 - val_loss: 3.8980 - val_accuracy: 0.5468\n",
      "Epoch 775/1500\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0360 - accuracy: 0.9935 - val_loss: 3.7573 - val_accuracy: 0.5378\n",
      "Epoch 776/1500\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0065 - accuracy: 0.9984 - val_loss: 3.6642 - val_accuracy: 0.5468\n",
      "Epoch 777/1500\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0093 - accuracy: 0.9976 - val_loss: 3.5424 - val_accuracy: 0.5438\n",
      "Epoch 778/1500\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9992 - val_loss: 3.5585 - val_accuracy: 0.5498\n",
      "Epoch 779/1500\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0359 - accuracy: 0.9951 - val_loss: 3.5675 - val_accuracy: 0.5287\n",
      "Epoch 780/1500\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.4514 - val_accuracy: 0.5408\n",
      "Epoch 781/1500\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0139 - accuracy: 0.9943 - val_loss: 3.5085 - val_accuracy: 0.5529\n",
      "Epoch 782/1500\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0108 - accuracy: 0.9967 - val_loss: 3.4619 - val_accuracy: 0.5438\n",
      "Epoch 783/1500\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0296 - accuracy: 0.9976 - val_loss: 3.3357 - val_accuracy: 0.5498\n",
      "Epoch 784/1500\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0083 - accuracy: 0.9976 - val_loss: 3.6416 - val_accuracy: 0.5287\n",
      "Epoch 785/1500\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0084 - accuracy: 0.9967 - val_loss: 3.3774 - val_accuracy: 0.5378\n",
      "Epoch 786/1500\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0271 - accuracy: 0.9959 - val_loss: 3.4453 - val_accuracy: 0.5347\n",
      "Epoch 787/1500\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0303 - accuracy: 0.9951 - val_loss: 3.6412 - val_accuracy: 0.5408\n",
      "Epoch 788/1500\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 3.4153 - val_accuracy: 0.5408\n",
      "Epoch 789/1500\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0172 - accuracy: 0.9967 - val_loss: 3.5006 - val_accuracy: 0.5378\n",
      "Epoch 790/1500\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0239 - accuracy: 0.9959 - val_loss: 3.5613 - val_accuracy: 0.5438\n",
      "Epoch 791/1500\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0061 - accuracy: 0.9976 - val_loss: 3.4688 - val_accuracy: 0.5438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 792/1500\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0043 - accuracy: 0.9984 - val_loss: 3.5750 - val_accuracy: 0.5317\n",
      "Epoch 793/1500\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0151 - accuracy: 0.9951 - val_loss: 3.4874 - val_accuracy: 0.5438\n",
      "Epoch 794/1500\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0148 - accuracy: 0.9984 - val_loss: 3.4286 - val_accuracy: 0.5408\n",
      "Epoch 795/1500\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0036 - accuracy: 0.9992 - val_loss: 3.4280 - val_accuracy: 0.5559\n",
      "Epoch 796/1500\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0052 - accuracy: 0.9992 - val_loss: 3.3724 - val_accuracy: 0.5378\n",
      "Epoch 797/1500\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0095 - accuracy: 0.9992 - val_loss: 3.5571 - val_accuracy: 0.5498\n",
      "Epoch 798/1500\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0167 - accuracy: 0.9967 - val_loss: 3.5600 - val_accuracy: 0.5438\n",
      "Epoch 799/1500\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0050 - accuracy: 0.9992 - val_loss: 3.6536 - val_accuracy: 0.5438\n",
      "Epoch 800/1500\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0066 - accuracy: 0.9976 - val_loss: 3.5078 - val_accuracy: 0.5498\n",
      "Epoch 801/1500\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0100 - accuracy: 0.9943 - val_loss: 3.5776 - val_accuracy: 0.5378\n",
      "Epoch 802/1500\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.5314 - val_accuracy: 0.5378\n",
      "Epoch 803/1500\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0077 - accuracy: 0.9984 - val_loss: 3.5473 - val_accuracy: 0.5468\n",
      "Epoch 804/1500\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0138 - accuracy: 0.9959 - val_loss: 3.6175 - val_accuracy: 0.5408\n",
      "Epoch 805/1500\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.7471 - val_accuracy: 0.5438\n",
      "Epoch 806/1500\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0099 - accuracy: 0.9976 - val_loss: 3.7469 - val_accuracy: 0.5619\n",
      "Epoch 807/1500\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0114 - accuracy: 0.9976 - val_loss: 3.6456 - val_accuracy: 0.5498\n",
      "Epoch 808/1500\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0122 - accuracy: 0.9951 - val_loss: 3.7359 - val_accuracy: 0.5529\n",
      "Epoch 809/1500\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0036 - accuracy: 0.9984 - val_loss: 3.6018 - val_accuracy: 0.5559\n",
      "Epoch 810/1500\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0066 - accuracy: 0.9984 - val_loss: 3.6481 - val_accuracy: 0.5529\n",
      "Epoch 811/1500\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0082 - accuracy: 0.9967 - val_loss: 3.6888 - val_accuracy: 0.5287\n",
      "Epoch 812/1500\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0152 - accuracy: 0.9943 - val_loss: 3.6355 - val_accuracy: 0.5498\n",
      "Epoch 813/1500\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0360 - accuracy: 0.9919 - val_loss: 3.8785 - val_accuracy: 0.5317\n",
      "Epoch 814/1500\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0091 - accuracy: 0.9984 - val_loss: 3.7946 - val_accuracy: 0.5408\n",
      "Epoch 815/1500\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 9.6755e-04 - accuracy: 1.0000 - val_loss: 3.6185 - val_accuracy: 0.5619\n",
      "Epoch 816/1500\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0137 - accuracy: 0.9951 - val_loss: 3.7379 - val_accuracy: 0.5498\n",
      "Epoch 817/1500\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9984 - val_loss: 3.7414 - val_accuracy: 0.5438\n",
      "Epoch 818/1500\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0048 - accuracy: 0.9992 - val_loss: 3.7366 - val_accuracy: 0.5680\n",
      "Epoch 819/1500\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0131 - accuracy: 0.9951 - val_loss: 3.5276 - val_accuracy: 0.5498\n",
      "Epoch 820/1500\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0043 - accuracy: 0.9992 - val_loss: 3.5072 - val_accuracy: 0.5710\n",
      "Epoch 821/1500\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0030 - accuracy: 0.9992 - val_loss: 3.7230 - val_accuracy: 0.5740\n",
      "Epoch 822/1500\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0165 - accuracy: 0.9935 - val_loss: 3.6319 - val_accuracy: 0.5740\n",
      "Epoch 823/1500\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0105 - accuracy: 0.9951 - val_loss: 3.6900 - val_accuracy: 0.5559\n",
      "Epoch 824/1500\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0085 - accuracy: 0.9992 - val_loss: 3.6275 - val_accuracy: 0.5710\n",
      "Epoch 825/1500\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0224 - accuracy: 0.9959 - val_loss: 3.5409 - val_accuracy: 0.5650\n",
      "Epoch 826/1500\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.1021 - accuracy: 0.9935 - val_loss: 3.6482 - val_accuracy: 0.5589\n",
      "Epoch 827/1500\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0233 - accuracy: 0.9959 - val_loss: 3.7165 - val_accuracy: 0.5408\n",
      "Epoch 828/1500\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0050 - accuracy: 0.9992 - val_loss: 3.7533 - val_accuracy: 0.5589\n",
      "Epoch 829/1500\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0102 - accuracy: 0.9976 - val_loss: 3.6961 - val_accuracy: 0.5650\n",
      "Epoch 830/1500\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0144 - accuracy: 0.9943 - val_loss: 3.6948 - val_accuracy: 0.5589\n",
      "Epoch 831/1500\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0081 - accuracy: 0.9984 - val_loss: 3.7442 - val_accuracy: 0.5498\n",
      "Epoch 832/1500\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0322 - accuracy: 0.9919 - val_loss: 3.7662 - val_accuracy: 0.5498\n",
      "Epoch 833/1500\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0070 - accuracy: 0.9984 - val_loss: 3.6381 - val_accuracy: 0.5438\n",
      "Epoch 834/1500\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 3.9187 - val_accuracy: 0.5559\n",
      "Epoch 835/1500\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.6818 - val_accuracy: 0.5529\n",
      "Epoch 836/1500\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0103 - accuracy: 0.9943 - val_loss: 3.7721 - val_accuracy: 0.5619\n",
      "Epoch 837/1500\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0135 - accuracy: 0.9951 - val_loss: 3.8047 - val_accuracy: 0.5378\n",
      "Epoch 838/1500\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0038 - accuracy: 0.9984 - val_loss: 3.7199 - val_accuracy: 0.5408\n",
      "Epoch 839/1500\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0062 - accuracy: 0.9967 - val_loss: 3.6390 - val_accuracy: 0.5498\n",
      "Epoch 840/1500\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.6712 - val_accuracy: 0.5468\n",
      "Epoch 841/1500\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0098 - accuracy: 0.9967 - val_loss: 3.6269 - val_accuracy: 0.5408\n",
      "Epoch 842/1500\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0066 - accuracy: 0.9976 - val_loss: 3.6578 - val_accuracy: 0.5650\n",
      "Epoch 843/1500\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0061 - accuracy: 0.9992 - val_loss: 3.6661 - val_accuracy: 0.5559\n",
      "Epoch 844/1500\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0499 - accuracy: 0.9959 - val_loss: 3.4337 - val_accuracy: 0.5589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 845/1500\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0090 - accuracy: 0.9967 - val_loss: 3.6468 - val_accuracy: 0.5498\n",
      "Epoch 846/1500\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0033 - accuracy: 0.9984 - val_loss: 3.6016 - val_accuracy: 0.5498\n",
      "Epoch 847/1500\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0102 - accuracy: 0.9967 - val_loss: 3.6691 - val_accuracy: 0.5438\n",
      "Epoch 848/1500\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0077 - accuracy: 0.9976 - val_loss: 3.6612 - val_accuracy: 0.5347\n",
      "Epoch 849/1500\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0040 - accuracy: 0.9992 - val_loss: 3.5302 - val_accuracy: 0.5498\n",
      "Epoch 850/1500\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0097 - accuracy: 0.9984 - val_loss: 3.5153 - val_accuracy: 0.5529\n",
      "Epoch 851/1500\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0063 - accuracy: 0.9984 - val_loss: 3.5106 - val_accuracy: 0.5529\n",
      "Epoch 852/1500\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0046 - accuracy: 0.9992 - val_loss: 3.8296 - val_accuracy: 0.5438\n",
      "Epoch 853/1500\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0022 - accuracy: 0.9992 - val_loss: 3.5495 - val_accuracy: 0.5498\n",
      "Epoch 854/1500\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0041 - accuracy: 0.9984 - val_loss: 3.8109 - val_accuracy: 0.5589\n",
      "Epoch 855/1500\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9984 - val_loss: 3.5418 - val_accuracy: 0.5650\n",
      "Epoch 856/1500\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0088 - accuracy: 0.9967 - val_loss: 3.7035 - val_accuracy: 0.5378\n",
      "Epoch 857/1500\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0075 - accuracy: 0.9967 - val_loss: 3.5680 - val_accuracy: 0.5529\n",
      "Epoch 858/1500\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 3.4595 - val_accuracy: 0.5559\n",
      "Epoch 859/1500\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0073 - accuracy: 0.9992 - val_loss: 3.7711 - val_accuracy: 0.5589\n",
      "Epoch 860/1500\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0066 - accuracy: 0.9976 - val_loss: 3.8130 - val_accuracy: 0.5438\n",
      "Epoch 861/1500\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0192 - accuracy: 0.9943 - val_loss: 3.8521 - val_accuracy: 0.5559\n",
      "Epoch 862/1500\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0172 - accuracy: 0.9976 - val_loss: 3.5150 - val_accuracy: 0.5619\n",
      "Epoch 863/1500\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0046 - accuracy: 0.9984 - val_loss: 3.4345 - val_accuracy: 0.5650\n",
      "Epoch 864/1500\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0056 - accuracy: 0.9976 - val_loss: 3.4746 - val_accuracy: 0.5619\n",
      "Epoch 865/1500\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0063 - accuracy: 0.9976 - val_loss: 3.7358 - val_accuracy: 0.5529\n",
      "Epoch 866/1500\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0107 - accuracy: 0.9984 - val_loss: 3.4402 - val_accuracy: 0.5589\n",
      "Epoch 867/1500\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0016 - accuracy: 0.9992 - val_loss: 3.4947 - val_accuracy: 0.5740\n",
      "Epoch 868/1500\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0464 - accuracy: 0.9935 - val_loss: 3.5264 - val_accuracy: 0.5619\n",
      "Epoch 869/1500\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0116 - accuracy: 0.9984 - val_loss: 3.6376 - val_accuracy: 0.5710\n",
      "Epoch 870/1500\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0108 - accuracy: 0.9976 - val_loss: 3.5908 - val_accuracy: 0.5680\n",
      "Epoch 871/1500\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0132 - accuracy: 0.9967 - val_loss: 3.6493 - val_accuracy: 0.5498\n",
      "Epoch 872/1500\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0113 - accuracy: 0.9959 - val_loss: 3.5532 - val_accuracy: 0.5559\n",
      "Epoch 873/1500\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0022 - accuracy: 0.9992 - val_loss: 3.3118 - val_accuracy: 0.5589\n",
      "Epoch 874/1500\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0102 - accuracy: 0.9967 - val_loss: 3.3105 - val_accuracy: 0.5770\n",
      "Epoch 875/1500\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0034 - accuracy: 0.9984 - val_loss: 3.3928 - val_accuracy: 0.5589\n",
      "Epoch 876/1500\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0056 - accuracy: 0.9976 - val_loss: 3.3836 - val_accuracy: 0.5650\n",
      "Epoch 877/1500\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0305 - accuracy: 0.9967 - val_loss: 3.3814 - val_accuracy: 0.5589\n",
      "Epoch 878/1500\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0161 - accuracy: 0.9951 - val_loss: 3.5823 - val_accuracy: 0.5498\n",
      "Epoch 879/1500\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0154 - accuracy: 0.9976 - val_loss: 3.6386 - val_accuracy: 0.5438\n",
      "Epoch 880/1500\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0135 - accuracy: 0.9984 - val_loss: 3.5752 - val_accuracy: 0.5408\n",
      "Epoch 881/1500\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0135 - accuracy: 0.9943 - val_loss: 3.3471 - val_accuracy: 0.5529\n",
      "Epoch 882/1500\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0125 - accuracy: 0.9976 - val_loss: 3.5120 - val_accuracy: 0.5468\n",
      "Epoch 883/1500\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0075 - accuracy: 0.9984 - val_loss: 3.7202 - val_accuracy: 0.5257\n",
      "Epoch 884/1500\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0487 - accuracy: 0.9894 - val_loss: 3.4788 - val_accuracy: 0.5559\n",
      "Epoch 885/1500\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0115 - accuracy: 0.9984 - val_loss: 3.5418 - val_accuracy: 0.5498\n",
      "Epoch 886/1500\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0080 - accuracy: 0.9967 - val_loss: 3.5671 - val_accuracy: 0.5438\n",
      "Epoch 887/1500\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0014 - accuracy: 0.9992 - val_loss: 3.4673 - val_accuracy: 0.5559\n",
      "Epoch 888/1500\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.5294 - val_accuracy: 0.5498\n",
      "Epoch 889/1500\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0074 - accuracy: 0.9967 - val_loss: 3.5558 - val_accuracy: 0.5680\n",
      "Epoch 890/1500\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 9.0157e-04 - accuracy: 1.0000 - val_loss: 3.5241 - val_accuracy: 0.5559\n",
      "Epoch 891/1500\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0089 - accuracy: 0.9976 - val_loss: 3.5207 - val_accuracy: 0.5619\n",
      "Epoch 892/1500\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.5668 - val_accuracy: 0.5559\n",
      "Epoch 893/1500\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0101 - accuracy: 0.9967 - val_loss: 3.4510 - val_accuracy: 0.5650\n",
      "Epoch 894/1500\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0054 - accuracy: 0.9976 - val_loss: 3.5090 - val_accuracy: 0.5619\n",
      "Epoch 895/1500\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0077 - accuracy: 0.9984 - val_loss: 3.4414 - val_accuracy: 0.5589\n",
      "Epoch 896/1500\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0312 - accuracy: 0.9943 - val_loss: 3.3117 - val_accuracy: 0.5680\n",
      "Epoch 897/1500\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0253 - accuracy: 0.9959 - val_loss: 3.3801 - val_accuracy: 0.5740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 898/1500\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0171 - accuracy: 0.9959 - val_loss: 3.3804 - val_accuracy: 0.5831\n",
      "Epoch 899/1500\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0039 - accuracy: 0.9992 - val_loss: 3.5025 - val_accuracy: 0.5740\n",
      "Epoch 900/1500\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 3.4695 - val_accuracy: 0.5559\n",
      "Epoch 901/1500\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0092 - accuracy: 0.9976 - val_loss: 3.5783 - val_accuracy: 0.5589\n",
      "Epoch 902/1500\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.7788 - val_accuracy: 0.5498\n",
      "Epoch 903/1500\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0087 - accuracy: 0.9984 - val_loss: 3.6951 - val_accuracy: 0.5136\n",
      "Epoch 904/1500\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0059 - accuracy: 0.9967 - val_loss: 3.5208 - val_accuracy: 0.5287\n",
      "Epoch 905/1500\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0051 - accuracy: 0.9967 - val_loss: 3.5851 - val_accuracy: 0.5589\n",
      "Epoch 906/1500\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0077 - accuracy: 0.9984 - val_loss: 3.6900 - val_accuracy: 0.5317\n",
      "Epoch 907/1500\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0020 - accuracy: 0.9992 - val_loss: 3.5117 - val_accuracy: 0.5529\n",
      "Epoch 908/1500\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0074 - accuracy: 0.9992 - val_loss: 3.6749 - val_accuracy: 0.5498\n",
      "Epoch 909/1500\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0039 - accuracy: 0.9984 - val_loss: 3.6397 - val_accuracy: 0.5770\n",
      "Epoch 910/1500\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0043 - accuracy: 0.9992 - val_loss: 3.7153 - val_accuracy: 0.5498\n",
      "Epoch 911/1500\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 9.8962e-04 - accuracy: 1.0000 - val_loss: 3.7821 - val_accuracy: 0.5529\n",
      "Epoch 912/1500\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0058 - accuracy: 0.9984 - val_loss: 3.7636 - val_accuracy: 0.5680\n",
      "Epoch 913/1500\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0196 - accuracy: 0.9959 - val_loss: 3.9632 - val_accuracy: 0.5347\n",
      "Epoch 914/1500\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0198 - accuracy: 0.9951 - val_loss: 3.7178 - val_accuracy: 0.5498\n",
      "Epoch 915/1500\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0094 - accuracy: 0.9984 - val_loss: 3.7023 - val_accuracy: 0.5589\n",
      "Epoch 916/1500\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0030 - accuracy: 0.9984 - val_loss: 3.6343 - val_accuracy: 0.5529\n",
      "Epoch 917/1500\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.6357 - val_accuracy: 0.5650\n",
      "Epoch 918/1500\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0061 - accuracy: 0.9976 - val_loss: 3.8231 - val_accuracy: 0.5468\n",
      "Epoch 919/1500\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0282 - accuracy: 0.9976 - val_loss: 3.6283 - val_accuracy: 0.5498\n",
      "Epoch 920/1500\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0102 - accuracy: 0.9984 - val_loss: 3.5763 - val_accuracy: 0.5770\n",
      "Epoch 921/1500\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0013 - accuracy: 0.9992 - val_loss: 3.6488 - val_accuracy: 0.5559\n",
      "Epoch 922/1500\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0059 - accuracy: 0.9992 - val_loss: 3.7597 - val_accuracy: 0.5619\n",
      "Epoch 923/1500\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0043 - accuracy: 0.9984 - val_loss: 3.9434 - val_accuracy: 0.5468\n",
      "Epoch 924/1500\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0158 - accuracy: 0.9935 - val_loss: 3.8533 - val_accuracy: 0.5680\n",
      "Epoch 925/1500\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0235 - accuracy: 0.9951 - val_loss: 3.7048 - val_accuracy: 0.5498\n",
      "Epoch 926/1500\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0111 - accuracy: 0.9967 - val_loss: 3.6168 - val_accuracy: 0.5589\n",
      "Epoch 927/1500\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0052 - accuracy: 0.9976 - val_loss: 3.8473 - val_accuracy: 0.5619\n",
      "Epoch 928/1500\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0076 - accuracy: 0.9976 - val_loss: 3.7110 - val_accuracy: 0.5347\n",
      "Epoch 929/1500\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0054 - accuracy: 0.9976 - val_loss: 3.7132 - val_accuracy: 0.5559\n",
      "Epoch 930/1500\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0043 - accuracy: 0.9984 - val_loss: 3.6993 - val_accuracy: 0.5498\n",
      "Epoch 931/1500\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0046 - accuracy: 0.9976 - val_loss: 3.8771 - val_accuracy: 0.5438\n",
      "Epoch 932/1500\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0065 - accuracy: 0.9984 - val_loss: 3.6819 - val_accuracy: 0.5468\n",
      "Epoch 933/1500\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0312 - accuracy: 0.9967 - val_loss: 3.7519 - val_accuracy: 0.5378\n",
      "Epoch 934/1500\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.7177 - val_accuracy: 0.5468\n",
      "Epoch 935/1500\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0138 - accuracy: 0.9959 - val_loss: 3.8901 - val_accuracy: 0.5347\n",
      "Epoch 936/1500\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0121 - accuracy: 0.9959 - val_loss: 3.7586 - val_accuracy: 0.5347\n",
      "Epoch 937/1500\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0138 - accuracy: 0.9943 - val_loss: 3.8569 - val_accuracy: 0.5559\n",
      "Epoch 938/1500\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0134 - accuracy: 0.9967 - val_loss: 3.8827 - val_accuracy: 0.5347\n",
      "Epoch 939/1500\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0102 - accuracy: 0.9943 - val_loss: 3.9461 - val_accuracy: 0.5257\n",
      "Epoch 940/1500\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.8420 - val_accuracy: 0.5347\n",
      "Epoch 941/1500\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0033 - accuracy: 0.9984 - val_loss: 3.8973 - val_accuracy: 0.5408\n",
      "Epoch 942/1500\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0417 - accuracy: 0.9927 - val_loss: 3.9030 - val_accuracy: 0.5650\n",
      "Epoch 943/1500\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9976 - val_loss: 3.8832 - val_accuracy: 0.5619\n",
      "Epoch 944/1500\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0027 - accuracy: 0.9984 - val_loss: 3.9082 - val_accuracy: 0.5498\n",
      "Epoch 945/1500\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9062 - val_accuracy: 0.5438\n",
      "Epoch 946/1500\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0057 - accuracy: 0.9976 - val_loss: 4.0423 - val_accuracy: 0.5408\n",
      "Epoch 947/1500\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0070 - accuracy: 0.9976 - val_loss: 3.8217 - val_accuracy: 0.5559\n",
      "Epoch 948/1500\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0044 - accuracy: 0.9976 - val_loss: 3.8402 - val_accuracy: 0.5680\n",
      "Epoch 949/1500\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0057 - accuracy: 0.9992 - val_loss: 3.7581 - val_accuracy: 0.5589\n",
      "Epoch 950/1500\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0132 - accuracy: 0.9951 - val_loss: 3.8813 - val_accuracy: 0.5861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 951/1500\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0129 - accuracy: 0.9951 - val_loss: 3.9354 - val_accuracy: 0.5680\n",
      "Epoch 952/1500\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0136 - accuracy: 0.9943 - val_loss: 3.8184 - val_accuracy: 0.5650\n",
      "Epoch 953/1500\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0073 - accuracy: 0.9976 - val_loss: 3.7994 - val_accuracy: 0.5650\n",
      "Epoch 954/1500\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0252 - accuracy: 0.9951 - val_loss: 3.6880 - val_accuracy: 0.5710\n",
      "Epoch 955/1500\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 3.7265 - val_accuracy: 0.5680\n",
      "Epoch 956/1500\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0049 - accuracy: 0.9984 - val_loss: 3.7598 - val_accuracy: 0.5770\n",
      "Epoch 957/1500\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0061 - accuracy: 0.9984 - val_loss: 4.0945 - val_accuracy: 0.5347\n",
      "Epoch 958/1500\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0067 - accuracy: 0.9967 - val_loss: 3.7650 - val_accuracy: 0.5529\n",
      "Epoch 959/1500\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.6513 - val_accuracy: 0.5650\n",
      "Epoch 960/1500\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.7577 - val_accuracy: 0.5650\n",
      "Epoch 961/1500\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0235 - accuracy: 0.9927 - val_loss: 3.9150 - val_accuracy: 0.5559\n",
      "Epoch 962/1500\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0049 - accuracy: 0.9984 - val_loss: 3.9097 - val_accuracy: 0.5710\n",
      "Epoch 963/1500\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0014 - accuracy: 0.9992 - val_loss: 3.7698 - val_accuracy: 0.5650\n",
      "Epoch 964/1500\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0190 - accuracy: 0.9976 - val_loss: 3.9774 - val_accuracy: 0.5619\n",
      "Epoch 965/1500\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.6362 - val_accuracy: 0.5468\n",
      "Epoch 966/1500\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0046 - accuracy: 0.9976 - val_loss: 3.7516 - val_accuracy: 0.5468\n",
      "Epoch 967/1500\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0104 - accuracy: 0.9967 - val_loss: 3.4713 - val_accuracy: 0.5650\n",
      "Epoch 968/1500\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0043 - accuracy: 0.9992 - val_loss: 3.4913 - val_accuracy: 0.5740\n",
      "Epoch 969/1500\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0068 - accuracy: 0.9976 - val_loss: 3.3602 - val_accuracy: 0.5680\n",
      "Epoch 970/1500\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0099 - accuracy: 0.9992 - val_loss: 3.5453 - val_accuracy: 0.5650\n",
      "Epoch 971/1500\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0036 - accuracy: 0.9992 - val_loss: 3.5691 - val_accuracy: 0.5710\n",
      "Epoch 972/1500\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 3.7504 - val_accuracy: 0.5770\n",
      "Epoch 973/1500\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0056 - accuracy: 0.9976 - val_loss: 3.7932 - val_accuracy: 0.5680\n",
      "Epoch 974/1500\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0052 - accuracy: 0.9967 - val_loss: 3.7256 - val_accuracy: 0.5559\n",
      "Epoch 975/1500\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0104 - accuracy: 0.9976 - val_loss: 3.7409 - val_accuracy: 0.5589\n",
      "Epoch 976/1500\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.7940 - val_accuracy: 0.5529\n",
      "Epoch 977/1500\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0067 - accuracy: 0.9967 - val_loss: 3.8005 - val_accuracy: 0.5589\n",
      "Epoch 978/1500\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0082 - accuracy: 0.9967 - val_loss: 3.8466 - val_accuracy: 0.5589\n",
      "Epoch 979/1500\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 3.8417 - val_accuracy: 0.5559\n",
      "Epoch 980/1500\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0083 - accuracy: 0.9976 - val_loss: 4.2734 - val_accuracy: 0.5287\n",
      "Epoch 981/1500\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.9814 - val_accuracy: 0.5650\n",
      "Epoch 982/1500\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 4.0312 - val_accuracy: 0.5498\n",
      "Epoch 983/1500\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0047 - accuracy: 0.9992 - val_loss: 3.9209 - val_accuracy: 0.5529\n",
      "Epoch 984/1500\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0066 - accuracy: 0.9976 - val_loss: 3.6806 - val_accuracy: 0.5650\n",
      "Epoch 985/1500\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.7564 - val_accuracy: 0.5650\n",
      "Epoch 986/1500\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0061 - accuracy: 0.9976 - val_loss: 3.7451 - val_accuracy: 0.5529\n",
      "Epoch 987/1500\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0040 - accuracy: 0.9992 - val_loss: 3.7059 - val_accuracy: 0.5710\n",
      "Epoch 988/1500\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 3.7546 - val_accuracy: 0.5559\n",
      "Epoch 989/1500\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0058 - accuracy: 0.9992 - val_loss: 3.7630 - val_accuracy: 0.5408\n",
      "Epoch 990/1500\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0040 - accuracy: 0.9992 - val_loss: 3.8918 - val_accuracy: 0.5378\n",
      "Epoch 991/1500\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0063 - accuracy: 0.9984 - val_loss: 3.8321 - val_accuracy: 0.5529\n",
      "Epoch 992/1500\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0017 - accuracy: 0.9992 - val_loss: 3.7509 - val_accuracy: 0.5468\n",
      "Epoch 993/1500\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0068 - accuracy: 0.9984 - val_loss: 3.8587 - val_accuracy: 0.5740\n",
      "Epoch 994/1500\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 0.9992 - val_loss: 3.9489 - val_accuracy: 0.5468\n",
      "Epoch 995/1500\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9992 - val_loss: 3.9126 - val_accuracy: 0.5438\n",
      "Epoch 996/1500\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0052 - accuracy: 0.9976 - val_loss: 3.8846 - val_accuracy: 0.5559\n",
      "Epoch 997/1500\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.8477 - val_accuracy: 0.5740\n",
      "Epoch 998/1500\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0079 - accuracy: 0.9967 - val_loss: 3.9469 - val_accuracy: 0.5529\n",
      "Epoch 999/1500\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0171 - accuracy: 0.9959 - val_loss: 3.9045 - val_accuracy: 0.5498\n",
      "Epoch 1000/1500\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0022 - accuracy: 0.9992 - val_loss: 3.8775 - val_accuracy: 0.5619\n",
      "Epoch 1001/1500\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0070 - accuracy: 0.9984 - val_loss: 3.7721 - val_accuracy: 0.5589\n",
      "Epoch 1002/1500\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0126 - accuracy: 0.9967 - val_loss: 3.7500 - val_accuracy: 0.5468\n",
      "Epoch 1003/1500\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0229 - accuracy: 0.9976 - val_loss: 3.9936 - val_accuracy: 0.5619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1004/1500\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.9221 - val_accuracy: 0.5498\n",
      "Epoch 1005/1500\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0164 - accuracy: 0.9992 - val_loss: 3.9603 - val_accuracy: 0.5589\n",
      "Epoch 1006/1500\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0063 - accuracy: 0.9984 - val_loss: 3.8824 - val_accuracy: 0.5498\n",
      "Epoch 1007/1500\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0058 - accuracy: 0.9984 - val_loss: 3.7571 - val_accuracy: 0.5589\n",
      "Epoch 1008/1500\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0034 - accuracy: 0.9992 - val_loss: 3.8259 - val_accuracy: 0.5498\n",
      "Epoch 1009/1500\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.7949 - val_accuracy: 0.5529\n",
      "Epoch 1010/1500\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 3.8659 - val_accuracy: 0.5438\n",
      "Epoch 1011/1500\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0058 - accuracy: 0.9976 - val_loss: 4.3491 - val_accuracy: 0.5347\n",
      "Epoch 1012/1500\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0073 - accuracy: 0.9984 - val_loss: 4.0691 - val_accuracy: 0.5589\n",
      "Epoch 1013/1500\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0043 - accuracy: 0.9984 - val_loss: 4.1654 - val_accuracy: 0.5650\n",
      "Epoch 1014/1500\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0062 - accuracy: 0.9976 - val_loss: 4.1634 - val_accuracy: 0.5408\n",
      "Epoch 1015/1500\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 4.0673 - val_accuracy: 0.5619\n",
      "Epoch 1016/1500\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0031 - accuracy: 0.9992 - val_loss: 3.7670 - val_accuracy: 0.5680\n",
      "Epoch 1017/1500\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0085 - accuracy: 0.9967 - val_loss: 3.7081 - val_accuracy: 0.5650\n",
      "Epoch 1018/1500\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0099 - accuracy: 0.9967 - val_loss: 3.8886 - val_accuracy: 0.5529\n",
      "Epoch 1019/1500\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0083 - accuracy: 0.9976 - val_loss: 4.0460 - val_accuracy: 0.5317\n",
      "Epoch 1020/1500\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 3.8515 - val_accuracy: 0.5498\n",
      "Epoch 1021/1500\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 9.6131e-04 - accuracy: 1.0000 - val_loss: 3.9762 - val_accuracy: 0.5498\n",
      "Epoch 1022/1500\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0016 - accuracy: 0.9992 - val_loss: 3.9779 - val_accuracy: 0.5619\n",
      "Epoch 1023/1500\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0634 - accuracy: 0.9967 - val_loss: 4.2284 - val_accuracy: 0.5559\n",
      "Epoch 1024/1500\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0081 - accuracy: 0.9984 - val_loss: 3.8660 - val_accuracy: 0.5619\n",
      "Epoch 1025/1500\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0079 - accuracy: 0.9967 - val_loss: 3.9331 - val_accuracy: 0.5347\n",
      "Epoch 1026/1500\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0081 - accuracy: 0.9967 - val_loss: 3.8501 - val_accuracy: 0.5619\n",
      "Epoch 1027/1500\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0054 - accuracy: 0.9976 - val_loss: 4.1511 - val_accuracy: 0.5347\n",
      "Epoch 1028/1500\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0325 - accuracy: 0.9976 - val_loss: 4.1454 - val_accuracy: 0.5408\n",
      "Epoch 1029/1500\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0056 - accuracy: 0.9992 - val_loss: 3.8283 - val_accuracy: 0.5650\n",
      "Epoch 1030/1500\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0017 - accuracy: 0.9992 - val_loss: 4.0302 - val_accuracy: 0.5619\n",
      "Epoch 1031/1500\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0063 - accuracy: 0.9976 - val_loss: 3.8775 - val_accuracy: 0.5619\n",
      "Epoch 1032/1500\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.8765 - val_accuracy: 0.5559\n",
      "Epoch 1033/1500\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0049 - accuracy: 0.9984 - val_loss: 3.9700 - val_accuracy: 0.5498\n",
      "Epoch 1034/1500\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 4.0425 - val_accuracy: 0.5438\n",
      "Epoch 1035/1500\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0073 - accuracy: 0.9984 - val_loss: 3.9681 - val_accuracy: 0.5438\n",
      "Epoch 1036/1500\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0137 - accuracy: 0.9959 - val_loss: 3.7541 - val_accuracy: 0.5529\n",
      "Epoch 1037/1500\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9992 - val_loss: 3.7842 - val_accuracy: 0.5529\n",
      "Epoch 1038/1500\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0121 - accuracy: 0.9984 - val_loss: 3.7440 - val_accuracy: 0.5529\n",
      "Epoch 1039/1500\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0195 - accuracy: 0.9967 - val_loss: 3.8964 - val_accuracy: 0.5619\n",
      "Epoch 1040/1500\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0039 - accuracy: 0.9984 - val_loss: 3.7852 - val_accuracy: 0.5770\n",
      "Epoch 1041/1500\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9984 - val_loss: 3.8010 - val_accuracy: 0.5650\n",
      "Epoch 1042/1500\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0090 - accuracy: 0.9976 - val_loss: 3.8558 - val_accuracy: 0.5650\n",
      "Epoch 1043/1500\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0078 - accuracy: 0.9959 - val_loss: 3.8300 - val_accuracy: 0.5619\n",
      "Epoch 1044/1500\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0122 - accuracy: 0.9976 - val_loss: 3.9288 - val_accuracy: 0.5559\n",
      "Epoch 1045/1500\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0060 - accuracy: 0.9967 - val_loss: 4.1035 - val_accuracy: 0.5498\n",
      "Epoch 1046/1500\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.9144 - val_accuracy: 0.5619\n",
      "Epoch 1047/1500\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0039 - accuracy: 0.9984 - val_loss: 3.8519 - val_accuracy: 0.5559\n",
      "Epoch 1048/1500\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0116 - accuracy: 0.9976 - val_loss: 3.8783 - val_accuracy: 0.5498\n",
      "Epoch 1049/1500\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 5.1967e-04 - accuracy: 1.0000 - val_loss: 3.8884 - val_accuracy: 0.5498\n",
      "Epoch 1050/1500\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0275 - accuracy: 0.9967 - val_loss: 3.8960 - val_accuracy: 0.5317\n",
      "Epoch 1051/1500\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 3.9539 - val_accuracy: 0.5468\n",
      "Epoch 1052/1500\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0077 - accuracy: 0.9976 - val_loss: 3.9760 - val_accuracy: 0.5589\n",
      "Epoch 1053/1500\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0020 - accuracy: 0.9992 - val_loss: 3.8575 - val_accuracy: 0.5468\n",
      "Epoch 1054/1500\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0072 - accuracy: 0.9967 - val_loss: 3.9397 - val_accuracy: 0.5559\n",
      "Epoch 1055/1500\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0055 - accuracy: 0.9992 - val_loss: 3.8834 - val_accuracy: 0.5650\n",
      "Epoch 1056/1500\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0057 - accuracy: 0.9976 - val_loss: 4.1047 - val_accuracy: 0.5196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1057/1500\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0162 - accuracy: 0.9943 - val_loss: 4.1079 - val_accuracy: 0.5347\n",
      "Epoch 1058/1500\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0048 - accuracy: 0.9984 - val_loss: 3.8330 - val_accuracy: 0.5680\n",
      "Epoch 1059/1500\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9984 - val_loss: 3.6954 - val_accuracy: 0.5801\n",
      "Epoch 1060/1500\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0177 - accuracy: 0.9984 - val_loss: 3.9680 - val_accuracy: 0.5559\n",
      "Epoch 1061/1500\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0023 - accuracy: 0.9992 - val_loss: 4.0305 - val_accuracy: 0.5559\n",
      "Epoch 1062/1500\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0660 - accuracy: 0.9959 - val_loss: 3.9540 - val_accuracy: 0.5650\n",
      "Epoch 1063/1500\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 3.9361 - val_accuracy: 0.5650\n",
      "Epoch 1064/1500\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0050 - accuracy: 0.9992 - val_loss: 3.8174 - val_accuracy: 0.5619\n",
      "Epoch 1065/1500\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0209 - accuracy: 0.9976 - val_loss: 3.8112 - val_accuracy: 0.5650\n",
      "Epoch 1066/1500\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0128 - accuracy: 0.9959 - val_loss: 3.8992 - val_accuracy: 0.5680\n",
      "Epoch 1067/1500\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0060 - accuracy: 0.9992 - val_loss: 3.8805 - val_accuracy: 0.5589\n",
      "Epoch 1068/1500\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0413 - accuracy: 0.9959 - val_loss: 3.7651 - val_accuracy: 0.5589\n",
      "Epoch 1069/1500\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0020 - accuracy: 0.9992 - val_loss: 3.8253 - val_accuracy: 0.5529\n",
      "Epoch 1070/1500\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0088 - accuracy: 0.9967 - val_loss: 3.8630 - val_accuracy: 0.5529\n",
      "Epoch 1071/1500\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.7226 - val_accuracy: 0.5770\n",
      "Epoch 1072/1500\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0254 - accuracy: 0.9984 - val_loss: 3.7440 - val_accuracy: 0.5770\n",
      "Epoch 1073/1500\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0066 - accuracy: 0.9959 - val_loss: 3.8003 - val_accuracy: 0.5559\n",
      "Epoch 1074/1500\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0092 - accuracy: 0.9976 - val_loss: 3.8981 - val_accuracy: 0.5619\n",
      "Epoch 1075/1500\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0018 - accuracy: 0.9992 - val_loss: 3.8262 - val_accuracy: 0.5650\n",
      "Epoch 1076/1500\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0042 - accuracy: 0.9984 - val_loss: 3.7911 - val_accuracy: 0.5710\n",
      "Epoch 1077/1500\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0057 - accuracy: 0.9976 - val_loss: 3.8111 - val_accuracy: 0.5589\n",
      "Epoch 1078/1500\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9992 - val_loss: 3.7721 - val_accuracy: 0.5740\n",
      "Epoch 1079/1500\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0045 - accuracy: 0.9984 - val_loss: 3.8400 - val_accuracy: 0.5589\n",
      "Epoch 1080/1500\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 4.1200 - val_accuracy: 0.5498\n",
      "Epoch 1081/1500\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0029 - accuracy: 0.9984 - val_loss: 3.9751 - val_accuracy: 0.5619\n",
      "Epoch 1082/1500\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0143 - accuracy: 0.9967 - val_loss: 3.8944 - val_accuracy: 0.5378\n",
      "Epoch 1083/1500\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0049 - accuracy: 0.9976 - val_loss: 3.9298 - val_accuracy: 0.5529\n",
      "Epoch 1084/1500\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0044 - accuracy: 0.9984 - val_loss: 3.9483 - val_accuracy: 0.5408\n",
      "Epoch 1085/1500\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0044 - accuracy: 0.9992 - val_loss: 3.8122 - val_accuracy: 0.5650\n",
      "Epoch 1086/1500\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 0.9992 - val_loss: 3.8533 - val_accuracy: 0.5589\n",
      "Epoch 1087/1500\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 3.9046 - val_accuracy: 0.5589\n",
      "Epoch 1088/1500\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.0275 - val_accuracy: 0.5498\n",
      "Epoch 1089/1500\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0201 - accuracy: 0.9976 - val_loss: 4.1374 - val_accuracy: 0.5498\n",
      "Epoch 1090/1500\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0049 - accuracy: 0.9992 - val_loss: 4.1820 - val_accuracy: 0.5498\n",
      "Epoch 1091/1500\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0036 - accuracy: 0.9984 - val_loss: 3.9710 - val_accuracy: 0.5498\n",
      "Epoch 1092/1500\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0062 - accuracy: 0.9976 - val_loss: 3.9282 - val_accuracy: 0.5529\n",
      "Epoch 1093/1500\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.9137 - val_accuracy: 0.5468\n",
      "Epoch 1094/1500\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 9.2625e-04 - accuracy: 0.9992 - val_loss: 3.9284 - val_accuracy: 0.5529\n",
      "Epoch 1095/1500\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0030 - accuracy: 0.9992 - val_loss: 4.3271 - val_accuracy: 0.5287\n",
      "Epoch 1096/1500\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0047 - accuracy: 0.9984 - val_loss: 4.2002 - val_accuracy: 0.5559\n",
      "Epoch 1097/1500\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.3402 - val_accuracy: 0.5468\n",
      "Epoch 1098/1500\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 4.1264 - val_accuracy: 0.5468\n",
      "Epoch 1099/1500\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0054 - accuracy: 0.9984 - val_loss: 4.1022 - val_accuracy: 0.5529\n",
      "Epoch 1100/1500\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0059 - accuracy: 0.9984 - val_loss: 3.9638 - val_accuracy: 0.5770\n",
      "Epoch 1101/1500\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9992 - val_loss: 4.0054 - val_accuracy: 0.5650\n",
      "Epoch 1102/1500\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 9.0562e-04 - accuracy: 1.0000 - val_loss: 3.9681 - val_accuracy: 0.5650\n",
      "Epoch 1103/1500\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0043 - accuracy: 0.9984 - val_loss: 3.8639 - val_accuracy: 0.5861\n",
      "Epoch 1104/1500\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0045 - accuracy: 0.9984 - val_loss: 3.8899 - val_accuracy: 0.5801\n",
      "Epoch 1105/1500\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 3.4970e-04 - accuracy: 1.0000 - val_loss: 3.9190 - val_accuracy: 0.5710\n",
      "Epoch 1106/1500\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0034 - accuracy: 0.9984 - val_loss: 3.9759 - val_accuracy: 0.5861\n",
      "Epoch 1107/1500\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 3.9061 - val_accuracy: 0.5801\n",
      "Epoch 1108/1500\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0088 - accuracy: 0.9967 - val_loss: 4.1529 - val_accuracy: 0.5680\n",
      "Epoch 1109/1500\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0090 - accuracy: 0.9984 - val_loss: 3.8668 - val_accuracy: 0.5770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1110/1500\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0113 - accuracy: 0.9959 - val_loss: 3.8329 - val_accuracy: 0.5740\n",
      "Epoch 1111/1500\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.8725 - val_accuracy: 0.5770\n",
      "Epoch 1112/1500\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 4.6165e-04 - accuracy: 1.0000 - val_loss: 3.9825 - val_accuracy: 0.5831\n",
      "Epoch 1113/1500\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0030 - accuracy: 0.9984 - val_loss: 4.0275 - val_accuracy: 0.5831\n",
      "Epoch 1114/1500\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0068 - accuracy: 0.9976 - val_loss: 4.0568 - val_accuracy: 0.5710\n",
      "Epoch 1115/1500\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0046 - accuracy: 0.9984 - val_loss: 4.2634 - val_accuracy: 0.5468\n",
      "Epoch 1116/1500\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 5.5605e-04 - accuracy: 1.0000 - val_loss: 4.0648 - val_accuracy: 0.5559\n",
      "Epoch 1117/1500\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0044 - accuracy: 0.9984 - val_loss: 3.9876 - val_accuracy: 0.5559\n",
      "Epoch 1118/1500\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0021 - accuracy: 0.9992 - val_loss: 3.9061 - val_accuracy: 0.5650\n",
      "Epoch 1119/1500\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0052 - accuracy: 0.9976 - val_loss: 3.9608 - val_accuracy: 0.5680\n",
      "Epoch 1120/1500\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0130 - accuracy: 0.9959 - val_loss: 3.9065 - val_accuracy: 0.5559\n",
      "Epoch 1121/1500\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 3.9977 - val_accuracy: 0.5468\n",
      "Epoch 1122/1500\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0052 - accuracy: 0.9984 - val_loss: 4.1057 - val_accuracy: 0.5589\n",
      "Epoch 1123/1500\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0053 - accuracy: 0.9984 - val_loss: 4.1635 - val_accuracy: 0.5650\n",
      "Epoch 1124/1500\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0203 - accuracy: 0.9984 - val_loss: 4.2317 - val_accuracy: 0.5529\n",
      "Epoch 1125/1500\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0072 - accuracy: 0.9976 - val_loss: 4.1508 - val_accuracy: 0.5650\n",
      "Epoch 1126/1500\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.0453 - val_accuracy: 0.5589\n",
      "Epoch 1127/1500\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 5.0246e-04 - accuracy: 1.0000 - val_loss: 4.0304 - val_accuracy: 0.5498\n",
      "Epoch 1128/1500\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0059 - accuracy: 0.9984 - val_loss: 4.0498 - val_accuracy: 0.5589\n",
      "Epoch 1129/1500\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0137 - accuracy: 0.9951 - val_loss: 3.9120 - val_accuracy: 0.5770\n",
      "Epoch 1130/1500\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.9169 - val_accuracy: 0.5710\n",
      "Epoch 1131/1500\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.9792 - val_accuracy: 0.5740\n",
      "Epoch 1132/1500\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0127 - accuracy: 0.9959 - val_loss: 3.9719 - val_accuracy: 0.5589\n",
      "Epoch 1133/1500\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0207 - accuracy: 0.9967 - val_loss: 4.1444 - val_accuracy: 0.5438\n",
      "Epoch 1134/1500\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0029 - accuracy: 0.9984 - val_loss: 4.1553 - val_accuracy: 0.5559\n",
      "Epoch 1135/1500\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0132 - accuracy: 0.9967 - val_loss: 3.9974 - val_accuracy: 0.5559\n",
      "Epoch 1136/1500\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9984 - val_loss: 4.2780 - val_accuracy: 0.5468\n",
      "Epoch 1137/1500\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0037 - accuracy: 0.9984 - val_loss: 3.8139 - val_accuracy: 0.5861\n",
      "Epoch 1138/1500\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0027 - accuracy: 0.9984 - val_loss: 3.7886 - val_accuracy: 0.5740\n",
      "Epoch 1139/1500\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0070 - accuracy: 0.9967 - val_loss: 3.9425 - val_accuracy: 0.5710\n",
      "Epoch 1140/1500\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0047 - accuracy: 0.9984 - val_loss: 3.7748 - val_accuracy: 0.5740\n",
      "Epoch 1141/1500\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0251 - accuracy: 0.9976 - val_loss: 3.8928 - val_accuracy: 0.5680\n",
      "Epoch 1142/1500\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 5.3870e-04 - accuracy: 1.0000 - val_loss: 3.8282 - val_accuracy: 0.5650\n",
      "Epoch 1143/1500\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0223 - accuracy: 0.9976 - val_loss: 3.8888 - val_accuracy: 0.5498\n",
      "Epoch 1144/1500\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0172 - accuracy: 0.9951 - val_loss: 4.0008 - val_accuracy: 0.5559\n",
      "Epoch 1145/1500\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 7.2800e-04 - accuracy: 1.0000 - val_loss: 3.8633 - val_accuracy: 0.5589\n",
      "Epoch 1146/1500\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.9602 - val_accuracy: 0.5529\n",
      "Epoch 1147/1500\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0105 - accuracy: 0.9959 - val_loss: 4.2174 - val_accuracy: 0.5559\n",
      "Epoch 1148/1500\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0069 - accuracy: 0.9967 - val_loss: 3.9421 - val_accuracy: 0.5529\n",
      "Epoch 1149/1500\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0215 - accuracy: 0.9959 - val_loss: 3.9127 - val_accuracy: 0.5619\n",
      "Epoch 1150/1500\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0038 - accuracy: 0.9976 - val_loss: 3.8908 - val_accuracy: 0.5740\n",
      "Epoch 1151/1500\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0021 - accuracy: 0.9992 - val_loss: 4.0751 - val_accuracy: 0.5619\n",
      "Epoch 1152/1500\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 3.8431 - val_accuracy: 0.5891\n",
      "Epoch 1153/1500\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0058 - accuracy: 0.9984 - val_loss: 3.8408 - val_accuracy: 0.5740\n",
      "Epoch 1154/1500\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0138 - accuracy: 0.9959 - val_loss: 3.8274 - val_accuracy: 0.5770\n",
      "Epoch 1155/1500\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0099 - accuracy: 0.9976 - val_loss: 3.9817 - val_accuracy: 0.5710\n",
      "Epoch 1156/1500\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0058 - accuracy: 0.9984 - val_loss: 4.3129 - val_accuracy: 0.5317\n",
      "Epoch 1157/1500\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0702 - accuracy: 0.9951 - val_loss: 3.9527 - val_accuracy: 0.5650\n",
      "Epoch 1158/1500\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0073 - accuracy: 0.9976 - val_loss: 4.1672 - val_accuracy: 0.5710\n",
      "Epoch 1159/1500\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.3143 - val_accuracy: 0.5559\n",
      "Epoch 1160/1500\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0297 - accuracy: 0.9951 - val_loss: 4.1699 - val_accuracy: 0.5589\n",
      "Epoch 1161/1500\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0088 - accuracy: 0.9984 - val_loss: 4.0229 - val_accuracy: 0.5559\n",
      "Epoch 1162/1500\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0187 - accuracy: 0.9959 - val_loss: 4.0956 - val_accuracy: 0.5529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1163/1500\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 4.0677 - val_accuracy: 0.5650\n",
      "Epoch 1164/1500\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0025 - accuracy: 0.9992 - val_loss: 4.0681 - val_accuracy: 0.5619\n",
      "Epoch 1165/1500\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0086 - accuracy: 0.9976 - val_loss: 3.9920 - val_accuracy: 0.5952\n",
      "Epoch 1166/1500\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 4.5272 - val_accuracy: 0.5498\n",
      "Epoch 1167/1500\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0053 - accuracy: 0.9976 - val_loss: 4.3199 - val_accuracy: 0.5498\n",
      "Epoch 1168/1500\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 4.1932 - val_accuracy: 0.5680\n",
      "Epoch 1169/1500\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0058 - accuracy: 0.9992 - val_loss: 4.2385 - val_accuracy: 0.5740\n",
      "Epoch 1170/1500\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0053 - accuracy: 0.9992 - val_loss: 4.5357 - val_accuracy: 0.5438\n",
      "Epoch 1171/1500\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0037 - accuracy: 0.9984 - val_loss: 4.3610 - val_accuracy: 0.5619\n",
      "Epoch 1172/1500\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0115 - accuracy: 0.9967 - val_loss: 4.3410 - val_accuracy: 0.5770\n",
      "Epoch 1173/1500\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0083 - accuracy: 0.9976 - val_loss: 4.2266 - val_accuracy: 0.5831\n",
      "Epoch 1174/1500\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0120 - accuracy: 0.9992 - val_loss: 3.9831 - val_accuracy: 0.5710\n",
      "Epoch 1175/1500\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0189 - accuracy: 0.9967 - val_loss: 4.1813 - val_accuracy: 0.5559\n",
      "Epoch 1176/1500\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0021 - accuracy: 0.9992 - val_loss: 3.8862 - val_accuracy: 0.5650\n",
      "Epoch 1177/1500\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0054 - accuracy: 0.9992 - val_loss: 4.0209 - val_accuracy: 0.5650\n",
      "Epoch 1178/1500\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0108 - accuracy: 0.9976 - val_loss: 3.9450 - val_accuracy: 0.5619\n",
      "Epoch 1179/1500\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0109 - accuracy: 0.9976 - val_loss: 3.8964 - val_accuracy: 0.5589\n",
      "Epoch 1180/1500\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0033 - accuracy: 0.9984 - val_loss: 4.1272 - val_accuracy: 0.5589\n",
      "Epoch 1181/1500\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0163 - accuracy: 0.9951 - val_loss: 4.2484 - val_accuracy: 0.5619\n",
      "Epoch 1182/1500\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0038 - accuracy: 0.9984 - val_loss: 4.1405 - val_accuracy: 0.5801\n",
      "Epoch 1183/1500\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0076 - accuracy: 0.9984 - val_loss: 4.0972 - val_accuracy: 0.5589\n",
      "Epoch 1184/1500\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0046 - accuracy: 0.9992 - val_loss: 4.1968 - val_accuracy: 0.5529\n",
      "Epoch 1185/1500\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0021 - accuracy: 0.9992 - val_loss: 4.1383 - val_accuracy: 0.5529\n",
      "Epoch 1186/1500\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 4.1046 - val_accuracy: 0.5589\n",
      "Epoch 1187/1500\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0042 - accuracy: 0.9984 - val_loss: 4.0883 - val_accuracy: 0.5559\n",
      "Epoch 1188/1500\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0217 - accuracy: 0.9976 - val_loss: 4.1052 - val_accuracy: 0.5619\n",
      "Epoch 1189/1500\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0121 - accuracy: 0.9959 - val_loss: 4.0566 - val_accuracy: 0.5619\n",
      "Epoch 1190/1500\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9976 - val_loss: 4.1168 - val_accuracy: 0.5589\n",
      "Epoch 1191/1500\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0132 - accuracy: 0.9976 - val_loss: 4.0907 - val_accuracy: 0.5498\n",
      "Epoch 1192/1500\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0035 - accuracy: 0.9992 - val_loss: 4.1349 - val_accuracy: 0.5347\n",
      "Epoch 1193/1500\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0015 - accuracy: 0.9992 - val_loss: 3.9084 - val_accuracy: 0.5740\n",
      "Epoch 1194/1500\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.9648 - val_accuracy: 0.5619\n",
      "Epoch 1195/1500\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 5.6259e-04 - accuracy: 1.0000 - val_loss: 4.0316 - val_accuracy: 0.5589\n",
      "Epoch 1196/1500\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0070 - accuracy: 0.9967 - val_loss: 4.0638 - val_accuracy: 0.5529\n",
      "Epoch 1197/1500\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 3.9752 - val_accuracy: 0.5619\n",
      "Epoch 1198/1500\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0050 - accuracy: 0.9984 - val_loss: 4.0074 - val_accuracy: 0.5559\n",
      "Epoch 1199/1500\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 3.8692e-04 - accuracy: 1.0000 - val_loss: 4.1486 - val_accuracy: 0.5619\n",
      "Epoch 1200/1500\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 9.0504e-04 - accuracy: 1.0000 - val_loss: 4.1282 - val_accuracy: 0.5589\n",
      "Epoch 1201/1500\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0261 - accuracy: 0.9959 - val_loss: 4.1429 - val_accuracy: 0.5740\n",
      "Epoch 1202/1500\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0162 - accuracy: 0.9976 - val_loss: 4.3129 - val_accuracy: 0.5740\n",
      "Epoch 1203/1500\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0014 - accuracy: 0.9992 - val_loss: 4.2472 - val_accuracy: 0.5710\n",
      "Epoch 1204/1500\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0059 - accuracy: 0.9984 - val_loss: 4.2476 - val_accuracy: 0.5408\n",
      "Epoch 1205/1500\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0718 - accuracy: 0.9951 - val_loss: 4.2481 - val_accuracy: 0.5589\n",
      "Epoch 1206/1500\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.0742 - val_accuracy: 0.5529\n",
      "Epoch 1207/1500\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0355 - accuracy: 0.9984 - val_loss: 4.1763 - val_accuracy: 0.5529\n",
      "Epoch 1208/1500\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 8.2969e-04 - accuracy: 1.0000 - val_loss: 4.0903 - val_accuracy: 0.5559\n",
      "Epoch 1209/1500\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0162 - accuracy: 0.9967 - val_loss: 4.3166 - val_accuracy: 0.5650\n",
      "Epoch 1210/1500\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1413 - val_accuracy: 0.5770\n",
      "Epoch 1211/1500\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 1.8321e-04 - accuracy: 1.0000 - val_loss: 4.0911 - val_accuracy: 0.5589\n",
      "Epoch 1212/1500\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0070 - accuracy: 0.9984 - val_loss: 3.9611 - val_accuracy: 0.5589\n",
      "Epoch 1213/1500\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0109 - accuracy: 0.9967 - val_loss: 4.1798 - val_accuracy: 0.5408\n",
      "Epoch 1214/1500\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0074 - accuracy: 0.9992 - val_loss: 4.0944 - val_accuracy: 0.5680\n",
      "Epoch 1215/1500\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 0.9992 - val_loss: 4.1020 - val_accuracy: 0.5680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1216/1500\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0036 - accuracy: 0.9992 - val_loss: 4.0525 - val_accuracy: 0.5680\n",
      "Epoch 1217/1500\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0056 - accuracy: 0.9992 - val_loss: 3.9876 - val_accuracy: 0.5559\n",
      "Epoch 1218/1500\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0045 - accuracy: 0.9984 - val_loss: 4.0084 - val_accuracy: 0.5861\n",
      "Epoch 1219/1500\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0030 - accuracy: 0.9992 - val_loss: 3.9625 - val_accuracy: 0.5559\n",
      "Epoch 1220/1500\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 4.0521 - val_accuracy: 0.5650\n",
      "Epoch 1221/1500\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0059 - accuracy: 0.9976 - val_loss: 3.9444 - val_accuracy: 0.5619\n",
      "Epoch 1222/1500\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0014 - accuracy: 0.9992 - val_loss: 3.9843 - val_accuracy: 0.5770\n",
      "Epoch 1223/1500\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0073 - accuracy: 0.9976 - val_loss: 4.0608 - val_accuracy: 0.5740\n",
      "Epoch 1224/1500\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 4.0346 - val_accuracy: 0.5710\n",
      "Epoch 1225/1500\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0147 - accuracy: 0.9959 - val_loss: 4.0793 - val_accuracy: 0.5680\n",
      "Epoch 1226/1500\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.9235 - val_accuracy: 0.5589\n",
      "Epoch 1227/1500\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0030 - accuracy: 0.9992 - val_loss: 4.0910 - val_accuracy: 0.5710\n",
      "Epoch 1228/1500\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0024 - accuracy: 0.9984 - val_loss: 4.0668 - val_accuracy: 0.5680\n",
      "Epoch 1229/1500\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0117 - accuracy: 0.9976 - val_loss: 4.2578 - val_accuracy: 0.5710\n",
      "Epoch 1230/1500\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 1.4096e-04 - accuracy: 1.0000 - val_loss: 4.2030 - val_accuracy: 0.5619\n",
      "Epoch 1231/1500\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0030 - accuracy: 0.9992 - val_loss: 4.1169 - val_accuracy: 0.5680\n",
      "Epoch 1232/1500\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0097 - accuracy: 0.9967 - val_loss: 4.0579 - val_accuracy: 0.5710\n",
      "Epoch 1233/1500\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 7.8668e-04 - accuracy: 1.0000 - val_loss: 4.1859 - val_accuracy: 0.5770\n",
      "Epoch 1234/1500\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1741 - val_accuracy: 0.5680\n",
      "Epoch 1235/1500\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0062 - accuracy: 0.9967 - val_loss: 4.2962 - val_accuracy: 0.5619\n",
      "Epoch 1236/1500\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9984 - val_loss: 4.2156 - val_accuracy: 0.5559\n",
      "Epoch 1237/1500\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0378 - accuracy: 0.9902 - val_loss: 4.1135 - val_accuracy: 0.5650\n",
      "Epoch 1238/1500\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 4.0686 - val_accuracy: 0.5619\n",
      "Epoch 1239/1500\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0176 - accuracy: 0.9951 - val_loss: 4.1197 - val_accuracy: 0.5589\n",
      "Epoch 1240/1500\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0013 - accuracy: 0.9992 - val_loss: 4.0378 - val_accuracy: 0.5650\n",
      "Epoch 1241/1500\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0101 - accuracy: 0.9984 - val_loss: 4.1685 - val_accuracy: 0.5498\n",
      "Epoch 1242/1500\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 4.3811 - val_accuracy: 0.5468\n",
      "Epoch 1243/1500\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 4.3823 - val_accuracy: 0.5438\n",
      "Epoch 1244/1500\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 9.4899e-04 - accuracy: 1.0000 - val_loss: 4.1376 - val_accuracy: 0.5589\n",
      "Epoch 1245/1500\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0048 - accuracy: 0.9992 - val_loss: 4.1105 - val_accuracy: 0.5468\n",
      "Epoch 1246/1500\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0020 - accuracy: 0.9984 - val_loss: 4.1216 - val_accuracy: 0.5529\n",
      "Epoch 1247/1500\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 4.1929 - val_accuracy: 0.5710\n",
      "Epoch 1248/1500\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0047 - accuracy: 0.9984 - val_loss: 4.2964 - val_accuracy: 0.5619\n",
      "Epoch 1249/1500\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0060 - accuracy: 0.9984 - val_loss: 4.2404 - val_accuracy: 0.5619\n",
      "Epoch 1250/1500\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0149 - accuracy: 0.9976 - val_loss: 4.2578 - val_accuracy: 0.5650\n",
      "Epoch 1251/1500\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0103 - accuracy: 0.9984 - val_loss: 4.3513 - val_accuracy: 0.5740\n",
      "Epoch 1252/1500\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0252 - accuracy: 0.9976 - val_loss: 4.5149 - val_accuracy: 0.5378\n",
      "Epoch 1253/1500\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0072 - accuracy: 0.9976 - val_loss: 4.2854 - val_accuracy: 0.5468\n",
      "Epoch 1254/1500\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0172 - accuracy: 0.9984 - val_loss: 4.1143 - val_accuracy: 0.5378\n",
      "Epoch 1255/1500\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9992 - val_loss: 4.1052 - val_accuracy: 0.5317\n",
      "Epoch 1256/1500\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 4.5888 - val_accuracy: 0.5498\n",
      "Epoch 1257/1500\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 4.1665 - val_accuracy: 0.5468\n",
      "Epoch 1258/1500\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0233 - accuracy: 0.9984 - val_loss: 4.2474 - val_accuracy: 0.5529\n",
      "Epoch 1259/1500\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0047 - accuracy: 0.9984 - val_loss: 4.3200 - val_accuracy: 0.5498\n",
      "Epoch 1260/1500\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 7.2919e-04 - accuracy: 1.0000 - val_loss: 4.2599 - val_accuracy: 0.5468\n",
      "Epoch 1261/1500\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0033 - accuracy: 0.9984 - val_loss: 4.3086 - val_accuracy: 0.5619\n",
      "Epoch 1262/1500\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 4.6009e-04 - accuracy: 1.0000 - val_loss: 4.3473 - val_accuracy: 0.5529\n",
      "Epoch 1263/1500\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0124 - accuracy: 0.9984 - val_loss: 4.2502 - val_accuracy: 0.5680\n",
      "Epoch 1264/1500\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 2.2481e-04 - accuracy: 1.0000 - val_loss: 4.2263 - val_accuracy: 0.5589\n",
      "Epoch 1265/1500\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0036 - accuracy: 0.9992 - val_loss: 4.1143 - val_accuracy: 0.5680\n",
      "Epoch 1266/1500\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0084 - accuracy: 0.9951 - val_loss: 4.1010 - val_accuracy: 0.5408\n",
      "Epoch 1267/1500\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 4.1202 - val_accuracy: 0.5559\n",
      "Epoch 1268/1500\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0022 - accuracy: 0.9992 - val_loss: 4.1487 - val_accuracy: 0.5529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1269/1500\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0034 - accuracy: 0.9984 - val_loss: 4.1447 - val_accuracy: 0.5498\n",
      "Epoch 1270/1500\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.3960 - val_accuracy: 0.5408\n",
      "Epoch 1271/1500\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 5.1916e-04 - accuracy: 1.0000 - val_loss: 4.2696 - val_accuracy: 0.5619\n",
      "Epoch 1272/1500\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0327 - accuracy: 0.9935 - val_loss: 4.2980 - val_accuracy: 0.5498\n",
      "Epoch 1273/1500\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 4.4443 - val_accuracy: 0.5710\n",
      "Epoch 1274/1500\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0157 - accuracy: 0.9984 - val_loss: 4.3926 - val_accuracy: 0.5529\n",
      "Epoch 1275/1500\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 4.4740 - val_accuracy: 0.5559\n",
      "Epoch 1276/1500\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0581 - accuracy: 0.9976 - val_loss: 4.3262 - val_accuracy: 0.5468\n",
      "Epoch 1277/1500\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0042 - accuracy: 0.9992 - val_loss: 4.3976 - val_accuracy: 0.5378\n",
      "Epoch 1278/1500\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0090 - accuracy: 0.9984 - val_loss: 4.2373 - val_accuracy: 0.5529\n",
      "Epoch 1279/1500\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0021 - accuracy: 0.9992 - val_loss: 4.2556 - val_accuracy: 0.5619\n",
      "Epoch 1280/1500\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 9.7856e-04 - accuracy: 1.0000 - val_loss: 4.3219 - val_accuracy: 0.5408\n",
      "Epoch 1281/1500\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0245 - accuracy: 0.9959 - val_loss: 4.1804 - val_accuracy: 0.5408\n",
      "Epoch 1282/1500\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0259 - accuracy: 0.9992 - val_loss: 4.1933 - val_accuracy: 0.5498\n",
      "Epoch 1283/1500\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0069 - accuracy: 0.9984 - val_loss: 4.0192 - val_accuracy: 0.5680\n",
      "Epoch 1284/1500\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0043 - accuracy: 0.9976 - val_loss: 3.9773 - val_accuracy: 0.5589\n",
      "Epoch 1285/1500\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0143 - accuracy: 0.9984 - val_loss: 4.0231 - val_accuracy: 0.5529\n",
      "Epoch 1286/1500\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0051 - accuracy: 0.9984 - val_loss: 4.0796 - val_accuracy: 0.5559\n",
      "Epoch 1287/1500\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.9992 - val_accuracy: 0.5498\n",
      "Epoch 1288/1500\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0015 - accuracy: 0.9992 - val_loss: 4.0961 - val_accuracy: 0.5589\n",
      "Epoch 1289/1500\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0078 - accuracy: 0.9976 - val_loss: 4.1383 - val_accuracy: 0.5589\n",
      "Epoch 1290/1500\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0333 - accuracy: 0.9967 - val_loss: 4.1738 - val_accuracy: 0.5529\n",
      "Epoch 1291/1500\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 9.6107e-04 - accuracy: 1.0000 - val_loss: 4.3567 - val_accuracy: 0.5529\n",
      "Epoch 1292/1500\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0047 - accuracy: 0.9984 - val_loss: 4.2638 - val_accuracy: 0.5619\n",
      "Epoch 1293/1500\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0131 - accuracy: 0.9984 - val_loss: 4.2518 - val_accuracy: 0.5468\n",
      "Epoch 1294/1500\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0061 - accuracy: 0.9976 - val_loss: 4.3296 - val_accuracy: 0.5589\n",
      "Epoch 1295/1500\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0053 - accuracy: 0.9984 - val_loss: 4.3781 - val_accuracy: 0.5559\n",
      "Epoch 1296/1500\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0282 - accuracy: 0.9927 - val_loss: 4.3902 - val_accuracy: 0.5740\n",
      "Epoch 1297/1500\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0092 - accuracy: 0.9976 - val_loss: 4.3376 - val_accuracy: 0.5740\n",
      "Epoch 1298/1500\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0044 - accuracy: 0.9992 - val_loss: 4.1902 - val_accuracy: 0.5650\n",
      "Epoch 1299/1500\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0016 - accuracy: 0.9992 - val_loss: 4.2499 - val_accuracy: 0.5589\n",
      "Epoch 1300/1500\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0013 - accuracy: 0.9984 - val_loss: 4.2204 - val_accuracy: 0.5589\n",
      "Epoch 1301/1500\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0137 - accuracy: 0.9951 - val_loss: 4.2764 - val_accuracy: 0.5589\n",
      "Epoch 1302/1500\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0172 - accuracy: 0.9992 - val_loss: 4.1027 - val_accuracy: 0.5529\n",
      "Epoch 1303/1500\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9984 - val_loss: 4.1636 - val_accuracy: 0.5619\n",
      "Epoch 1304/1500\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0176 - accuracy: 0.9984 - val_loss: 4.1706 - val_accuracy: 0.5710\n",
      "Epoch 1305/1500\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0125 - accuracy: 0.9976 - val_loss: 4.4029 - val_accuracy: 0.5498\n",
      "Epoch 1306/1500\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0159 - accuracy: 0.9984 - val_loss: 4.3487 - val_accuracy: 0.5438\n",
      "Epoch 1307/1500\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0081 - accuracy: 0.9992 - val_loss: 4.4937 - val_accuracy: 0.5529\n",
      "Epoch 1308/1500\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0114 - accuracy: 0.9967 - val_loss: 4.3944 - val_accuracy: 0.5498\n",
      "Epoch 1309/1500\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0044 - accuracy: 0.9967 - val_loss: 4.2207 - val_accuracy: 0.5529\n",
      "Epoch 1310/1500\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1694 - val_accuracy: 0.5498\n",
      "Epoch 1311/1500\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 9.1806e-04 - accuracy: 1.0000 - val_loss: 4.2661 - val_accuracy: 0.5529\n",
      "Epoch 1312/1500\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 7.3435e-04 - accuracy: 1.0000 - val_loss: 4.2696 - val_accuracy: 0.5619\n",
      "Epoch 1313/1500\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 9.8339e-04 - accuracy: 1.0000 - val_loss: 4.3158 - val_accuracy: 0.5589\n",
      "Epoch 1314/1500\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0050 - accuracy: 0.9984 - val_loss: 4.4128 - val_accuracy: 0.5589\n",
      "Epoch 1315/1500\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 4.4332 - val_accuracy: 0.5468\n",
      "Epoch 1316/1500\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0020 - accuracy: 0.9984 - val_loss: 4.4313 - val_accuracy: 0.5498\n",
      "Epoch 1317/1500\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0063 - accuracy: 0.9976 - val_loss: 4.5899 - val_accuracy: 0.5408\n",
      "Epoch 1318/1500\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0022 - accuracy: 0.9992 - val_loss: 4.2535 - val_accuracy: 0.5529\n",
      "Epoch 1319/1500\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0094 - accuracy: 0.9976 - val_loss: 4.2113 - val_accuracy: 0.5468\n",
      "Epoch 1320/1500\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0048 - accuracy: 0.9992 - val_loss: 4.2302 - val_accuracy: 0.5559\n",
      "Epoch 1321/1500\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0013 - accuracy: 0.9992 - val_loss: 4.1953 - val_accuracy: 0.5680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1322/1500\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 2.6137e-04 - accuracy: 1.0000 - val_loss: 4.1924 - val_accuracy: 0.5650\n",
      "Epoch 1323/1500\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0047 - accuracy: 0.9992 - val_loss: 4.6020 - val_accuracy: 0.5438\n",
      "Epoch 1324/1500\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0089 - accuracy: 0.9984 - val_loss: 4.2915 - val_accuracy: 0.5529\n",
      "Epoch 1325/1500\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0058 - accuracy: 0.9984 - val_loss: 4.3214 - val_accuracy: 0.5801\n",
      "Epoch 1326/1500\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0122 - accuracy: 0.9976 - val_loss: 4.3726 - val_accuracy: 0.5680\n",
      "Epoch 1327/1500\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0085 - accuracy: 0.9976 - val_loss: 4.3208 - val_accuracy: 0.5619\n",
      "Epoch 1328/1500\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0016 - accuracy: 0.9992 - val_loss: 4.1657 - val_accuracy: 0.5619\n",
      "Epoch 1329/1500\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0201 - accuracy: 0.9943 - val_loss: 4.2289 - val_accuracy: 0.5498\n",
      "Epoch 1330/1500\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0019 - accuracy: 0.9992 - val_loss: 4.1729 - val_accuracy: 0.5529\n",
      "Epoch 1331/1500\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0053 - accuracy: 0.9984 - val_loss: 4.1868 - val_accuracy: 0.5529\n",
      "Epoch 1332/1500\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0203 - accuracy: 0.9951 - val_loss: 4.3360 - val_accuracy: 0.5589\n",
      "Epoch 1333/1500\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 6.9669e-04 - accuracy: 1.0000 - val_loss: 4.1562 - val_accuracy: 0.5619\n",
      "Epoch 1334/1500\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0058 - accuracy: 0.9992 - val_loss: 4.2738 - val_accuracy: 0.5589\n",
      "Epoch 1335/1500\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.2780 - val_accuracy: 0.5680\n",
      "Epoch 1336/1500\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0095 - accuracy: 0.9992 - val_loss: 4.3279 - val_accuracy: 0.5710\n",
      "Epoch 1337/1500\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0099 - accuracy: 0.9967 - val_loss: 4.1278 - val_accuracy: 0.5589\n",
      "Epoch 1338/1500\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0064 - accuracy: 0.9984 - val_loss: 4.2729 - val_accuracy: 0.5498\n",
      "Epoch 1339/1500\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0111 - accuracy: 0.9959 - val_loss: 4.3144 - val_accuracy: 0.5498\n",
      "Epoch 1340/1500\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0117 - accuracy: 0.9951 - val_loss: 4.2155 - val_accuracy: 0.5831\n",
      "Epoch 1341/1500\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 8.2278e-04 - accuracy: 1.0000 - val_loss: 4.3313 - val_accuracy: 0.5498\n",
      "Epoch 1342/1500\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0126 - accuracy: 0.9992 - val_loss: 4.3200 - val_accuracy: 0.5740\n",
      "Epoch 1343/1500\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0138 - accuracy: 0.9984 - val_loss: 4.0728 - val_accuracy: 0.5740\n",
      "Epoch 1344/1500\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.2885 - val_accuracy: 0.5650\n",
      "Epoch 1345/1500\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 5.3022e-04 - accuracy: 1.0000 - val_loss: 4.4215 - val_accuracy: 0.5680\n",
      "Epoch 1346/1500\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0072 - accuracy: 0.9992 - val_loss: 4.6533 - val_accuracy: 0.5438\n",
      "Epoch 1347/1500\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0017 - accuracy: 0.9992 - val_loss: 4.2728 - val_accuracy: 0.5740\n",
      "Epoch 1348/1500\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0150 - accuracy: 0.9984 - val_loss: 4.4362 - val_accuracy: 0.5378\n",
      "Epoch 1349/1500\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0034 - accuracy: 0.9992 - val_loss: 4.2359 - val_accuracy: 0.5619\n",
      "Epoch 1350/1500\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 5.0988e-04 - accuracy: 1.0000 - val_loss: 4.3361 - val_accuracy: 0.5438\n",
      "Epoch 1351/1500\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0027 - accuracy: 0.9992 - val_loss: 4.2408 - val_accuracy: 0.5498\n",
      "Epoch 1352/1500\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0041 - accuracy: 0.9984 - val_loss: 4.4410 - val_accuracy: 0.5619\n",
      "Epoch 1353/1500\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.4889 - val_accuracy: 0.5650\n",
      "Epoch 1354/1500\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0020 - accuracy: 0.9992 - val_loss: 4.5925 - val_accuracy: 0.5589\n",
      "Epoch 1355/1500\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 4.5902 - val_accuracy: 0.5589\n",
      "Epoch 1356/1500\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0040 - accuracy: 0.9992 - val_loss: 4.2192 - val_accuracy: 0.5650\n",
      "Epoch 1357/1500\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0127 - accuracy: 0.9984 - val_loss: 4.1673 - val_accuracy: 0.5740\n",
      "Epoch 1358/1500\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 3.2321e-04 - accuracy: 1.0000 - val_loss: 4.2244 - val_accuracy: 0.5710\n",
      "Epoch 1359/1500\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0104 - accuracy: 0.9967 - val_loss: 4.4037 - val_accuracy: 0.5650\n",
      "Epoch 1360/1500\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0020 - accuracy: 0.9992 - val_loss: 4.2563 - val_accuracy: 0.5559\n",
      "Epoch 1361/1500\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.3317 - val_accuracy: 0.5650\n",
      "Epoch 1362/1500\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 3.3227e-04 - accuracy: 1.0000 - val_loss: 4.1757 - val_accuracy: 0.5589\n",
      "Epoch 1363/1500\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0135 - accuracy: 0.9984 - val_loss: 4.6105 - val_accuracy: 0.5589\n",
      "Epoch 1364/1500\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0059 - accuracy: 0.9967 - val_loss: 4.4945 - val_accuracy: 0.5589\n",
      "Epoch 1365/1500\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 4.3181e-04 - accuracy: 1.0000 - val_loss: 4.5529 - val_accuracy: 0.5619\n",
      "Epoch 1366/1500\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0135 - accuracy: 0.9967 - val_loss: 4.0893 - val_accuracy: 0.5438\n",
      "Epoch 1367/1500\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0046 - accuracy: 0.9984 - val_loss: 4.4007 - val_accuracy: 0.5378\n",
      "Epoch 1368/1500\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0045 - accuracy: 0.9984 - val_loss: 4.3262 - val_accuracy: 0.5589\n",
      "Epoch 1369/1500\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0056 - accuracy: 0.9992 - val_loss: 4.4877 - val_accuracy: 0.5619\n",
      "Epoch 1370/1500\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.4376 - val_accuracy: 0.5559\n",
      "Epoch 1371/1500\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.5365 - val_accuracy: 0.5468\n",
      "Epoch 1372/1500\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 4.5823e-04 - accuracy: 1.0000 - val_loss: 4.3725 - val_accuracy: 0.5589\n",
      "Epoch 1373/1500\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0236 - accuracy: 0.9951 - val_loss: 4.6588 - val_accuracy: 0.5468\n",
      "Epoch 1374/1500\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0038 - accuracy: 0.9984 - val_loss: 4.3889 - val_accuracy: 0.5559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1375/1500\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0022 - accuracy: 0.9992 - val_loss: 4.3706 - val_accuracy: 0.5589\n",
      "Epoch 1376/1500\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0052 - accuracy: 0.9984 - val_loss: 4.3343 - val_accuracy: 0.5650\n",
      "Epoch 1377/1500\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0105 - accuracy: 0.9984 - val_loss: 4.2871 - val_accuracy: 0.5650\n",
      "Epoch 1378/1500\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0048 - accuracy: 0.9984 - val_loss: 4.3437 - val_accuracy: 0.5650\n",
      "Epoch 1379/1500\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0012 - accuracy: 0.9992 - val_loss: 4.3402 - val_accuracy: 0.5650\n",
      "Epoch 1380/1500\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0106 - accuracy: 0.9984 - val_loss: 4.3975 - val_accuracy: 0.5468\n",
      "Epoch 1381/1500\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 5.6272e-04 - accuracy: 1.0000 - val_loss: 4.3843 - val_accuracy: 0.5559\n",
      "Epoch 1382/1500\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 4.4802 - val_accuracy: 0.5559\n",
      "Epoch 1383/1500\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0052 - accuracy: 0.9976 - val_loss: 4.3032 - val_accuracy: 0.5529\n",
      "Epoch 1384/1500\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0151 - accuracy: 0.9984 - val_loss: 4.2425 - val_accuracy: 0.5529\n",
      "Epoch 1385/1500\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0034 - accuracy: 0.9992 - val_loss: 4.4069 - val_accuracy: 0.5650\n",
      "Epoch 1386/1500\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 9.8080e-04 - accuracy: 0.9992 - val_loss: 4.1715 - val_accuracy: 0.5498\n",
      "Epoch 1387/1500\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0047 - accuracy: 0.9984 - val_loss: 4.5511 - val_accuracy: 0.5468\n",
      "Epoch 1388/1500\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2442 - val_accuracy: 0.5589\n",
      "Epoch 1389/1500\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0075 - accuracy: 0.9967 - val_loss: 4.3156 - val_accuracy: 0.5589\n",
      "Epoch 1390/1500\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 6.5986e-04 - accuracy: 1.0000 - val_loss: 4.4096 - val_accuracy: 0.5529\n",
      "Epoch 1391/1500\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.5465 - val_accuracy: 0.5438\n",
      "Epoch 1392/1500\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0047 - accuracy: 0.9984 - val_loss: 4.4145 - val_accuracy: 0.5498\n",
      "Epoch 1393/1500\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0057 - accuracy: 0.9967 - val_loss: 4.4038 - val_accuracy: 0.5529\n",
      "Epoch 1394/1500\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0146 - accuracy: 0.9959 - val_loss: 4.4610 - val_accuracy: 0.5468\n",
      "Epoch 1395/1500\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0195 - accuracy: 0.9992 - val_loss: 4.3004 - val_accuracy: 0.5468\n",
      "Epoch 1396/1500\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0171 - accuracy: 0.9967 - val_loss: 4.3915 - val_accuracy: 0.5378\n",
      "Epoch 1397/1500\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 4.2771 - val_accuracy: 0.5468\n",
      "Epoch 1398/1500\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0268 - accuracy: 0.9967 - val_loss: 4.3251 - val_accuracy: 0.5378\n",
      "Epoch 1399/1500\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.3498 - val_accuracy: 0.5438\n",
      "Epoch 1400/1500\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0178 - accuracy: 0.9984 - val_loss: 4.6701 - val_accuracy: 0.5468\n",
      "Epoch 1401/1500\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0117 - accuracy: 0.9976 - val_loss: 4.5553 - val_accuracy: 0.5438\n",
      "Epoch 1402/1500\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0058 - accuracy: 0.9976 - val_loss: 4.4530 - val_accuracy: 0.5559\n",
      "Epoch 1403/1500\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0072 - accuracy: 0.9992 - val_loss: 4.3776 - val_accuracy: 0.5498\n",
      "Epoch 1404/1500\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0017 - accuracy: 0.9992 - val_loss: 4.2693 - val_accuracy: 0.5589\n",
      "Epoch 1405/1500\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0534 - accuracy: 0.9943 - val_loss: 5.0184 - val_accuracy: 0.5347\n",
      "Epoch 1406/1500\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0117 - accuracy: 0.9967 - val_loss: 4.4721 - val_accuracy: 0.5589\n",
      "Epoch 1407/1500\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 9.6154e-04 - accuracy: 0.9992 - val_loss: 4.4330 - val_accuracy: 0.5529\n",
      "Epoch 1408/1500\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0013 - accuracy: 0.9992 - val_loss: 4.3129 - val_accuracy: 0.5529\n",
      "Epoch 1409/1500\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0049 - accuracy: 0.9984 - val_loss: 4.2488 - val_accuracy: 0.5438\n",
      "Epoch 1410/1500\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0245 - accuracy: 0.9976 - val_loss: 4.4377 - val_accuracy: 0.5589\n",
      "Epoch 1411/1500\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0048 - accuracy: 0.9984 - val_loss: 4.4191 - val_accuracy: 0.5529\n",
      "Epoch 1412/1500\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0039 - accuracy: 0.9984 - val_loss: 4.3714 - val_accuracy: 0.5498\n",
      "Epoch 1413/1500\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0159 - accuracy: 0.9992 - val_loss: 4.4242 - val_accuracy: 0.5498\n",
      "Epoch 1414/1500\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0014 - accuracy: 0.9992 - val_loss: 4.4320 - val_accuracy: 0.5438\n",
      "Epoch 1415/1500\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0047 - accuracy: 0.9976 - val_loss: 4.2652 - val_accuracy: 0.5559\n",
      "Epoch 1416/1500\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0051 - accuracy: 0.9984 - val_loss: 4.2957 - val_accuracy: 0.5710\n",
      "Epoch 1417/1500\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 4.6703 - val_accuracy: 0.5529\n",
      "Epoch 1418/1500\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0045 - accuracy: 0.9984 - val_loss: 4.7375 - val_accuracy: 0.5498\n",
      "Epoch 1419/1500\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0265 - accuracy: 0.9976 - val_loss: 4.5273 - val_accuracy: 0.5468\n",
      "Epoch 1420/1500\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 4.1293 - val_accuracy: 0.5650\n",
      "Epoch 1421/1500\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0062 - accuracy: 0.9984 - val_loss: 4.1790 - val_accuracy: 0.5619\n",
      "Epoch 1422/1500\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 3.0598e-04 - accuracy: 1.0000 - val_loss: 4.1596 - val_accuracy: 0.5529\n",
      "Epoch 1423/1500\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0024 - accuracy: 0.9984 - val_loss: 4.1420 - val_accuracy: 0.5559\n",
      "Epoch 1424/1500\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 5.4420e-04 - accuracy: 1.0000 - val_loss: 4.1255 - val_accuracy: 0.5559\n",
      "Epoch 1425/1500\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0052 - accuracy: 0.9976 - val_loss: 4.5005 - val_accuracy: 0.5317\n",
      "Epoch 1426/1500\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0105 - accuracy: 0.9976 - val_loss: 4.3988 - val_accuracy: 0.5498\n",
      "Epoch 1427/1500\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0086 - accuracy: 0.9976 - val_loss: 4.2326 - val_accuracy: 0.5680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1428/1500\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 7.0605e-04 - accuracy: 1.0000 - val_loss: 4.2295 - val_accuracy: 0.5559\n",
      "Epoch 1429/1500\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 4.1515 - val_accuracy: 0.5559\n",
      "Epoch 1430/1500\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0038 - accuracy: 0.9984 - val_loss: 4.3774 - val_accuracy: 0.5498\n",
      "Epoch 1431/1500\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0044 - accuracy: 0.9984 - val_loss: 4.2294 - val_accuracy: 0.5740\n",
      "Epoch 1432/1500\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0087 - accuracy: 0.9967 - val_loss: 4.5148 - val_accuracy: 0.5559\n",
      "Epoch 1433/1500\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0111 - accuracy: 0.9984 - val_loss: 4.4730 - val_accuracy: 0.5589\n",
      "Epoch 1434/1500\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0097 - accuracy: 0.9967 - val_loss: 4.5994 - val_accuracy: 0.5468\n",
      "Epoch 1435/1500\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 4.3102 - val_accuracy: 0.5529\n",
      "Epoch 1436/1500\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 4.4908 - val_accuracy: 0.5589\n",
      "Epoch 1437/1500\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0023 - accuracy: 0.9992 - val_loss: 4.4302 - val_accuracy: 0.5680\n",
      "Epoch 1438/1500\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 2.5013e-04 - accuracy: 1.0000 - val_loss: 4.4723 - val_accuracy: 0.5619\n",
      "Epoch 1439/1500\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 4.7494 - val_accuracy: 0.5287\n",
      "Epoch 1440/1500\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0013 - accuracy: 0.9992 - val_loss: 4.2918 - val_accuracy: 0.5559\n",
      "Epoch 1441/1500\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0052 - accuracy: 0.9992 - val_loss: 4.6661 - val_accuracy: 0.5468\n",
      "Epoch 1442/1500\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0022 - accuracy: 0.9992 - val_loss: 4.5504 - val_accuracy: 0.5378\n",
      "Epoch 1443/1500\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0040 - accuracy: 0.9992 - val_loss: 4.4139 - val_accuracy: 0.5559\n",
      "Epoch 1444/1500\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.5637 - val_accuracy: 0.5529\n",
      "Epoch 1445/1500\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0019 - accuracy: 0.9992 - val_loss: 4.5397 - val_accuracy: 0.5619\n",
      "Epoch 1446/1500\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 3.5625e-04 - accuracy: 1.0000 - val_loss: 4.3937 - val_accuracy: 0.5559\n",
      "Epoch 1447/1500\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0027 - accuracy: 0.9992 - val_loss: 4.3862 - val_accuracy: 0.5408\n",
      "Epoch 1448/1500\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 9.2829e-04 - accuracy: 1.0000 - val_loss: 4.1318 - val_accuracy: 0.5529\n",
      "Epoch 1449/1500\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0161 - accuracy: 0.9976 - val_loss: 4.1949 - val_accuracy: 0.5650\n",
      "Epoch 1450/1500\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0041 - accuracy: 0.9976 - val_loss: 4.3453 - val_accuracy: 0.5529\n",
      "Epoch 1451/1500\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0030 - accuracy: 0.9984 - val_loss: 4.6332 - val_accuracy: 0.5498\n",
      "Epoch 1452/1500\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0077 - accuracy: 0.9984 - val_loss: 4.6286 - val_accuracy: 0.5710\n",
      "Epoch 1453/1500\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0109 - accuracy: 0.9992 - val_loss: 4.1566 - val_accuracy: 0.5468\n",
      "Epoch 1454/1500\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.1529 - val_accuracy: 0.5529\n",
      "Epoch 1455/1500\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0102 - accuracy: 0.9984 - val_loss: 4.2695 - val_accuracy: 0.5529\n",
      "Epoch 1456/1500\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0045 - accuracy: 0.9992 - val_loss: 4.4426 - val_accuracy: 0.5498\n",
      "Epoch 1457/1500\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 6.1757e-04 - accuracy: 1.0000 - val_loss: 4.4861 - val_accuracy: 0.5589\n",
      "Epoch 1458/1500\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0083 - accuracy: 0.9967 - val_loss: 4.5340 - val_accuracy: 0.5468\n",
      "Epoch 1459/1500\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0062 - accuracy: 0.9992 - val_loss: 4.4403 - val_accuracy: 0.5408\n",
      "Epoch 1460/1500\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0035 - accuracy: 0.9984 - val_loss: 4.4639 - val_accuracy: 0.5589\n",
      "Epoch 1461/1500\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0033 - accuracy: 0.9984 - val_loss: 4.4654 - val_accuracy: 0.5347\n",
      "Epoch 1462/1500\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0080 - accuracy: 0.9967 - val_loss: 4.0816 - val_accuracy: 0.5619\n",
      "Epoch 1463/1500\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 8.9660e-04 - accuracy: 1.0000 - val_loss: 4.2214 - val_accuracy: 0.5559\n",
      "Epoch 1464/1500\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0015 - accuracy: 0.9992 - val_loss: 4.2903 - val_accuracy: 0.5680\n",
      "Epoch 1465/1500\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0013 - accuracy: 0.9992 - val_loss: 4.3488 - val_accuracy: 0.5498\n",
      "Epoch 1466/1500\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0022 - accuracy: 0.9992 - val_loss: 4.3486 - val_accuracy: 0.5589\n",
      "Epoch 1467/1500\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0039 - accuracy: 0.9984 - val_loss: 4.4656 - val_accuracy: 0.5619\n",
      "Epoch 1468/1500\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0014 - accuracy: 0.9992 - val_loss: 4.2834 - val_accuracy: 0.5559\n",
      "Epoch 1469/1500\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0029 - accuracy: 0.9984 - val_loss: 4.4448 - val_accuracy: 0.5619\n",
      "Epoch 1470/1500\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 4.3462 - val_accuracy: 0.5680\n",
      "Epoch 1471/1500\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0017 - accuracy: 0.9992 - val_loss: 4.4752 - val_accuracy: 0.5529\n",
      "Epoch 1472/1500\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0062 - accuracy: 0.9992 - val_loss: 4.3391 - val_accuracy: 0.5770\n",
      "Epoch 1473/1500\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0016 - accuracy: 0.9992 - val_loss: 4.2687 - val_accuracy: 0.5710\n",
      "Epoch 1474/1500\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.2794 - val_accuracy: 0.5650\n",
      "Epoch 1475/1500\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0223 - accuracy: 0.9976 - val_loss: 4.1797 - val_accuracy: 0.5710\n",
      "Epoch 1476/1500\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0091 - accuracy: 0.9967 - val_loss: 4.3155 - val_accuracy: 0.5589\n",
      "Epoch 1477/1500\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0038 - accuracy: 0.9992 - val_loss: 4.2265 - val_accuracy: 0.5559\n",
      "Epoch 1478/1500\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0055 - accuracy: 0.9992 - val_loss: 4.4503 - val_accuracy: 0.5861\n",
      "Epoch 1479/1500\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 3.7201e-04 - accuracy: 1.0000 - val_loss: 4.3968 - val_accuracy: 0.5498\n",
      "Epoch 1480/1500\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 7.2482e-04 - accuracy: 1.0000 - val_loss: 4.7218 - val_accuracy: 0.5740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1481/1500\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0050 - accuracy: 0.9984 - val_loss: 4.6777 - val_accuracy: 0.5650\n",
      "Epoch 1482/1500\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 4.6383 - val_accuracy: 0.5650\n",
      "Epoch 1483/1500\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 5.0640e-04 - accuracy: 1.0000 - val_loss: 4.7585 - val_accuracy: 0.5650\n",
      "Epoch 1484/1500\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0063 - accuracy: 0.9984 - val_loss: 4.4115 - val_accuracy: 0.5710\n",
      "Epoch 1485/1500\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 2.6362e-04 - accuracy: 1.0000 - val_loss: 4.4648 - val_accuracy: 0.5378\n",
      "Epoch 1486/1500\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0254 - accuracy: 0.9976 - val_loss: 4.3769 - val_accuracy: 0.5559\n",
      "Epoch 1487/1500\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0039 - accuracy: 0.9992 - val_loss: 4.3376 - val_accuracy: 0.5529\n",
      "Epoch 1488/1500\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0025 - accuracy: 0.9984 - val_loss: 4.3497 - val_accuracy: 0.5498\n",
      "Epoch 1489/1500\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0050 - accuracy: 0.9984 - val_loss: 4.3495 - val_accuracy: 0.5529\n",
      "Epoch 1490/1500\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 7.2119e-04 - accuracy: 1.0000 - val_loss: 4.2374 - val_accuracy: 0.5650\n",
      "Epoch 1491/1500\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0103 - accuracy: 0.9984 - val_loss: 4.3163 - val_accuracy: 0.5589\n",
      "Epoch 1492/1500\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0075 - accuracy: 0.9984 - val_loss: 4.5620 - val_accuracy: 0.5650\n",
      "Epoch 1493/1500\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0028 - accuracy: 0.9984 - val_loss: 4.6157 - val_accuracy: 0.5619\n",
      "Epoch 1494/1500\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 6.7398e-04 - accuracy: 1.0000 - val_loss: 4.5818 - val_accuracy: 0.5498\n",
      "Epoch 1495/1500\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 4.4604 - val_accuracy: 0.5468\n",
      "Epoch 1496/1500\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0066 - accuracy: 0.9976 - val_loss: 4.4217 - val_accuracy: 0.5650\n",
      "Epoch 1497/1500\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 0.0043 - accuracy: 0.9992 - val_loss: 4.3997 - val_accuracy: 0.5498\n",
      "Epoch 1498/1500\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0106 - accuracy: 0.9967 - val_loss: 4.4886 - val_accuracy: 0.5589\n",
      "Epoch 1499/1500\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 1.41917\n",
      "39/38 - 14s - loss: 0.0053 - accuracy: 0.9992 - val_loss: 4.6467 - val_accuracy: 0.5438\n",
      "Epoch 1500/1500\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 1.41917\n",
      "39/38 - 13s - loss: 4.2710e-04 - accuracy: 1.0000 - val_loss: 4.3978 - val_accuracy: 0.5498\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='piece_model_1.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "piece_hist = piece_model.fit_generator(generator=piece_train_iter, \n",
    "                          steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                          validation_data=piece_valid_iter, \n",
    "                          validation_steps=STEP_SIZE_VALID,\n",
    "                          epochs=1500, \n",
    "                          callbacks=[checkpointer], \n",
    "                          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.46865672\n"
     ]
    }
   ],
   "source": [
    "#Reset test iterator\n",
    "STEP_SIZE_TEST = piece_test_iter.n/piece_test_iter.batch_size\n",
    "piece_test_iter.reset()\n",
    "# load the weights that yielded the best validation accuracy\n",
    "piece_model.load_weights('piece_model.weights.best.hdf5')\n",
    "# evaluate and print test accuracy\n",
    "score = piece_model.evaluate_generator(generator=piece_test_iter,steps=STEP_SIZE_TEST)\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 0s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "piece_test_iter.reset()\n",
    "piece_pred = piece_model.predict_generator(piece_test_iter,steps=STEP_SIZE_TEST,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 2 0 0 2 0 2 2 4 0 0 2 0 4 3 0 0 2 2 2 0 2 2 4 0 2 4 0 2 2 1 5 5 1\n",
      " 0 4 0 0 0 2 2 2 2 5 1 4 1 4 4 1 3 4 4 4 1 1 1 1 1 1 5 4 1 2 4 1 4 1 4 4 1\n",
      " 5 1 4 1 1 1 0 0 3 0 0 3 2 5 4 0 3 0 3 3 0 2 2 2 2 2 2 2 2 2 4 0 4 5 5 2 0\n",
      " 0 4 2 1 1 1 2 2 1 5 2 0 2 4 0 4 0 1 2 2 0 1 2 4 1 4 4 4 2 2 0 3 3 3 3 3 3\n",
      " 3 3 1 3 3 0 5 5 1 3 5 2 1 1 4 2 5 3 4 4 2 2 2 0 2 0 2 4 0 1 2 2 5 1 0 0 0\n",
      " 0 0 1 2 0 1 0 4 2 4 4 4 0 4 4 0 5 2 1 4 1 1 0 2 4 1 2 1 1 4 1 1 5 2 1 4 4\n",
      " 4 5 5 2 5 4 0 2 5 5 1 2 2 5 2 2 2 3 2 0 5 5 5 3 3 3 0 5 5 0 5 5 4 4 5 1 5\n",
      " 5 2 4 1 5 0 5 2 1 5 2 5 5 1 1 5 5 5 5 5 5 1 1 1 2 1 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6]\n"
     ]
    }
   ],
   "source": [
    "predicted_class_indices=np.argmax(piece_pred,axis=1)\n",
    "print(predicted_class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (piece_test_iter.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "truth =  [labels[k] for k in piece_test_iter.classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[labels[k] for k in piece_test_iter.classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bishop/1538777310.1624303.jpg',\n",
       " 'bishop/1538777556.4695792.jpg',\n",
       " 'bishop/1538777675.2653856.jpg',\n",
       " 'bishop/1538777891.4931645.jpg',\n",
       " 'bishop/1538778091.9695802.jpg',\n",
       " 'bishop/1538778093.2397656.jpg',\n",
       " 'bishop/1538778214.3746974.jpg',\n",
       " 'bishop/1538778215.8878355.jpg',\n",
       " 'bishop/1538778410.6081142.jpg',\n",
       " 'bishop/1538778427.8668559.jpg',\n",
       " 'bishop/1538778537.1458335.jpg',\n",
       " 'bishop/1538778538.3017461.jpg',\n",
       " 'bishop/1538778643.4925923.jpg',\n",
       " 'bishop/1538778653.1963263.jpg',\n",
       " 'bishop/1538778656.6147914.jpg',\n",
       " 'bishop/1538778729.1405196.jpg',\n",
       " 'bishop/1538778752.467536.jpg',\n",
       " 'bishop/1538778832.1937056.jpg',\n",
       " 'bishop/1538778833.610855.jpg',\n",
       " 'bishop/1538778878.3252861.jpg',\n",
       " 'bishop/1538778958.324379.jpg',\n",
       " 'bishop/1538779144.9958382.jpg',\n",
       " 'bishop/1538779155.2682924.jpg',\n",
       " 'bishop/1538779417.7487714.jpg',\n",
       " 'bishop/1538779519.5865016.jpg',\n",
       " 'bishop/1539017656.2202125_22.jpg',\n",
       " 'bishop/1539017656.6888525_5.jpg',\n",
       " 'bishop/1539023140.397658.jpg',\n",
       " 'bishop/1539023363.830272.jpg',\n",
       " 'bishop/1539023732.561326.jpg',\n",
       " 'bishop/1539024039.3749897.jpg',\n",
       " 'bishop/1539024424.9014952.jpg',\n",
       " 'bishop/1539024593.521976.jpg',\n",
       " 'bishop/1539024725.794323.jpg',\n",
       " 'bishop/1539024760.3120384.jpg',\n",
       " 'bishop/1539025334.2429533.jpg',\n",
       " 'bishop/1539025355.3753638.jpg',\n",
       " 'bishop/1539025373.0113628.jpg',\n",
       " 'bishop/1539025415.635129.jpg',\n",
       " 'bishop/1539025434.58347.jpg',\n",
       " 'bishop/1539025474.8601475.jpg',\n",
       " 'bishop/1539025486.6533895.jpg',\n",
       " 'bishop/1539025736.4874806.jpg',\n",
       " 'bishop/1539025801.510389.jpg',\n",
       " 'bishop/1539025832.837544.jpg',\n",
       " 'bishop/1539025959.458598.jpg',\n",
       " 'bishop/1539026035.275597.jpg',\n",
       " 'king/1538774883.9670901.jpg',\n",
       " 'king/1538775155.1271548.jpg',\n",
       " 'king/1538775504.1042295.jpg',\n",
       " 'king/1538776272.1332486.jpg',\n",
       " 'king/1538776522.496111.jpg',\n",
       " 'king/1538777095.0775242.jpg',\n",
       " 'king/1538777418.4191957.jpg',\n",
       " 'king/1538777707.301926.jpg',\n",
       " 'king/1538778011.0228102.jpg',\n",
       " 'king/1538778406.86247.jpg',\n",
       " 'king/1538778972.6889744.jpg',\n",
       " 'king/1538779020.8452945.jpg',\n",
       " 'king/1538779136.6977031.jpg',\n",
       " 'king/1538779354.1704876.jpg',\n",
       " 'king/1538779455.0068257.jpg',\n",
       " 'king/1538779987.557536.jpg',\n",
       " 'king/1538780110.995072.jpg',\n",
       " 'king/1539017656.0015128_29.jpg',\n",
       " 'king/1539017656.4857748_24.jpg',\n",
       " 'king/1539017656.5013957_31.jpg',\n",
       " 'king/1539017657.7939315_15.jpg',\n",
       " 'king/1539017657.8720484_58.jpg',\n",
       " 'king/1539017658.0594935_23.jpg',\n",
       " 'king/1539022987.3141916.jpg',\n",
       " 'king/1539023157.5381975.jpg',\n",
       " 'king/1539023185.6079886.jpg',\n",
       " 'king/1539023430.39102.jpg',\n",
       " 'king/1539023823.1830983.jpg',\n",
       " 'king/1539024597.23388.jpg',\n",
       " 'king/1539024744.5240357.jpg',\n",
       " 'king/1539025742.555007.jpg',\n",
       " 'king/1539026833.4767778.jpg',\n",
       " 'king/1539027947.6755114.jpg',\n",
       " 'knight/1538775492.0805464.jpg',\n",
       " 'knight/1538776219.5658782.jpg',\n",
       " 'knight/1538777112.8554816.jpg',\n",
       " 'knight/1538777213.003978.jpg',\n",
       " 'knight/1538778117.6826968.jpg',\n",
       " 'knight/1538778197.602519.jpg',\n",
       " 'knight/1538778212.992307.jpg',\n",
       " 'knight/1538778356.0347438.jpg',\n",
       " 'knight/1538778478.4419494.jpg',\n",
       " 'knight/1538778511.7367067.jpg',\n",
       " 'knight/1538778628.9804418.jpg',\n",
       " 'knight/1538778670.1169665.jpg',\n",
       " 'knight/1538778725.7980158.jpg',\n",
       " 'knight/1538778798.7536736.jpg',\n",
       " 'knight/1538779127.0457888.jpg',\n",
       " 'knight/1538779157.0249271.jpg',\n",
       " 'knight/1538779329.0645812.jpg',\n",
       " 'knight/1538779362.8107798.jpg',\n",
       " 'knight/1538779439.9036374.jpg',\n",
       " 'knight/1538779459.4036417.jpg',\n",
       " 'knight/1538779559.3820636.jpg',\n",
       " 'knight/1538779701.3311424.jpg',\n",
       " 'knight/1538779739.058273.jpg',\n",
       " 'knight/1538779919.6389072.jpg',\n",
       " 'knight/1539017656.048383_53.jpg',\n",
       " 'knight/1539017656.2045898_14.jpg',\n",
       " 'knight/1539017656.532639_55.jpg',\n",
       " 'knight/1539017656.6732304_1.jpg',\n",
       " 'knight/1539017656.7825797_62.jpg',\n",
       " 'knight/1539017657.0012786_23.jpg',\n",
       " 'knight/1539017657.0643072_43.jpg',\n",
       " 'knight/1539017657.0682952_45.jpg',\n",
       " 'knight/1539017657.2756457_13.jpg',\n",
       " 'knight/1539017657.2756457_18.jpg',\n",
       " 'knight/1539017657.291264_26.jpg',\n",
       " 'knight/1539017657.5752366_26.jpg',\n",
       " 'knight/1539017657.5752366_28.jpg',\n",
       " 'knight/1539017657.7939315_13.jpg',\n",
       " 'knight/1539017657.8251727_32.jpg',\n",
       " 'knight/1539017657.8251727_33.jpg',\n",
       " 'knight/1539017657.8407946_39.jpg',\n",
       " 'knight/1539017658.0438766_13.jpg',\n",
       " 'knight/1539017658.090735_39.jpg',\n",
       " 'knight/1539017658.2938118_13.jpg',\n",
       " 'knight/1539017658.3094337_22.jpg',\n",
       " 'knight/1539017658.3406758_39.jpg',\n",
       " 'knight/1539017658.3562984_41.jpg',\n",
       " 'knight/1539017658.5749974_27.jpg',\n",
       " 'knight/1539017658.5749974_28.jpg',\n",
       " 'knight/1539017658.809318_29.jpg',\n",
       " 'knight/1539023028.0835974.jpg',\n",
       " 'knight/1539023081.4048553.jpg',\n",
       " 'knight/1539023160.9380662.jpg',\n",
       " 'knight/1539023190.5311208.jpg',\n",
       " 'knight/1539023209.5838358.jpg',\n",
       " 'knight/1539023214.4817233.jpg',\n",
       " 'knight/1539023817.175047.jpg',\n",
       " 'knight/1539026872.2657654.jpg',\n",
       " 'knight/1539027337.492997.jpg',\n",
       " 'knight/1539028085.6649253.jpg',\n",
       " 'knight/1539028221.7775393.jpg',\n",
       " 'pawn/1538776446.2195349.jpg',\n",
       " 'pawn/1538778988.6561909.jpg',\n",
       " 'pawn/1538778996.169403.jpg',\n",
       " 'pawn/1538778999.7679274.jpg',\n",
       " 'pawn/1538779001.1267478.jpg',\n",
       " 'pawn/1538779112.965819.jpg',\n",
       " 'pawn/1538779143.7233405.jpg',\n",
       " 'pawn/1538779184.9161654.jpg',\n",
       " 'pawn/1538779186.269667.jpg',\n",
       " 'pawn/1539017659.0342584_2.jpg',\n",
       " 'pawn/1539017659.0342584_5.jpg',\n",
       " 'pawn/1539017659.273254_5.jpg',\n",
       " 'pawn/1539017659.320111_30.jpg',\n",
       " 'pawn/1539017659.9293435_51.jpg',\n",
       " 'pawn/1539017660.148043_49.jpg',\n",
       " 'pawn/1539017660.3198786_9.jpg',\n",
       " 'pawn/1539017660.3511202_27.jpg',\n",
       " 'pawn/1539017660.5854392_17.jpg',\n",
       " 'pawn/1539017660.804138_1.jpg',\n",
       " 'pawn/1539017660.9134872_58.jpg',\n",
       " 'pawn/1539017661.6291142_49.jpg',\n",
       " 'pawn/1539017661.8009472_9.jpg',\n",
       " 'pawn/1539017661.8321903_20.jpg',\n",
       " 'pawn/1539017662.0429847_1.jpg',\n",
       " 'pawn/1539017662.058607_12.jpg',\n",
       " 'pawn/1539017664.3632076_13.jpg',\n",
       " 'pawn/1539017664.4256973_53.jpg',\n",
       " 'pawn/1539022975.1625128.jpg',\n",
       " 'pawn/1539023017.1607347.jpg',\n",
       " 'pawn/1539023228.1371589.jpg',\n",
       " 'pawn/1539023368.838083.jpg',\n",
       " 'pawn/1539023428.3806713.jpg',\n",
       " 'pawn/1539023466.5992458.jpg',\n",
       " 'pawn/1539023766.4123085.jpg',\n",
       " 'pawn/1539024357.4180336.jpg',\n",
       " 'pawn/1539024430.756946.jpg',\n",
       " 'pawn/1539024541.9791014.jpg',\n",
       " 'pawn/1539024583.6953154.jpg',\n",
       " 'pawn/1539025313.0497591.jpg',\n",
       " 'pawn/1539025362.0886774.jpg',\n",
       " 'pawn/1539025391.7579608.jpg',\n",
       " 'pawn/1539025607.6992056.jpg',\n",
       " 'pawn/1539025673.4654312.jpg',\n",
       " 'pawn/1539025757.220354.jpg',\n",
       " 'pawn/1539025921.7744703.jpg',\n",
       " 'pawn/1539026038.6849546.jpg',\n",
       " 'pawn/1539026837.0018582.jpg',\n",
       " 'pawn/1539026875.391828.jpg',\n",
       " 'pawn/1539026923.465744.jpg',\n",
       " 'pawn/1539026980.8596466.jpg',\n",
       " 'pawn/1539027155.227193.jpg',\n",
       " 'queen/1538774886.4350865.jpg',\n",
       " 'queen/1538774903.7820106.jpg',\n",
       " 'queen/1538775023.0136087.jpg',\n",
       " 'queen/1538775387.0960882.jpg',\n",
       " 'queen/1538775521.7555773.jpg',\n",
       " 'queen/1538776153.1161563.jpg',\n",
       " 'queen/1538776241.9984956.jpg',\n",
       " 'queen/1538776666.2012632.jpg',\n",
       " 'queen/1538777233.7800925.jpg',\n",
       " 'queen/1538777276.2401419.jpg',\n",
       " 'queen/1538777951.5345926.jpg',\n",
       " 'queen/1538778211.4124253.jpg',\n",
       " 'queen/1538778645.0375922.jpg',\n",
       " 'queen/1538779089.730211.jpg',\n",
       " 'queen/1538779104.4953988.jpg',\n",
       " 'queen/1538779711.7096522.jpg',\n",
       " 'queen/1539017656.0171356_34.jpg',\n",
       " 'queen/1539017656.0171356_37.jpg',\n",
       " 'queen/1539017656.9544163_8.jpg',\n",
       " 'queen/1539017657.5596101_20.jpg',\n",
       " 'queen/1539017658.0751133_28.jpg',\n",
       " 'queen/1539017658.809318_21.jpg',\n",
       " 'queen/1539023282.7923298.jpg',\n",
       " 'queen/1539023360.027172.jpg',\n",
       " 'queen/1539024135.86198.jpg',\n",
       " 'queen/1539024330.4476821.jpg',\n",
       " 'queen/1539025459.6575894.jpg',\n",
       " 'queen/1539025544.3600874.jpg',\n",
       " 'queen/1539026802.3034804.jpg',\n",
       " 'queen/1539027143.1900225.jpg',\n",
       " 'queen/1539027242.9633646.jpg',\n",
       " 'queen/1539028286.9873247.jpg',\n",
       " 'rook/1538776436.5497537.jpg',\n",
       " 'rook/1538778101.779042.jpg',\n",
       " 'rook/1538778196.1506379.jpg',\n",
       " 'rook/1538778246.5786252.jpg',\n",
       " 'rook/1538778420.713808.jpg',\n",
       " 'rook/1538778508.6299925.jpg',\n",
       " 'rook/1538778565.9025004.jpg',\n",
       " 'rook/1538778590.4654863.jpg',\n",
       " 'rook/1538778808.3629324.jpg',\n",
       " 'rook/1538778830.4086065.jpg',\n",
       " 'rook/1538778848.9860363.jpg',\n",
       " 'rook/1538778901.7553732.jpg',\n",
       " 'rook/1538778949.280067.jpg',\n",
       " 'rook/1538779051.3667924.jpg',\n",
       " 'rook/1538779142.3172784.jpg',\n",
       " 'rook/1538779147.5203454.jpg',\n",
       " 'rook/1538779183.5230417.jpg',\n",
       " 'rook/1538779355.8192723.jpg',\n",
       " 'rook/1538779366.148124.jpg',\n",
       " 'rook/1538779421.4983451.jpg',\n",
       " 'rook/1538779431.4099524.jpg',\n",
       " 'rook/1538779446.819697.jpg',\n",
       " 'rook/1538779524.2512803.jpg',\n",
       " 'rook/1538779688.6545618.jpg',\n",
       " 'rook/1538779718.4556568.jpg',\n",
       " 'rook/1538780007.7006266.jpg',\n",
       " 'rook/1538780044.5343263.jpg',\n",
       " 'rook/1538780084.217775.jpg',\n",
       " 'rook/1538780193.7644806.jpg',\n",
       " 'rook/1539017656.1889713_1.jpg',\n",
       " 'rook/1539017656.2983181_57.jpg',\n",
       " 'rook/1539017656.4545326_0.jpg',\n",
       " 'rook/1539017656.5482595_56.jpg',\n",
       " 'rook/1539017656.6732304_0.jpg',\n",
       " 'rook/1539017656.7825797_56.jpg',\n",
       " 'rook/1539017656.7825797_63.jpg',\n",
       " 'rook/1539017657.0325217_32.jpg',\n",
       " 'rook/1539017657.1038074_61.jpg',\n",
       " 'rook/1539017657.3537493_58.jpg',\n",
       " 'rook/1539017657.3537493_60.jpg',\n",
       " 'rook/1539017657.6064732_44.jpg',\n",
       " 'rook/1539017657.6220949_52.jpg',\n",
       " 'rook/1539017657.8720484_52.jpg',\n",
       " 'rook/1539017657.8876572_60.jpg',\n",
       " 'rook/1539017658.121979_59.jpg',\n",
       " 'rook/1539017658.1376002_63.jpg',\n",
       " 'rook/1539017658.2781916_2.jpg',\n",
       " 'rook/1539017658.387541_63.jpg',\n",
       " 'rook/1539017658.5906172_32.jpg',\n",
       " 'rook/1539023040.9695048.jpg',\n",
       " 'rook/1539025256.5206127.jpg',\n",
       " 'rook/1539025399.1357424.jpg',\n",
       " 'rook/1539025545.9052472.jpg',\n",
       " 'rook/1539025595.0919125.jpg',\n",
       " 'rook/1539025702.8716216.jpg',\n",
       " 'rook/1539025785.8488226.jpg',\n",
       " 'rook/1539026043.774472.jpg',\n",
       " 'rook/1539026846.958911.jpg',\n",
       " 'rook/1539026987.1509364.jpg',\n",
       " 'rook/1539027229.5479693.jpg',\n",
       " 'rook/1539027241.4846253.jpg',\n",
       " 'square/1538775356.6463027.jpg',\n",
       " 'square/1538775359.6263092.jpg',\n",
       " 'square/1538775377.837528.jpg',\n",
       " 'square/1538775380.3348455.jpg',\n",
       " 'square/1538775381.8908746.jpg',\n",
       " 'square/1538775383.3700583.jpg',\n",
       " 'square/1538775392.500712.jpg',\n",
       " 'square/1538775393.9093816.jpg',\n",
       " 'square/1538775395.379885.jpg',\n",
       " 'square/1538775397.2511947.jpg',\n",
       " 'square/1538775410.2587154.jpg',\n",
       " 'square/1538775411.657127.jpg',\n",
       " 'square/1538775413.002367.jpg',\n",
       " 'square/1538775414.3379526.jpg',\n",
       " 'square/1538775428.0259957.jpg',\n",
       " 'square/1538775429.9237733.jpg',\n",
       " 'square/1538775431.6329722.jpg',\n",
       " 'square/1538775433.0617445.jpg',\n",
       " 'square/1538775459.9201775.jpg',\n",
       " 'square/1538775463.3285487.jpg',\n",
       " 'square/1538775464.8436766.jpg',\n",
       " 'square/1538775472.3628545.jpg',\n",
       " 'square/1538775473.8751059.jpg',\n",
       " 'square/1538775477.4368722.jpg',\n",
       " 'square/1538775478.8303273.jpg',\n",
       " 'square/1538775482.290001.jpg',\n",
       " 'square/1538775490.1613836.jpg',\n",
       " 'square/1538775496.149376.jpg',\n",
       " 'square/1538775497.691954.jpg',\n",
       " 'square/1538775501.9954052.jpg',\n",
       " 'square/1538775507.261289.jpg',\n",
       " 'square/1538775508.9165573.jpg',\n",
       " 'square/1538775518.2826645.jpg',\n",
       " 'square/1538775525.8732312.jpg',\n",
       " 'square/1538775527.3804672.jpg',\n",
       " 'square/1538775528.933519.jpg',\n",
       " 'square/1538775534.8773243.jpg',\n",
       " 'square/1538775541.8031077.jpg',\n",
       " 'square/1538775546.3549168.jpg',\n",
       " 'square/1538775553.2582464.jpg',\n",
       " 'square/1538775554.7329156.jpg',\n",
       " 'square/1538775556.1137612.jpg',\n",
       " 'square/1538775557.4647563.jpg',\n",
       " 'square/1538775563.8003812.jpg',\n",
       " 'square/1538775575.8332133.jpg',\n",
       " 'square/1538775578.4874399.jpg',\n",
       " 'square/1538775583.9372883.jpg',\n",
       " 'square/1538775585.983797.jpg',\n",
       " 'square/1538775589.5684185.jpg',\n",
       " 'square/1538775593.2157.jpg']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piece_test_iter.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=piece_test_iter.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Truth\": truth,\n",
    "                      \"Predictions\":predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bishop/1538777310.1624303.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bishop/1538777556.4695792.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bishop/1538777675.2653856.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bishop/1538777891.4931645.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bishop/1538778091.9695802.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bishop/1538778093.2397656.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bishop/1538778214.3746974.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bishop/1538778215.8878355.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bishop/1538778410.6081142.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bishop/1538778427.8668559.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bishop/1538778537.1458335.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bishop/1538778538.3017461.jpg</td>\n",
       "      <td>queen</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bishop/1538778643.4925923.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bishop/1538778653.1963263.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bishop/1538778656.6147914.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bishop/1538778729.1405196.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bishop/1538778752.467536.jpg</td>\n",
       "      <td>queen</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bishop/1538778832.1937056.jpg</td>\n",
       "      <td>pawn</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bishop/1538778833.610855.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bishop/1538778878.3252861.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bishop/1538778958.324379.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bishop/1538779144.9958382.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bishop/1538779155.2682924.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bishop/1538779417.7487714.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bishop/1538779519.5865016.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bishop/1539017656.2202125_22.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bishop/1539017656.6888525_5.jpg</td>\n",
       "      <td>queen</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bishop/1539023140.397658.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bishop/1539023363.830272.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bishop/1539023732.561326.jpg</td>\n",
       "      <td>queen</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>square/1538775464.8436766.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>square/1538775472.3628545.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>square/1538775473.8751059.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>square/1538775477.4368722.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>square/1538775478.8303273.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>square/1538775482.290001.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>square/1538775490.1613836.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>square/1538775496.149376.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>square/1538775497.691954.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>square/1538775501.9954052.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>square/1538775507.261289.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>square/1538775508.9165573.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>square/1538775518.2826645.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>square/1538775525.8732312.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>square/1538775527.3804672.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>square/1538775528.933519.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>square/1538775534.8773243.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>square/1538775541.8031077.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>square/1538775546.3549168.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>square/1538775553.2582464.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>square/1538775554.7329156.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>square/1538775556.1137612.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>square/1538775557.4647563.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>square/1538775563.8003812.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>square/1538775575.8332133.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>square/1538775578.4874399.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>square/1538775583.9372883.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>square/1538775585.983797.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>square/1538775589.5684185.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>square/1538775593.2157.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Filename Predictions   Truth\n",
       "0       bishop/1538777310.1624303.jpg      bishop  bishop\n",
       "1       bishop/1538777556.4695792.jpg      bishop  bishop\n",
       "2       bishop/1538777675.2653856.jpg      bishop  bishop\n",
       "3       bishop/1538777891.4931645.jpg      bishop  bishop\n",
       "4       bishop/1538778091.9695802.jpg      knight  bishop\n",
       "5       bishop/1538778093.2397656.jpg      bishop  bishop\n",
       "6       bishop/1538778214.3746974.jpg      bishop  bishop\n",
       "7       bishop/1538778215.8878355.jpg      knight  bishop\n",
       "8       bishop/1538778410.6081142.jpg      bishop  bishop\n",
       "9       bishop/1538778427.8668559.jpg      knight  bishop\n",
       "10      bishop/1538778537.1458335.jpg      knight  bishop\n",
       "11      bishop/1538778538.3017461.jpg       queen  bishop\n",
       "12      bishop/1538778643.4925923.jpg      bishop  bishop\n",
       "13      bishop/1538778653.1963263.jpg      bishop  bishop\n",
       "14      bishop/1538778656.6147914.jpg      knight  bishop\n",
       "15      bishop/1538778729.1405196.jpg      bishop  bishop\n",
       "16       bishop/1538778752.467536.jpg       queen  bishop\n",
       "17      bishop/1538778832.1937056.jpg        pawn  bishop\n",
       "18       bishop/1538778833.610855.jpg      bishop  bishop\n",
       "19      bishop/1538778878.3252861.jpg      bishop  bishop\n",
       "20       bishop/1538778958.324379.jpg      knight  bishop\n",
       "21      bishop/1538779144.9958382.jpg      knight  bishop\n",
       "22      bishop/1538779155.2682924.jpg      knight  bishop\n",
       "23      bishop/1538779417.7487714.jpg      bishop  bishop\n",
       "24      bishop/1538779519.5865016.jpg      knight  bishop\n",
       "25   bishop/1539017656.2202125_22.jpg      knight  bishop\n",
       "26    bishop/1539017656.6888525_5.jpg       queen  bishop\n",
       "27       bishop/1539023140.397658.jpg      bishop  bishop\n",
       "28       bishop/1539023363.830272.jpg      knight  bishop\n",
       "29       bishop/1539023732.561326.jpg       queen  bishop\n",
       "..                                ...         ...     ...\n",
       "305     square/1538775464.8436766.jpg      square  square\n",
       "306     square/1538775472.3628545.jpg      square  square\n",
       "307     square/1538775473.8751059.jpg      square  square\n",
       "308     square/1538775477.4368722.jpg      square  square\n",
       "309     square/1538775478.8303273.jpg      square  square\n",
       "310      square/1538775482.290001.jpg      square  square\n",
       "311     square/1538775490.1613836.jpg      square  square\n",
       "312      square/1538775496.149376.jpg      square  square\n",
       "313      square/1538775497.691954.jpg      square  square\n",
       "314     square/1538775501.9954052.jpg      square  square\n",
       "315      square/1538775507.261289.jpg      square  square\n",
       "316     square/1538775508.9165573.jpg      square  square\n",
       "317     square/1538775518.2826645.jpg      square  square\n",
       "318     square/1538775525.8732312.jpg      square  square\n",
       "319     square/1538775527.3804672.jpg      square  square\n",
       "320      square/1538775528.933519.jpg      square  square\n",
       "321     square/1538775534.8773243.jpg      square  square\n",
       "322     square/1538775541.8031077.jpg      square  square\n",
       "323     square/1538775546.3549168.jpg      square  square\n",
       "324     square/1538775553.2582464.jpg      square  square\n",
       "325     square/1538775554.7329156.jpg      square  square\n",
       "326     square/1538775556.1137612.jpg      square  square\n",
       "327     square/1538775557.4647563.jpg      square  square\n",
       "328     square/1538775563.8003812.jpg      square  square\n",
       "329     square/1538775575.8332133.jpg      square  square\n",
       "330     square/1538775578.4874399.jpg      square  square\n",
       "331     square/1538775583.9372883.jpg      square  square\n",
       "332      square/1538775585.983797.jpg      square  square\n",
       "333     square/1538775589.5684185.jpg      square  square\n",
       "334        square/1538775593.2157.jpg      square  square\n",
       "\n",
       "[335 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.40      0.38        47\n",
      "           1       0.32      0.52      0.40        33\n",
      "           2       0.32      0.34      0.33        61\n",
      "           3       0.52      0.24      0.32        51\n",
      "           4       0.26      0.38      0.30        32\n",
      "           5       0.60      0.43      0.50        61\n",
      "           6       1.00      1.00      1.00        50\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       335\n",
      "   macro avg       0.48      0.47      0.46       335\n",
      "weighted avg       0.50      0.47      0.47       335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(piece_test_iter.classes,predicted_class_indices)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  2 17  1  5  3  0]\n",
      " [ 0 17  1  1 12  2  0]\n",
      " [14  7 21  5 10  4  0]\n",
      " [12  8 10 12  4  5  0]\n",
      " [ 3  9  5  0 12  3  0]\n",
      " [ 5 10 12  4  4 26  0]\n",
      " [ 0  0  0  0  0  0 50]]\n",
      "{0: 'bishop', 1: 'king', 2: 'knight', 3: 'pawn', 4: 'queen', 5: 'rook', 6: 'square'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmatrix = confusion_matrix(piece_test_iter.classes,predicted_class_indices)\n",
    "print(cmatrix)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(cmatrix, annot=True, xticklabels=['bishop','king','pawn','knight','queen','rook','square'],yticklabels=['bishop','king','pawn','knight','queen','rook','square'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(piece_hist.history['accuracy'])\n",
    "plt.plot(piece_hist.history['val_accuracy'])\n",
    "plt.title('Piece Model accuracy no augmentation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(piece_hist.history['loss'])\n",
    "plt.plot(piece_hist.history['val_loss'])\n",
    "plt.title('Piece Model Log Loss no augmentation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4452106330276175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss_result = log_loss(piece_test_iter.classes,piece_pred)\n",
    "print(log_loss_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
