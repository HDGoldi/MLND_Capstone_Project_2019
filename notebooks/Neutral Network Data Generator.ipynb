{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\JanSulaiman\\\\Downloads\\\\MLND_Capstone_Project_2019\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1227 images belonging to 7 classes.\n",
      "Found 335 images belonging to 7 classes.\n",
      "Found 331 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Image Data Generator w/ no augmentation\n",
    "#Scaling for pixels\n",
    "piece_train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "piece_test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "piece_valid_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "\n",
    "\n",
    "#Flow data from directory\n",
    "\n",
    "piece_train_iter = piece_train_datagen.flow_from_directory(\n",
    "    directory = '../data/piece_data/train',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "piece_test_iter = piece_test_datagen.flow_from_directory(\n",
    "    directory = '../data/piece_data/test',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "piece_valid_iter = piece_valid_datagen.flow_from_directory(\n",
    "    directory = '../data/piece_data/valid',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JanSulaiman\\.conda\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\JanSulaiman\\.conda\\envs\\capstone\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 135, 135, 16)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 67, 67, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 67, 67, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 67, 67, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 33, 33, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 33, 33, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 33, 33, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       32896     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 64)          32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 64)          16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              133120    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 14343     \n",
      "=================================================================\n",
      "Total params: 4,506,439\n",
      "Trainable params: 4,505,703\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define NN architecture\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "piece_model = Sequential()\n",
    "piece_model.add(Conv2D(filters=16, kernel_size=5, padding='same', activation='relu', \n",
    "                        input_shape=(135, 135, 1)))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Dropout(0.3))\n",
    "piece_model.add(GlobalAveragePooling2D())\n",
    "piece_model.add(Dense(2048, activation='relu'))\n",
    "piece_model.add(Dense(2048, activation='relu'))\n",
    "piece_model.add(Dropout(0.4))\n",
    "piece_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "piece_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "from keras.optimizers import RMSprop\n",
    "piece_model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.00001), \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "STEP_SIZE_TRAIN = piece_train_iter.n/piece_train_iter.batch_size\n",
    "STEP_SIZE_VALID = piece_valid_iter.n/piece_valid_iter.batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JanSulaiman\\.conda\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1500\n",
      " - 34s - loss: 1.8781 - acc: 0.2727 - val_loss: 1.8458 - val_acc: 0.2598\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.84577, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 2/1500\n",
      " - 29s - loss: 1.7968 - acc: 0.2849 - val_loss: 1.7978 - val_acc: 0.2598\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.84577 to 1.79776, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 3/1500\n",
      " - 26s - loss: 1.7435 - acc: 0.3085 - val_loss: 1.7608 - val_acc: 0.2689\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.79776 to 1.76076, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 4/1500\n",
      " - 26s - loss: 1.7047 - acc: 0.3245 - val_loss: 1.7267 - val_acc: 0.2779\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.76076 to 1.72666, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 5/1500\n",
      " - 29s - loss: 1.6575 - acc: 0.3539 - val_loss: 1.6949 - val_acc: 0.2900\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.72666 to 1.69490, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 6/1500\n",
      " - 29s - loss: 1.6319 - acc: 0.3621 - val_loss: 1.6649 - val_acc: 0.3051\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.69490 to 1.66491, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 7/1500\n",
      " - 31s - loss: 1.5775 - acc: 0.3987 - val_loss: 1.6418 - val_acc: 0.3051\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.66491 to 1.64184, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 8/1500\n",
      " - 28s - loss: 1.5684 - acc: 0.3911 - val_loss: 1.6202 - val_acc: 0.3082\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.64184 to 1.62023, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 9/1500\n",
      " - 29s - loss: 1.5338 - acc: 0.4186 - val_loss: 1.6058 - val_acc: 0.3142\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.62023 to 1.60581, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 10/1500\n",
      " - 28s - loss: 1.4884 - acc: 0.4382 - val_loss: 1.5866 - val_acc: 0.3142\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.60581 to 1.58662, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 11/1500\n",
      " - 28s - loss: 1.4585 - acc: 0.4521 - val_loss: 1.5751 - val_acc: 0.3353\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.58662 to 1.57510, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 12/1500\n",
      " - 30s - loss: 1.4472 - acc: 0.4594 - val_loss: 1.5666 - val_acc: 0.3565\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.57510 to 1.56662, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 13/1500\n",
      " - 29s - loss: 1.4237 - acc: 0.4595 - val_loss: 1.5520 - val_acc: 0.3535\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.56662 to 1.55197, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 14/1500\n",
      " - 28s - loss: 1.4062 - acc: 0.4820 - val_loss: 1.5372 - val_acc: 0.3595\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.55197 to 1.53720, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 15/1500\n",
      " - 31s - loss: 1.3885 - acc: 0.4915 - val_loss: 1.5227 - val_acc: 0.3656\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.53720 to 1.52267, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 16/1500\n",
      " - 29s - loss: 1.3695 - acc: 0.4966 - val_loss: 1.5201 - val_acc: 0.3656\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.52267 to 1.52006, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 17/1500\n",
      " - 33s - loss: 1.3396 - acc: 0.5247 - val_loss: 1.5116 - val_acc: 0.3716\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.52006 to 1.51160, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 18/1500\n",
      " - 32s - loss: 1.3365 - acc: 0.5182 - val_loss: 1.5170 - val_acc: 0.3656\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.51160\n",
      "Epoch 19/1500\n",
      " - 31s - loss: 1.3093 - acc: 0.5220 - val_loss: 1.4911 - val_acc: 0.3656\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.51160 to 1.49112, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 20/1500\n",
      " - 28s - loss: 1.2936 - acc: 0.5158 - val_loss: 1.4923 - val_acc: 0.3807\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.49112\n",
      "Epoch 21/1500\n",
      " - 31s - loss: 1.2911 - acc: 0.5234 - val_loss: 1.4845 - val_acc: 0.3837\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.49112 to 1.48447, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 22/1500\n",
      " - 29s - loss: 1.2524 - acc: 0.5459 - val_loss: 1.4788 - val_acc: 0.3776\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.48447 to 1.47876, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 23/1500\n",
      " - 27s - loss: 1.2538 - acc: 0.5390 - val_loss: 1.4710 - val_acc: 0.3776\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.47876 to 1.47104, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 24/1500\n",
      " - 28s - loss: 1.2406 - acc: 0.5394 - val_loss: 1.4668 - val_acc: 0.3746\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.47104 to 1.46682, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 25/1500\n",
      " - 28s - loss: 1.2052 - acc: 0.5667 - val_loss: 1.4635 - val_acc: 0.3746\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.46682 to 1.46352, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 26/1500\n",
      " - 29s - loss: 1.1970 - acc: 0.5702 - val_loss: 1.4558 - val_acc: 0.3867\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.46352 to 1.45584, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 27/1500\n",
      " - 31s - loss: 1.1849 - acc: 0.5788 - val_loss: 1.4536 - val_acc: 0.3927\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.45584 to 1.45362, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 28/1500\n",
      " - 28s - loss: 1.1613 - acc: 0.5790 - val_loss: 1.4430 - val_acc: 0.4048\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.45362 to 1.44304, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 29/1500\n",
      " - 31s - loss: 1.1424 - acc: 0.5804 - val_loss: 1.4384 - val_acc: 0.4199\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.44304 to 1.43844, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 30/1500\n",
      " - 28s - loss: 1.1230 - acc: 0.5909 - val_loss: 1.4337 - val_acc: 0.4079\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.43844 to 1.43372, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 31/1500\n",
      " - 30s - loss: 1.1239 - acc: 0.6042 - val_loss: 1.4279 - val_acc: 0.4169\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.43372 to 1.42788, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 32/1500\n",
      " - 29s - loss: 1.1234 - acc: 0.5868 - val_loss: 1.4492 - val_acc: 0.3958\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.42788\n",
      "Epoch 33/1500\n",
      " - 29s - loss: 1.0918 - acc: 0.6015 - val_loss: 1.4221 - val_acc: 0.4199\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.42788 to 1.42209, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 34/1500\n",
      " - 31s - loss: 1.0636 - acc: 0.6221 - val_loss: 1.4224 - val_acc: 0.4260\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.42209\n",
      "Epoch 35/1500\n",
      " - 39s - loss: 1.0391 - acc: 0.6359 - val_loss: 1.4153 - val_acc: 0.4260\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.42209 to 1.41532, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 36/1500\n",
      " - 37s - loss: 1.0280 - acc: 0.6420 - val_loss: 1.4060 - val_acc: 0.4260\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.41532 to 1.40599, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 37/1500\n",
      " - 32s - loss: 1.0276 - acc: 0.6390 - val_loss: 1.4188 - val_acc: 0.4230\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.40599\n",
      "Epoch 38/1500\n",
      " - 30s - loss: 1.0232 - acc: 0.6326 - val_loss: 1.3944 - val_acc: 0.4411\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.40599 to 1.39444, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 39/1500\n",
      " - 30s - loss: 0.9911 - acc: 0.6626 - val_loss: 1.3990 - val_acc: 0.4411\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.39444\n",
      "Epoch 40/1500\n",
      " - 29s - loss: 0.9910 - acc: 0.6537 - val_loss: 1.4080 - val_acc: 0.4048\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.39444\n",
      "Epoch 41/1500\n",
      " - 30s - loss: 0.9622 - acc: 0.6684 - val_loss: 1.4103 - val_acc: 0.4502\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.39444\n",
      "Epoch 42/1500\n",
      " - 33s - loss: 0.9495 - acc: 0.6557 - val_loss: 1.3887 - val_acc: 0.4350\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.39444 to 1.38865, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 43/1500\n",
      " - 29s - loss: 0.9485 - acc: 0.6710 - val_loss: 1.3764 - val_acc: 0.4592\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.38865 to 1.37637, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 44/1500\n",
      " - 29s - loss: 0.9301 - acc: 0.6788 - val_loss: 1.3804 - val_acc: 0.4441\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.37637\n",
      "Epoch 45/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 30s - loss: 0.9293 - acc: 0.6754 - val_loss: 1.3787 - val_acc: 0.4471\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.37637\n",
      "Epoch 46/1500\n",
      " - 30s - loss: 0.9362 - acc: 0.6740 - val_loss: 1.3744 - val_acc: 0.4532\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.37637 to 1.37436, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 47/1500\n",
      " - 29s - loss: 0.8723 - acc: 0.7030 - val_loss: 1.3755 - val_acc: 0.4562\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.37436\n",
      "Epoch 48/1500\n",
      " - 30s - loss: 0.8582 - acc: 0.7033 - val_loss: 1.3770 - val_acc: 0.4441\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.37436\n",
      "Epoch 49/1500\n",
      " - 31s - loss: 0.8485 - acc: 0.7168 - val_loss: 1.3608 - val_acc: 0.4683\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.37436 to 1.36076, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 50/1500\n",
      " - 31s - loss: 0.8560 - acc: 0.7008 - val_loss: 1.3520 - val_acc: 0.4924\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.36076 to 1.35199, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 51/1500\n",
      " - 30s - loss: 0.8047 - acc: 0.7310 - val_loss: 1.3621 - val_acc: 0.4713\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.35199\n",
      "Epoch 52/1500\n",
      " - 29s - loss: 0.8310 - acc: 0.7130 - val_loss: 1.3601 - val_acc: 0.4743\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.35199\n",
      "Epoch 53/1500\n",
      " - 29s - loss: 0.8088 - acc: 0.7166 - val_loss: 1.3576 - val_acc: 0.4773\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.35199\n",
      "Epoch 54/1500\n",
      " - 29s - loss: 0.8077 - acc: 0.7108 - val_loss: 1.3651 - val_acc: 0.4773\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.35199\n",
      "Epoch 55/1500\n",
      " - 25s - loss: 0.7922 - acc: 0.7120 - val_loss: 1.3621 - val_acc: 0.4834\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.35199\n",
      "Epoch 56/1500\n",
      " - 24s - loss: 0.7728 - acc: 0.7298 - val_loss: 1.3489 - val_acc: 0.4804\n",
      "\n",
      "Epoch 00056: val_loss improved from 1.35199 to 1.34888, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 57/1500\n",
      " - 26s - loss: 0.7553 - acc: 0.7367 - val_loss: 1.3511 - val_acc: 0.4834\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.34888\n",
      "Epoch 58/1500\n",
      " - 25s - loss: 0.7584 - acc: 0.7328 - val_loss: 1.3687 - val_acc: 0.4955\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.34888\n",
      "Epoch 59/1500\n",
      " - 25s - loss: 0.7484 - acc: 0.7439 - val_loss: 1.3531 - val_acc: 0.4834\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.34888\n",
      "Epoch 60/1500\n",
      " - 26s - loss: 0.7201 - acc: 0.7586 - val_loss: 1.3515 - val_acc: 0.4924\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.34888\n",
      "Epoch 61/1500\n",
      " - 27s - loss: 0.7215 - acc: 0.7541 - val_loss: 1.3477 - val_acc: 0.4834\n",
      "\n",
      "Epoch 00061: val_loss improved from 1.34888 to 1.34774, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 62/1500\n",
      " - 23s - loss: 0.7072 - acc: 0.7632 - val_loss: 1.3465 - val_acc: 0.4985\n",
      "\n",
      "Epoch 00062: val_loss improved from 1.34774 to 1.34645, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 63/1500\n",
      " - 24s - loss: 0.7082 - acc: 0.7566 - val_loss: 1.3686 - val_acc: 0.5045\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.34645\n",
      "Epoch 64/1500\n",
      " - 24s - loss: 0.6774 - acc: 0.7704 - val_loss: 1.3547 - val_acc: 0.4955\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.34645\n",
      "Epoch 65/1500\n",
      " - 24s - loss: 0.6913 - acc: 0.7665 - val_loss: 1.3427 - val_acc: 0.4985\n",
      "\n",
      "Epoch 00065: val_loss improved from 1.34645 to 1.34271, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 66/1500\n",
      " - 24s - loss: 0.6854 - acc: 0.7730 - val_loss: 1.3505 - val_acc: 0.5196\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.34271\n",
      "Epoch 67/1500\n",
      " - 24s - loss: 0.6636 - acc: 0.7621 - val_loss: 1.3494 - val_acc: 0.4924\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.34271\n",
      "Epoch 68/1500\n",
      " - 26s - loss: 0.6203 - acc: 0.7814 - val_loss: 1.3406 - val_acc: 0.5076\n",
      "\n",
      "Epoch 00068: val_loss improved from 1.34271 to 1.34062, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 69/1500\n",
      " - 25s - loss: 0.6226 - acc: 0.7967 - val_loss: 1.3278 - val_acc: 0.5015\n",
      "\n",
      "Epoch 00069: val_loss improved from 1.34062 to 1.32777, saving model to piece_model_1.weights.best.hdf5\n",
      "Epoch 70/1500\n",
      " - 24s - loss: 0.6277 - acc: 0.7823 - val_loss: 1.3530 - val_acc: 0.5045\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.32777\n",
      "Epoch 71/1500\n",
      " - 25s - loss: 0.6155 - acc: 0.7849 - val_loss: 1.3329 - val_acc: 0.4955\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.32777\n",
      "Epoch 72/1500\n",
      " - 24s - loss: 0.6178 - acc: 0.7888 - val_loss: 1.3416 - val_acc: 0.4894\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.32777\n",
      "Epoch 73/1500\n",
      " - 25s - loss: 0.6264 - acc: 0.7857 - val_loss: 1.3602 - val_acc: 0.4985\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.32777\n",
      "Epoch 74/1500\n",
      " - 25s - loss: 0.6011 - acc: 0.7912 - val_loss: 1.3521 - val_acc: 0.5015\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.32777\n",
      "Epoch 75/1500\n",
      " - 24s - loss: 0.5728 - acc: 0.8158 - val_loss: 1.3695 - val_acc: 0.5076\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.32777\n",
      "Epoch 76/1500\n",
      " - 26s - loss: 0.5850 - acc: 0.7961 - val_loss: 1.3736 - val_acc: 0.5015\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.32777\n",
      "Epoch 77/1500\n",
      " - 27s - loss: 0.5668 - acc: 0.8086 - val_loss: 1.3552 - val_acc: 0.5045\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.32777\n",
      "Epoch 78/1500\n",
      " - 25s - loss: 0.5376 - acc: 0.8110 - val_loss: 1.3610 - val_acc: 0.5076\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.32777\n",
      "Epoch 79/1500\n",
      " - 26s - loss: 0.5535 - acc: 0.8136 - val_loss: 1.3528 - val_acc: 0.5045\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.32777\n",
      "Epoch 80/1500\n",
      " - 26s - loss: 0.5651 - acc: 0.8047 - val_loss: 1.3469 - val_acc: 0.5196\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.32777\n",
      "Epoch 81/1500\n",
      " - 27s - loss: 0.5226 - acc: 0.8256 - val_loss: 1.3445 - val_acc: 0.5136\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.32777\n",
      "Epoch 82/1500\n",
      " - 27s - loss: 0.5206 - acc: 0.8242 - val_loss: 1.3419 - val_acc: 0.5227\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.32777\n",
      "Epoch 83/1500\n",
      " - 26s - loss: 0.5263 - acc: 0.8241 - val_loss: 1.3480 - val_acc: 0.5106\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.32777\n",
      "Epoch 84/1500\n",
      " - 24s - loss: 0.5140 - acc: 0.8240 - val_loss: 1.3414 - val_acc: 0.5136\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.32777\n",
      "Epoch 85/1500\n",
      " - 24s - loss: 0.5118 - acc: 0.8263 - val_loss: 1.3555 - val_acc: 0.5136\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.32777\n",
      "Epoch 86/1500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d44144dc85f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                           verbose=2)\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\capstone\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\capstone\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\capstone\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\capstone\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\capstone\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\capstone\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='piece_model_1.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "piece_hist = piece_model.fit_generator(generator=piece_train_iter, \n",
    "                          steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                          validation_data=piece_valid_iter, \n",
    "                          validation_steps=STEP_SIZE_VALID,\n",
    "                          epochs=1500, \n",
    "                          callbacks=[checkpointer], \n",
    "                          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset test iterator\n",
    "STEP_SIZE_TEST = piece_test_iter.n/piece_test_iter.batch_size\n",
    "piece_test_iter.reset()\n",
    "# load the weights that yielded the best validation accuracy\n",
    "piece_model.load_weights('piece_model.weights.best.hdf5')\n",
    "# evaluate and print test accuracy\n",
    "score = piece_model.evaluate_generator(generator=piece_test_iter,steps=STEP_SIZE_TEST)\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_test_iter.reset()\n",
    "piece_pred = piece_model.predict_generator(piece_test_iter,steps=STEP_SIZE_TEST,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(piece_pred,axis=1)\n",
    "print(predicted_class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (piece_test_iter.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "truth =  [labels[k] for k in piece_test_iter.classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[labels[k] for k in piece_test_iter.classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_test_iter.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=piece_test_iter.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Truth\": truth,\n",
    "                      \"Predictions\":predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(piece_test_iter.classes,predicted_class_indices)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmatrix = confusion_matrix(piece_test_iter.classes,predicted_class_indices)\n",
    "print(cmatrix)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(cmatrix, annot=True, xticklabels=['bishop','king','pawn','knight','queen','rook','square'],yticklabels=['bishop','king','pawn','knight','queen','rook','square'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(piece_hist.history['acc'])\n",
    "plt.plot(piece_hist.history['val_acc'])\n",
    "plt.title('Piece Model accuracy no augmentation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(piece_hist.history['loss'])\n",
    "plt.plot(piece_hist.history['val_loss'])\n",
    "plt.title('Piece Model Log Loss no augmentation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss_result = log_loss(piece_test_iter.classes,piece_pred)\n",
    "print(log_loss_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
