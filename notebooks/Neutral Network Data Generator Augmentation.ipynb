{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1227 images belonging to 7 classes.\n",
      "Found 335 images belonging to 7 classes.\n",
      "Found 331 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Image Data Generator w/ no augmentation\n",
    "#Scaling for pixels\n",
    "piece_train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "piece_test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "piece_valid_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "\n",
    "\n",
    "#Flow data from directory\n",
    "\n",
    "piece_train_iter = piece_train_datagen.flow_from_directory(\n",
    "    directory = '../data/piece_data/train',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "piece_test_iter = piece_test_datagen.flow_from_directory(\n",
    "    directory = '../data/piece_data/test',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "piece_valid_iter = piece_valid_datagen.flow_from_directory(\n",
    "    directory = '../data/piece_data/valid',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 135, 135, 16)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 67, 67, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 67, 67, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 67, 67, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 33, 33, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_1 (Ba (None, 33, 33, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 33, 33, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_2 (Ba (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       32896     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_3 (Ba (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_4 (Ba (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 64)          16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_5 (Ba (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              133120    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 14343     \n",
      "=================================================================\n",
      "Total params: 4,506,439\n",
      "Trainable params: 4,505,703\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define NN architecture\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "piece_model = Sequential()\n",
    "piece_model.add(Conv2D(filters=16, kernel_size=5, padding='same', activation='relu', \n",
    "                        input_shape=(135, 135, 1)))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Dropout(0.3))\n",
    "piece_model.add(GlobalAveragePooling2D())\n",
    "piece_model.add(Dense(2048, activation='relu'))\n",
    "piece_model.add(Dense(2048, activation='relu'))\n",
    "piece_model.add(Dropout(0.4))\n",
    "piece_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "piece_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "piece_model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.00001), \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "STEP_SIZE_TRAIN = piece_train_iter.n/piece_train_iter.batch_size\n",
    "STEP_SIZE_VALID = piece_valid_iter.n/piece_valid_iter.batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.06374, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 22s - loss: 1.9017 - accuracy: 0.2184 - val_loss: 2.0637 - val_accuracy: 0.1511\n",
      "Epoch 2/1500\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.06374\n",
      "39/38 - 14s - loss: 1.8284 - accuracy: 0.2510 - val_loss: 2.0772 - val_accuracy: 0.1511\n",
      "Epoch 3/1500\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.06374\n",
      "39/38 - 14s - loss: 1.7900 - accuracy: 0.2755 - val_loss: 2.1099 - val_accuracy: 0.1511\n",
      "Epoch 4/1500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.06374\n",
      "39/38 - 13s - loss: 1.7626 - accuracy: 0.2747 - val_loss: 2.2205 - val_accuracy: 0.1511\n",
      "Epoch 5/1500\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.06374\n",
      "39/38 - 14s - loss: 1.7164 - accuracy: 0.3073 - val_loss: 2.3962 - val_accuracy: 0.1511\n",
      "Epoch 6/1500\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.06374\n",
      "39/38 - 13s - loss: 1.6898 - accuracy: 0.3268 - val_loss: 2.6402 - val_accuracy: 0.1511\n",
      "Epoch 7/1500\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.06374\n",
      "39/38 - 13s - loss: 1.6615 - accuracy: 0.3374 - val_loss: 2.7897 - val_accuracy: 0.1511\n",
      "Epoch 8/1500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.06374\n",
      "39/38 - 14s - loss: 1.6310 - accuracy: 0.3456 - val_loss: 2.7578 - val_accuracy: 0.1511\n",
      "Epoch 9/1500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.06374\n",
      "39/38 - 13s - loss: 1.6031 - accuracy: 0.3513 - val_loss: 2.6132 - val_accuracy: 0.1511\n",
      "Epoch 10/1500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.06374\n",
      "39/38 - 14s - loss: 1.6062 - accuracy: 0.3521 - val_loss: 2.4524 - val_accuracy: 0.1511\n",
      "Epoch 11/1500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.06374\n",
      "39/38 - 14s - loss: 1.5757 - accuracy: 0.4002 - val_loss: 2.3091 - val_accuracy: 0.1541\n",
      "Epoch 12/1500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.06374\n",
      "39/38 - 14s - loss: 1.5558 - accuracy: 0.3814 - val_loss: 2.1338 - val_accuracy: 0.1873\n",
      "Epoch 13/1500\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.06374 to 2.00808, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.5390 - accuracy: 0.3920 - val_loss: 2.0081 - val_accuracy: 0.2236\n",
      "Epoch 14/1500\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.00808 to 1.90747, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.5303 - accuracy: 0.4108 - val_loss: 1.9075 - val_accuracy: 0.2266\n",
      "Epoch 15/1500\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.90747 to 1.81433, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.5059 - accuracy: 0.4238 - val_loss: 1.8143 - val_accuracy: 0.2719\n",
      "Epoch 16/1500\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.81433 to 1.77204, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.4978 - accuracy: 0.4230 - val_loss: 1.7720 - val_accuracy: 0.2870\n",
      "Epoch 17/1500\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.77204 to 1.71746, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.4702 - accuracy: 0.4393 - val_loss: 1.7175 - val_accuracy: 0.3323\n",
      "Epoch 18/1500\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.71746 to 1.69268, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.4877 - accuracy: 0.4091 - val_loss: 1.6927 - val_accuracy: 0.3353\n",
      "Epoch 19/1500\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.69268 to 1.63829, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.4624 - accuracy: 0.4311 - val_loss: 1.6383 - val_accuracy: 0.3505\n",
      "Epoch 20/1500\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.63829 to 1.62814, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.4497 - accuracy: 0.4393 - val_loss: 1.6281 - val_accuracy: 0.3565\n",
      "Epoch 21/1500\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.62814 to 1.59588, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.4558 - accuracy: 0.4230 - val_loss: 1.5959 - val_accuracy: 0.3595\n",
      "Epoch 22/1500\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.59588 to 1.59089, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.4448 - accuracy: 0.4262 - val_loss: 1.5909 - val_accuracy: 0.3686\n",
      "Epoch 23/1500\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.59089 to 1.58285, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.4217 - accuracy: 0.4368 - val_loss: 1.5828 - val_accuracy: 0.3716\n",
      "Epoch 24/1500\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.58285 to 1.56897, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.4299 - accuracy: 0.4246 - val_loss: 1.5690 - val_accuracy: 0.3565\n",
      "Epoch 25/1500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.56897\n",
      "39/38 - 14s - loss: 1.4223 - accuracy: 0.4328 - val_loss: 1.5790 - val_accuracy: 0.3686\n",
      "Epoch 26/1500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.56897\n",
      "39/38 - 13s - loss: 1.4067 - accuracy: 0.4491 - val_loss: 1.5698 - val_accuracy: 0.3807\n",
      "Epoch 27/1500\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.56897 to 1.56092, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.3917 - accuracy: 0.4442 - val_loss: 1.5609 - val_accuracy: 0.3837\n",
      "Epoch 28/1500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.56092\n",
      "39/38 - 13s - loss: 1.3889 - accuracy: 0.4548 - val_loss: 1.5626 - val_accuracy: 0.3867\n",
      "Epoch 29/1500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.56092\n",
      "39/38 - 14s - loss: 1.3868 - accuracy: 0.4613 - val_loss: 1.5670 - val_accuracy: 0.3746\n",
      "Epoch 30/1500\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.56092\n",
      "39/38 - 13s - loss: 1.3746 - accuracy: 0.4621 - val_loss: 1.5668 - val_accuracy: 0.3716\n",
      "Epoch 31/1500\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.56092\n",
      "39/38 - 13s - loss: 1.3711 - accuracy: 0.4482 - val_loss: 1.5627 - val_accuracy: 0.3716\n",
      "Epoch 32/1500\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.56092 to 1.55743, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.3412 - accuracy: 0.4817 - val_loss: 1.5574 - val_accuracy: 0.3686\n",
      "Epoch 33/1500\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.55743 to 1.52856, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.3615 - accuracy: 0.4466 - val_loss: 1.5286 - val_accuracy: 0.3837\n",
      "Epoch 34/1500\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.52856\n",
      "39/38 - 13s - loss: 1.3535 - accuracy: 0.4792 - val_loss: 1.5359 - val_accuracy: 0.3958\n",
      "Epoch 35/1500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.52856\n",
      "39/38 - 14s - loss: 1.3533 - accuracy: 0.4540 - val_loss: 1.5383 - val_accuracy: 0.3867\n",
      "Epoch 36/1500\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.52856\n",
      "39/38 - 13s - loss: 1.3522 - accuracy: 0.4629 - val_loss: 1.5422 - val_accuracy: 0.3927\n",
      "Epoch 37/1500\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.52856 to 1.49287, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.3373 - accuracy: 0.4792 - val_loss: 1.4929 - val_accuracy: 0.3837\n",
      "Epoch 38/1500\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.49287 to 1.48865, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.3344 - accuracy: 0.4866 - val_loss: 1.4887 - val_accuracy: 0.3897\n",
      "Epoch 39/1500\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.48865\n",
      "39/38 - 13s - loss: 1.3454 - accuracy: 0.4727 - val_loss: 1.5148 - val_accuracy: 0.3837\n",
      "Epoch 40/1500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.48865\n",
      "39/38 - 13s - loss: 1.3586 - accuracy: 0.4678 - val_loss: 1.5007 - val_accuracy: 0.3837\n",
      "Epoch 41/1500\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.48865 to 1.48685, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.3446 - accuracy: 0.4605 - val_loss: 1.4869 - val_accuracy: 0.3776\n",
      "Epoch 42/1500\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.48685\n",
      "39/38 - 14s - loss: 1.3128 - accuracy: 0.4931 - val_loss: 1.4882 - val_accuracy: 0.3837\n",
      "Epoch 43/1500\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.48685 to 1.48550, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.3204 - accuracy: 0.4645 - val_loss: 1.4855 - val_accuracy: 0.3927\n",
      "Epoch 44/1500\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.48550 to 1.47661, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.3136 - accuracy: 0.4874 - val_loss: 1.4766 - val_accuracy: 0.3837\n",
      "Epoch 45/1500\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.47661\n",
      "39/38 - 13s - loss: 1.2989 - accuracy: 0.5045 - val_loss: 1.4928 - val_accuracy: 0.4018\n",
      "Epoch 46/1500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.47661\n",
      "39/38 - 14s - loss: 1.3126 - accuracy: 0.4735 - val_loss: 1.4950 - val_accuracy: 0.3927\n",
      "Epoch 47/1500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.47661\n",
      "39/38 - 13s - loss: 1.2916 - accuracy: 0.4971 - val_loss: 1.4855 - val_accuracy: 0.3897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/1500\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.47661\n",
      "39/38 - 13s - loss: 1.2993 - accuracy: 0.5020 - val_loss: 1.4839 - val_accuracy: 0.3988\n",
      "Epoch 49/1500\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.47661 to 1.44482, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2761 - accuracy: 0.5020 - val_loss: 1.4448 - val_accuracy: 0.4048\n",
      "Epoch 50/1500\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.44482 to 1.44461, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2995 - accuracy: 0.4923 - val_loss: 1.4446 - val_accuracy: 0.4018\n",
      "Epoch 51/1500\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.44461\n",
      "39/38 - 13s - loss: 1.2757 - accuracy: 0.5020 - val_loss: 1.4531 - val_accuracy: 0.4079\n",
      "Epoch 52/1500\n",
      "\n",
      "Epoch 00052: val_loss improved from 1.44461 to 1.43893, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2839 - accuracy: 0.4898 - val_loss: 1.4389 - val_accuracy: 0.4018\n",
      "Epoch 53/1500\n",
      "\n",
      "Epoch 00053: val_loss improved from 1.43893 to 1.43505, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2770 - accuracy: 0.4890 - val_loss: 1.4350 - val_accuracy: 0.4018\n",
      "Epoch 54/1500\n",
      "\n",
      "Epoch 00054: val_loss improved from 1.43505 to 1.42707, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2440 - accuracy: 0.5363 - val_loss: 1.4271 - val_accuracy: 0.4079\n",
      "Epoch 55/1500\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.42707\n",
      "39/38 - 14s - loss: 1.2605 - accuracy: 0.4939 - val_loss: 1.4484 - val_accuracy: 0.4290\n",
      "Epoch 56/1500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.42707\n",
      "39/38 - 13s - loss: 1.3078 - accuracy: 0.4776 - val_loss: 1.4458 - val_accuracy: 0.4199\n",
      "Epoch 57/1500\n",
      "\n",
      "Epoch 00057: val_loss improved from 1.42707 to 1.41866, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2714 - accuracy: 0.5004 - val_loss: 1.4187 - val_accuracy: 0.4290\n",
      "Epoch 58/1500\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.41866\n",
      "39/38 - 13s - loss: 1.2605 - accuracy: 0.5086 - val_loss: 1.4258 - val_accuracy: 0.4230\n",
      "Epoch 59/1500\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.41866\n",
      "39/38 - 13s - loss: 1.2553 - accuracy: 0.5110 - val_loss: 1.4212 - val_accuracy: 0.4350\n",
      "Epoch 60/1500\n",
      "\n",
      "Epoch 00060: val_loss improved from 1.41866 to 1.41843, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2525 - accuracy: 0.5175 - val_loss: 1.4184 - val_accuracy: 0.4199\n",
      "Epoch 61/1500\n",
      "\n",
      "Epoch 00061: val_loss improved from 1.41843 to 1.41533, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2540 - accuracy: 0.5167 - val_loss: 1.4153 - val_accuracy: 0.4169\n",
      "Epoch 62/1500\n",
      "\n",
      "Epoch 00062: val_loss improved from 1.41533 to 1.40454, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2297 - accuracy: 0.5346 - val_loss: 1.4045 - val_accuracy: 0.4290\n",
      "Epoch 63/1500\n",
      "\n",
      "Epoch 00063: val_loss improved from 1.40454 to 1.38975, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2426 - accuracy: 0.5012 - val_loss: 1.3897 - val_accuracy: 0.4350\n",
      "Epoch 64/1500\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.38975\n",
      "39/38 - 14s - loss: 1.2518 - accuracy: 0.5183 - val_loss: 1.3905 - val_accuracy: 0.4320\n",
      "Epoch 65/1500\n",
      "\n",
      "Epoch 00065: val_loss improved from 1.38975 to 1.38761, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2367 - accuracy: 0.5183 - val_loss: 1.3876 - val_accuracy: 0.4350\n",
      "Epoch 66/1500\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.38761\n",
      "39/38 - 13s - loss: 1.2442 - accuracy: 0.5208 - val_loss: 1.3891 - val_accuracy: 0.4411\n",
      "Epoch 67/1500\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.38761\n",
      "39/38 - 13s - loss: 1.2419 - accuracy: 0.5175 - val_loss: 1.3902 - val_accuracy: 0.4411\n",
      "Epoch 68/1500\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.38761\n",
      "39/38 - 14s - loss: 1.2145 - accuracy: 0.5281 - val_loss: 1.4085 - val_accuracy: 0.4441\n",
      "Epoch 69/1500\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.38761\n",
      "39/38 - 13s - loss: 1.2036 - accuracy: 0.5387 - val_loss: 1.3992 - val_accuracy: 0.4441\n",
      "Epoch 70/1500\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.38761\n",
      "39/38 - 13s - loss: 1.2105 - accuracy: 0.5338 - val_loss: 1.3952 - val_accuracy: 0.4441\n",
      "Epoch 71/1500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.38761\n",
      "39/38 - 13s - loss: 1.2038 - accuracy: 0.5257 - val_loss: 1.3899 - val_accuracy: 0.4471\n",
      "Epoch 72/1500\n",
      "\n",
      "Epoch 00072: val_loss improved from 1.38761 to 1.38360, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2313 - accuracy: 0.5134 - val_loss: 1.3836 - val_accuracy: 0.4532\n",
      "Epoch 73/1500\n",
      "\n",
      "Epoch 00073: val_loss improved from 1.38360 to 1.36837, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1853 - accuracy: 0.5509 - val_loss: 1.3684 - val_accuracy: 0.4532\n",
      "Epoch 74/1500\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.36837\n",
      "39/38 - 13s - loss: 1.2158 - accuracy: 0.5159 - val_loss: 1.3685 - val_accuracy: 0.4441\n",
      "Epoch 75/1500\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.36837\n",
      "39/38 - 13s - loss: 1.1921 - accuracy: 0.5403 - val_loss: 1.3710 - val_accuracy: 0.4471\n",
      "Epoch 76/1500\n",
      "\n",
      "Epoch 00076: val_loss improved from 1.36837 to 1.35882, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.2100 - accuracy: 0.5289 - val_loss: 1.3588 - val_accuracy: 0.4411\n",
      "Epoch 77/1500\n",
      "\n",
      "Epoch 00077: val_loss improved from 1.35882 to 1.35672, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1976 - accuracy: 0.5460 - val_loss: 1.3567 - val_accuracy: 0.4622\n",
      "Epoch 78/1500\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.35672\n",
      "39/38 - 14s - loss: 1.1742 - accuracy: 0.5558 - val_loss: 1.3580 - val_accuracy: 0.4683\n",
      "Epoch 79/1500\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.35672\n",
      "39/38 - 14s - loss: 1.1784 - accuracy: 0.5346 - val_loss: 1.3656 - val_accuracy: 0.4562\n",
      "Epoch 80/1500\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.35672\n",
      "39/38 - 13s - loss: 1.1948 - accuracy: 0.5338 - val_loss: 1.3568 - val_accuracy: 0.4532\n",
      "Epoch 81/1500\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.35672\n",
      "39/38 - 13s - loss: 1.1770 - accuracy: 0.5558 - val_loss: 1.3803 - val_accuracy: 0.4532\n",
      "Epoch 82/1500\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.35672\n",
      "39/38 - 13s - loss: 1.1867 - accuracy: 0.5395 - val_loss: 1.3694 - val_accuracy: 0.4532\n",
      "Epoch 83/1500\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.35672\n",
      "39/38 - 13s - loss: 1.1732 - accuracy: 0.5501 - val_loss: 1.3632 - val_accuracy: 0.4622\n",
      "Epoch 84/1500\n",
      "\n",
      "Epoch 00084: val_loss improved from 1.35672 to 1.32230, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1674 - accuracy: 0.5558 - val_loss: 1.3223 - val_accuracy: 0.4713\n",
      "Epoch 85/1500\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.32230\n",
      "39/38 - 13s - loss: 1.1541 - accuracy: 0.5460 - val_loss: 1.3405 - val_accuracy: 0.4562\n",
      "Epoch 86/1500\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.32230\n",
      "39/38 - 14s - loss: 1.1546 - accuracy: 0.5509 - val_loss: 1.3574 - val_accuracy: 0.4804\n",
      "Epoch 87/1500\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.32230\n",
      "39/38 - 13s - loss: 1.1513 - accuracy: 0.5615 - val_loss: 1.3574 - val_accuracy: 0.4653\n",
      "Epoch 88/1500\n",
      "\n",
      "Epoch 00088: val_loss improved from 1.32230 to 1.30449, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1817 - accuracy: 0.5346 - val_loss: 1.3045 - val_accuracy: 0.4743\n",
      "Epoch 89/1500\n",
      "\n",
      "Epoch 00089: val_loss improved from 1.30449 to 1.29940, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1546 - accuracy: 0.5452 - val_loss: 1.2994 - val_accuracy: 0.4864\n",
      "Epoch 90/1500\n",
      "\n",
      "Epoch 00090: val_loss improved from 1.29940 to 1.29291, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1311 - accuracy: 0.5501 - val_loss: 1.2929 - val_accuracy: 0.4834\n",
      "Epoch 91/1500\n",
      "\n",
      "Epoch 00091: val_loss improved from 1.29291 to 1.27232, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1536 - accuracy: 0.5558 - val_loss: 1.2723 - val_accuracy: 0.4743\n",
      "Epoch 92/1500\n",
      "\n",
      "Epoch 00092: val_loss improved from 1.27232 to 1.26339, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1327 - accuracy: 0.5721 - val_loss: 1.2634 - val_accuracy: 0.4804\n",
      "Epoch 93/1500\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.26339\n",
      "39/38 - 13s - loss: 1.1262 - accuracy: 0.5615 - val_loss: 1.2644 - val_accuracy: 0.4683\n",
      "Epoch 94/1500\n",
      "\n",
      "Epoch 00094: val_loss improved from 1.26339 to 1.25854, saving model to piece_model1.weights.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/38 - 14s - loss: 1.1626 - accuracy: 0.5526 - val_loss: 1.2585 - val_accuracy: 0.4743\n",
      "Epoch 95/1500\n",
      "\n",
      "Epoch 00095: val_loss improved from 1.25854 to 1.25799, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1128 - accuracy: 0.5721 - val_loss: 1.2580 - val_accuracy: 0.4955\n",
      "Epoch 96/1500\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.25799\n",
      "39/38 - 14s - loss: 1.1333 - accuracy: 0.5729 - val_loss: 1.2734 - val_accuracy: 0.4804\n",
      "Epoch 97/1500\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.25799\n",
      "39/38 - 13s - loss: 1.1601 - accuracy: 0.5363 - val_loss: 1.2615 - val_accuracy: 0.4985\n",
      "Epoch 98/1500\n",
      "\n",
      "Epoch 00098: val_loss improved from 1.25799 to 1.24678, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1466 - accuracy: 0.5811 - val_loss: 1.2468 - val_accuracy: 0.4985\n",
      "Epoch 99/1500\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.24678\n",
      "39/38 - 13s - loss: 1.1063 - accuracy: 0.5868 - val_loss: 1.2544 - val_accuracy: 0.4955\n",
      "Epoch 100/1500\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.24678\n",
      "39/38 - 14s - loss: 1.1209 - accuracy: 0.5599 - val_loss: 1.2958 - val_accuracy: 0.4894\n",
      "Epoch 101/1500\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.24678\n",
      "39/38 - 14s - loss: 1.1248 - accuracy: 0.5689 - val_loss: 1.2685 - val_accuracy: 0.4743\n",
      "Epoch 102/1500\n",
      "\n",
      "Epoch 00102: val_loss improved from 1.24678 to 1.24672, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1035 - accuracy: 0.5729 - val_loss: 1.2467 - val_accuracy: 0.5136\n",
      "Epoch 103/1500\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.24672\n",
      "39/38 - 14s - loss: 1.0945 - accuracy: 0.5656 - val_loss: 1.2652 - val_accuracy: 0.5136\n",
      "Epoch 104/1500\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.24672\n",
      "39/38 - 13s - loss: 1.1082 - accuracy: 0.5795 - val_loss: 1.2738 - val_accuracy: 0.5166\n",
      "Epoch 105/1500\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.24672\n",
      "39/38 - 13s - loss: 1.1189 - accuracy: 0.5566 - val_loss: 1.2600 - val_accuracy: 0.4955\n",
      "Epoch 106/1500\n",
      "\n",
      "Epoch 00106: val_loss improved from 1.24672 to 1.23460, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0852 - accuracy: 0.5974 - val_loss: 1.2346 - val_accuracy: 0.5106\n",
      "Epoch 107/1500\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.23460\n",
      "39/38 - 13s - loss: 1.1121 - accuracy: 0.5648 - val_loss: 1.2386 - val_accuracy: 0.5106\n",
      "Epoch 108/1500\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.23460\n",
      "39/38 - 13s - loss: 1.1112 - accuracy: 0.5640 - val_loss: 1.2347 - val_accuracy: 0.5076\n",
      "Epoch 109/1500\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.23460\n",
      "39/38 - 13s - loss: 1.0770 - accuracy: 0.5795 - val_loss: 1.2412 - val_accuracy: 0.5076\n",
      "Epoch 110/1500\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.23460\n",
      "39/38 - 13s - loss: 1.1050 - accuracy: 0.5729 - val_loss: 1.2447 - val_accuracy: 0.5015\n",
      "Epoch 111/1500\n",
      "\n",
      "Epoch 00111: val_loss improved from 1.23460 to 1.22834, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.1086 - accuracy: 0.5811 - val_loss: 1.2283 - val_accuracy: 0.5287\n",
      "Epoch 112/1500\n",
      "\n",
      "Epoch 00112: val_loss improved from 1.22834 to 1.21875, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0954 - accuracy: 0.5860 - val_loss: 1.2187 - val_accuracy: 0.5287\n",
      "Epoch 113/1500\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.21875\n",
      "39/38 - 13s - loss: 1.0832 - accuracy: 0.5844 - val_loss: 1.2255 - val_accuracy: 0.5317\n",
      "Epoch 114/1500\n",
      "\n",
      "Epoch 00114: val_loss improved from 1.21875 to 1.20520, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0832 - accuracy: 0.5738 - val_loss: 1.2052 - val_accuracy: 0.5498\n",
      "Epoch 115/1500\n",
      "\n",
      "Epoch 00115: val_loss improved from 1.20520 to 1.20184, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0811 - accuracy: 0.5909 - val_loss: 1.2018 - val_accuracy: 0.5498\n",
      "Epoch 116/1500\n",
      "\n",
      "Epoch 00116: val_loss improved from 1.20184 to 1.19183, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0706 - accuracy: 0.6031 - val_loss: 1.1918 - val_accuracy: 0.5529\n",
      "Epoch 117/1500\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.19183\n",
      "39/38 - 13s - loss: 1.0531 - accuracy: 0.5892 - val_loss: 1.1958 - val_accuracy: 0.5650\n",
      "Epoch 118/1500\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.19183\n",
      "39/38 - 13s - loss: 1.0411 - accuracy: 0.6186 - val_loss: 1.1928 - val_accuracy: 0.5529\n",
      "Epoch 119/1500\n",
      "\n",
      "Epoch 00119: val_loss improved from 1.19183 to 1.18200, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0764 - accuracy: 0.5901 - val_loss: 1.1820 - val_accuracy: 0.5619\n",
      "Epoch 120/1500\n",
      "\n",
      "Epoch 00120: val_loss improved from 1.18200 to 1.18073, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0665 - accuracy: 0.5868 - val_loss: 1.1807 - val_accuracy: 0.5559\n",
      "Epoch 121/1500\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.18073\n",
      "39/38 - 14s - loss: 1.0746 - accuracy: 0.5868 - val_loss: 1.2076 - val_accuracy: 0.5619\n",
      "Epoch 122/1500\n",
      "\n",
      "Epoch 00122: val_loss improved from 1.18073 to 1.15875, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0663 - accuracy: 0.6023 - val_loss: 1.1588 - val_accuracy: 0.5166\n",
      "Epoch 123/1500\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.15875\n",
      "39/38 - 13s - loss: 1.0396 - accuracy: 0.6015 - val_loss: 1.1902 - val_accuracy: 0.5257\n",
      "Epoch 124/1500\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.15875\n",
      "39/38 - 14s - loss: 1.0435 - accuracy: 0.6015 - val_loss: 1.1877 - val_accuracy: 0.5257\n",
      "Epoch 125/1500\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.15875\n",
      "39/38 - 14s - loss: 1.0722 - accuracy: 0.5803 - val_loss: 1.1806 - val_accuracy: 0.5196\n",
      "Epoch 126/1500\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.15875\n",
      "39/38 - 13s - loss: 1.0634 - accuracy: 0.6055 - val_loss: 1.1648 - val_accuracy: 0.5589\n",
      "Epoch 127/1500\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.15875\n",
      "39/38 - 14s - loss: 1.0094 - accuracy: 0.6039 - val_loss: 1.1776 - val_accuracy: 0.5680\n",
      "Epoch 128/1500\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.15875\n",
      "39/38 - 13s - loss: 1.0359 - accuracy: 0.5884 - val_loss: 1.1674 - val_accuracy: 0.5559\n",
      "Epoch 129/1500\n",
      "\n",
      "Epoch 00129: val_loss improved from 1.15875 to 1.15858, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0444 - accuracy: 0.5958 - val_loss: 1.1586 - val_accuracy: 0.5680\n",
      "Epoch 130/1500\n",
      "\n",
      "Epoch 00130: val_loss improved from 1.15858 to 1.12953, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0291 - accuracy: 0.6039 - val_loss: 1.1295 - val_accuracy: 0.5710\n",
      "Epoch 131/1500\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.12953\n",
      "39/38 - 13s - loss: 1.0129 - accuracy: 0.6064 - val_loss: 1.1414 - val_accuracy: 0.5740\n",
      "Epoch 132/1500\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.12953\n",
      "39/38 - 13s - loss: 1.0279 - accuracy: 0.6104 - val_loss: 1.1361 - val_accuracy: 0.5831\n",
      "Epoch 133/1500\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.12953\n",
      "39/38 - 13s - loss: 1.0071 - accuracy: 0.5974 - val_loss: 1.1379 - val_accuracy: 0.5831\n",
      "Epoch 134/1500\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.12953\n",
      "39/38 - 13s - loss: 1.0665 - accuracy: 0.6202 - val_loss: 1.1591 - val_accuracy: 0.5468\n",
      "Epoch 135/1500\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.12953\n",
      "39/38 - 13s - loss: 1.0114 - accuracy: 0.5982 - val_loss: 1.1911 - val_accuracy: 0.5287\n",
      "Epoch 136/1500\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.12953\n",
      "39/38 - 13s - loss: 1.0017 - accuracy: 0.6243 - val_loss: 1.1477 - val_accuracy: 0.5861\n",
      "Epoch 137/1500\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.12953\n",
      "39/38 - 13s - loss: 0.9926 - accuracy: 0.6349 - val_loss: 1.1319 - val_accuracy: 0.5468\n",
      "Epoch 138/1500\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.12953\n",
      "39/38 - 13s - loss: 1.0187 - accuracy: 0.5974 - val_loss: 1.1360 - val_accuracy: 0.5619\n",
      "Epoch 139/1500\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.12953\n",
      "39/38 - 13s - loss: 1.0431 - accuracy: 0.6055 - val_loss: 1.1879 - val_accuracy: 0.5196\n",
      "Epoch 140/1500\n",
      "\n",
      "Epoch 00140: val_loss improved from 1.12953 to 1.10853, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.9748 - accuracy: 0.6235 - val_loss: 1.1085 - val_accuracy: 0.5680\n",
      "Epoch 141/1500\n",
      "\n",
      "Epoch 00141: val_loss improved from 1.10853 to 1.10133, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0102 - accuracy: 0.6267 - val_loss: 1.1013 - val_accuracy: 0.5680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/1500\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.10133\n",
      "39/38 - 13s - loss: 0.9933 - accuracy: 0.6088 - val_loss: 1.1070 - val_accuracy: 0.6012\n",
      "Epoch 143/1500\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.10133\n",
      "39/38 - 13s - loss: 0.9913 - accuracy: 0.6186 - val_loss: 1.1014 - val_accuracy: 0.6073\n",
      "Epoch 144/1500\n",
      "\n",
      "Epoch 00144: val_loss improved from 1.10133 to 1.09879, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.9859 - accuracy: 0.6112 - val_loss: 1.0988 - val_accuracy: 0.6163\n",
      "Epoch 145/1500\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.09879\n",
      "39/38 - 13s - loss: 0.9959 - accuracy: 0.6300 - val_loss: 1.1038 - val_accuracy: 0.6012\n",
      "Epoch 146/1500\n",
      "\n",
      "Epoch 00146: val_loss improved from 1.09879 to 1.09054, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0265 - accuracy: 0.6186 - val_loss: 1.0905 - val_accuracy: 0.6163\n",
      "Epoch 147/1500\n",
      "\n",
      "Epoch 00147: val_loss improved from 1.09054 to 1.08813, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 1.0100 - accuracy: 0.6202 - val_loss: 1.0881 - val_accuracy: 0.5801\n",
      "Epoch 148/1500\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.08813\n",
      "39/38 - 14s - loss: 1.0101 - accuracy: 0.5974 - val_loss: 1.1072 - val_accuracy: 0.6133\n",
      "Epoch 149/1500\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.08813\n",
      "39/38 - 13s - loss: 0.9618 - accuracy: 0.6259 - val_loss: 1.0975 - val_accuracy: 0.6012\n",
      "Epoch 150/1500\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.08813\n",
      "39/38 - 13s - loss: 0.9651 - accuracy: 0.6284 - val_loss: 1.0968 - val_accuracy: 0.6193\n",
      "Epoch 151/1500\n",
      "\n",
      "Epoch 00151: val_loss improved from 1.08813 to 1.07231, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.9873 - accuracy: 0.6243 - val_loss: 1.0723 - val_accuracy: 0.6103\n",
      "Epoch 152/1500\n",
      "\n",
      "Epoch 00152: val_loss improved from 1.07231 to 1.06885, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.9720 - accuracy: 0.6365 - val_loss: 1.0688 - val_accuracy: 0.5982\n",
      "Epoch 153/1500\n",
      "\n",
      "Epoch 00153: val_loss improved from 1.06885 to 1.06112, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.9586 - accuracy: 0.6178 - val_loss: 1.0611 - val_accuracy: 0.6073\n",
      "Epoch 154/1500\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.06112\n",
      "39/38 - 14s - loss: 0.9675 - accuracy: 0.6292 - val_loss: 1.0638 - val_accuracy: 0.6042\n",
      "Epoch 155/1500\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.06112\n",
      "39/38 - 13s - loss: 0.9308 - accuracy: 0.6422 - val_loss: 1.0778 - val_accuracy: 0.5952\n",
      "Epoch 156/1500\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.06112\n",
      "39/38 - 14s - loss: 0.9423 - accuracy: 0.6316 - val_loss: 1.0974 - val_accuracy: 0.6012\n",
      "Epoch 157/1500\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.06112\n",
      "39/38 - 13s - loss: 0.9469 - accuracy: 0.6243 - val_loss: 1.0747 - val_accuracy: 0.6224\n",
      "Epoch 158/1500\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.06112\n",
      "39/38 - 13s - loss: 0.9501 - accuracy: 0.6479 - val_loss: 1.0721 - val_accuracy: 0.6314\n",
      "Epoch 159/1500\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.06112\n",
      "39/38 - 14s - loss: 0.9506 - accuracy: 0.6569 - val_loss: 1.0624 - val_accuracy: 0.6224\n",
      "Epoch 160/1500\n",
      "\n",
      "Epoch 00160: val_loss improved from 1.06112 to 1.02466, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.9573 - accuracy: 0.6243 - val_loss: 1.0247 - val_accuracy: 0.6193\n",
      "Epoch 161/1500\n",
      "\n",
      "Epoch 00161: val_loss improved from 1.02466 to 1.01438, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.9477 - accuracy: 0.6414 - val_loss: 1.0144 - val_accuracy: 0.6103\n",
      "Epoch 162/1500\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9210 - accuracy: 0.6471 - val_loss: 1.0452 - val_accuracy: 0.6314\n",
      "Epoch 163/1500\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9605 - accuracy: 0.6414 - val_loss: 1.0464 - val_accuracy: 0.6224\n",
      "Epoch 164/1500\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9233 - accuracy: 0.6438 - val_loss: 1.0762 - val_accuracy: 0.6193\n",
      "Epoch 167/1500\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9112 - accuracy: 0.6544 - val_loss: 1.0426 - val_accuracy: 0.6405\n",
      "Epoch 168/1500\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9244 - accuracy: 0.6430 - val_loss: 1.0444 - val_accuracy: 0.6314\n",
      "Epoch 169/1500\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9580 - accuracy: 0.6218 - val_loss: 1.0841 - val_accuracy: 0.5831\n",
      "Epoch 170/1500\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.01438\n",
      "39/38 - 14s - loss: 0.9616 - accuracy: 0.6235 - val_loss: 1.0623 - val_accuracy: 0.6224\n",
      "Epoch 171/1500\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.8979 - accuracy: 0.6650 - val_loss: 1.0705 - val_accuracy: 0.6284\n",
      "Epoch 172/1500\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9396 - accuracy: 0.6414 - val_loss: 1.0743 - val_accuracy: 0.5861\n",
      "Epoch 173/1500\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9117 - accuracy: 0.6634 - val_loss: 1.0555 - val_accuracy: 0.6193\n",
      "Epoch 174/1500\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9142 - accuracy: 0.6447 - val_loss: 1.0303 - val_accuracy: 0.6254\n",
      "Epoch 175/1500\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9073 - accuracy: 0.6569 - val_loss: 1.0184 - val_accuracy: 0.6193\n",
      "Epoch 176/1500\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9137 - accuracy: 0.6528 - val_loss: 1.0615 - val_accuracy: 0.5982\n",
      "Epoch 177/1500\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9127 - accuracy: 0.6528 - val_loss: 1.0603 - val_accuracy: 0.6042\n",
      "Epoch 178/1500\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.01438\n",
      "39/38 - 13s - loss: 0.9170 - accuracy: 0.6520 - val_loss: 1.0260 - val_accuracy: 0.5921\n",
      "Epoch 179/1500\n",
      "\n",
      "Epoch 00179: val_loss improved from 1.01438 to 0.98690, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.9267 - accuracy: 0.6381 - val_loss: 0.9869 - val_accuracy: 0.6284\n",
      "Epoch 180/1500\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.98690 to 0.98036, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8811 - accuracy: 0.6447 - val_loss: 0.9804 - val_accuracy: 0.6254\n",
      "Epoch 181/1500\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.98036\n",
      "39/38 - 13s - loss: 0.8964 - accuracy: 0.6683 - val_loss: 0.9993 - val_accuracy: 0.6284\n",
      "Epoch 182/1500\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.98036 to 0.97314, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.9018 - accuracy: 0.6634 - val_loss: 0.9731 - val_accuracy: 0.6375\n",
      "Epoch 183/1500\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.97314\n",
      "39/38 - 13s - loss: 0.8814 - accuracy: 0.6683 - val_loss: 0.9892 - val_accuracy: 0.6495\n",
      "Epoch 184/1500\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.97314\n",
      "39/38 - 13s - loss: 0.9017 - accuracy: 0.6561 - val_loss: 0.9858 - val_accuracy: 0.6405\n",
      "Epoch 185/1500\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.97314\n",
      "39/38 - 13s - loss: 0.8815 - accuracy: 0.6707 - val_loss: 0.9851 - val_accuracy: 0.6586\n",
      "Epoch 186/1500\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.97314\n",
      "39/38 - 13s - loss: 0.9277 - accuracy: 0.6577 - val_loss: 0.9778 - val_accuracy: 0.6556\n",
      "Epoch 187/1500\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.97314\n",
      "39/38 - 14s - loss: 0.8743 - accuracy: 0.6683 - val_loss: 0.9870 - val_accuracy: 0.6435\n",
      "Epoch 188/1500\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.97314 to 0.96748, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8905 - accuracy: 0.6447 - val_loss: 0.9675 - val_accuracy: 0.6526\n",
      "Epoch 189/1500\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.96748\n",
      "39/38 - 13s - loss: 0.8672 - accuracy: 0.6699 - val_loss: 0.9718 - val_accuracy: 0.6435\n",
      "Epoch 190/1500\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.96748 to 0.94816, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8647 - accuracy: 0.6634 - val_loss: 0.9482 - val_accuracy: 0.6526\n",
      "Epoch 191/1500\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.94816\n",
      "39/38 - 13s - loss: 0.8783 - accuracy: 0.6626 - val_loss: 0.9700 - val_accuracy: 0.6254\n",
      "Epoch 192/1500\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.94816\n",
      "39/38 - 13s - loss: 0.8573 - accuracy: 0.6667 - val_loss: 0.9858 - val_accuracy: 0.6193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/1500\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.94816\n",
      "39/38 - 14s - loss: 0.9117 - accuracy: 0.6520 - val_loss: 0.9638 - val_accuracy: 0.6495\n",
      "Epoch 194/1500\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.94816\n",
      "39/38 - 13s - loss: 0.8650 - accuracy: 0.6634 - val_loss: 0.9647 - val_accuracy: 0.6405\n",
      "Epoch 195/1500\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.94816\n",
      "39/38 - 14s - loss: 0.8814 - accuracy: 0.6561 - val_loss: 0.9643 - val_accuracy: 0.6435\n",
      "Epoch 196/1500\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.94816 to 0.93938, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8496 - accuracy: 0.6675 - val_loss: 0.9394 - val_accuracy: 0.6375\n",
      "Epoch 197/1500\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.93938\n",
      "39/38 - 13s - loss: 0.8814 - accuracy: 0.6463 - val_loss: 1.0053 - val_accuracy: 0.6193\n",
      "Epoch 198/1500\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.93938 to 0.93723, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8877 - accuracy: 0.6577 - val_loss: 0.9372 - val_accuracy: 0.6556\n",
      "Epoch 199/1500\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.93723 to 0.93660, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8795 - accuracy: 0.6561 - val_loss: 0.9366 - val_accuracy: 0.6405\n",
      "Epoch 200/1500\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.93660\n",
      "39/38 - 14s - loss: 0.8897 - accuracy: 0.6618 - val_loss: 0.9597 - val_accuracy: 0.6375\n",
      "Epoch 201/1500\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.93660\n",
      "39/38 - 13s - loss: 0.8587 - accuracy: 0.6781 - val_loss: 0.9888 - val_accuracy: 0.6314\n",
      "Epoch 202/1500\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.93660\n",
      "39/38 - 13s - loss: 0.8551 - accuracy: 0.6822 - val_loss: 0.9531 - val_accuracy: 0.6586\n",
      "Epoch 203/1500\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.93660\n",
      "39/38 - 13s - loss: 0.8289 - accuracy: 0.6879 - val_loss: 1.0053 - val_accuracy: 0.6193\n",
      "Epoch 204/1500\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.93660\n",
      "39/38 - 13s - loss: 0.8864 - accuracy: 0.6569 - val_loss: 0.9414 - val_accuracy: 0.6556\n",
      "Epoch 205/1500\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.93660\n",
      "39/38 - 14s - loss: 0.8566 - accuracy: 0.6699 - val_loss: 0.9444 - val_accuracy: 0.6556\n",
      "Epoch 206/1500\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.93660\n",
      "39/38 - 14s - loss: 0.8222 - accuracy: 0.6748 - val_loss: 0.9379 - val_accuracy: 0.6495\n",
      "Epoch 207/1500\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.93660\n",
      "39/38 - 13s - loss: 0.8281 - accuracy: 0.6667 - val_loss: 0.9820 - val_accuracy: 0.6556\n",
      "Epoch 208/1500\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.93660\n",
      "39/38 - 13s - loss: 0.8361 - accuracy: 0.6862 - val_loss: 1.0206 - val_accuracy: 0.6193\n",
      "Epoch 209/1500\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.93660\n",
      "39/38 - 13s - loss: 0.8886 - accuracy: 0.6707 - val_loss: 1.0163 - val_accuracy: 0.6254\n",
      "Epoch 210/1500\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.93660\n",
      "39/38 - 14s - loss: 0.8499 - accuracy: 0.6830 - val_loss: 1.0322 - val_accuracy: 0.6163\n",
      "Epoch 211/1500\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.93660\n",
      "39/38 - 13s - loss: 0.8222 - accuracy: 0.6854 - val_loss: 0.9693 - val_accuracy: 0.6284\n",
      "Epoch 212/1500\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.93660\n",
      "39/38 - 13s - loss: 0.8341 - accuracy: 0.6968 - val_loss: 0.9616 - val_accuracy: 0.6375\n",
      "Epoch 213/1500\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.93660 to 0.92430, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8223 - accuracy: 0.6813 - val_loss: 0.9243 - val_accuracy: 0.6284\n",
      "Epoch 214/1500\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.92430 to 0.91342, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8229 - accuracy: 0.6805 - val_loss: 0.9134 - val_accuracy: 0.6586\n",
      "Epoch 215/1500\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.91342\n",
      "39/38 - 13s - loss: 0.8596 - accuracy: 0.6724 - val_loss: 0.9294 - val_accuracy: 0.6616\n",
      "Epoch 216/1500\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.91342\n",
      "39/38 - 14s - loss: 0.8757 - accuracy: 0.6675 - val_loss: 0.9435 - val_accuracy: 0.6405\n",
      "Epoch 217/1500\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.91342\n",
      "39/38 - 13s - loss: 0.8283 - accuracy: 0.6764 - val_loss: 0.9627 - val_accuracy: 0.6556\n",
      "Epoch 218/1500\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.91342\n",
      "39/38 - 13s - loss: 0.8121 - accuracy: 0.6732 - val_loss: 0.9217 - val_accuracy: 0.6405\n",
      "Epoch 219/1500\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.91342 to 0.90400, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8269 - accuracy: 0.6691 - val_loss: 0.9040 - val_accuracy: 0.6465\n",
      "Epoch 220/1500\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.90400\n",
      "39/38 - 14s - loss: 0.8443 - accuracy: 0.6870 - val_loss: 0.9221 - val_accuracy: 0.6435\n",
      "Epoch 221/1500\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.90400\n",
      "39/38 - 13s - loss: 0.8591 - accuracy: 0.6756 - val_loss: 0.9082 - val_accuracy: 0.6677\n",
      "Epoch 222/1500\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.90400\n",
      "39/38 - 13s - loss: 0.8228 - accuracy: 0.6960 - val_loss: 0.9294 - val_accuracy: 0.6435\n",
      "Epoch 223/1500\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.90400\n",
      "39/38 - 13s - loss: 0.8022 - accuracy: 0.6976 - val_loss: 0.9115 - val_accuracy: 0.6616\n",
      "Epoch 224/1500\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.90400\n",
      "39/38 - 13s - loss: 0.8391 - accuracy: 0.6756 - val_loss: 0.9337 - val_accuracy: 0.6405\n",
      "Epoch 225/1500\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.90400\n",
      "39/38 - 13s - loss: 0.8388 - accuracy: 0.6781 - val_loss: 0.9066 - val_accuracy: 0.6435\n",
      "Epoch 227/1500\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.90400 to 0.88812, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8155 - accuracy: 0.6993 - val_loss: 0.8881 - val_accuracy: 0.6677\n",
      "Epoch 228/1500\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.88812\n",
      "39/38 - 13s - loss: 0.8130 - accuracy: 0.6879 - val_loss: 0.8979 - val_accuracy: 0.6586\n",
      "Epoch 231/1500\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.88812\n",
      "39/38 - 13s - loss: 0.8355 - accuracy: 0.6789 - val_loss: 0.8978 - val_accuracy: 0.6616\n",
      "Epoch 232/1500\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.85190\n",
      "39/38 - 13s - loss: 0.7890 - accuracy: 0.7050 - val_loss: 0.8619 - val_accuracy: 0.6737\n",
      "Epoch 235/1500\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.85190\n",
      "39/38 - 14s - loss: 0.7696 - accuracy: 0.7090 - val_loss: 0.8696 - val_accuracy: 0.6828\n",
      "Epoch 236/1500\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.85190 to 0.84376, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.7892 - accuracy: 0.6976 - val_loss: 0.8438 - val_accuracy: 0.6888\n",
      "Epoch 237/1500\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.84376\n",
      "39/38 - 13s - loss: 0.8152 - accuracy: 0.6879 - val_loss: 0.8521 - val_accuracy: 0.6828\n",
      "Epoch 238/1500\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.84376 to 0.84322, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8143 - accuracy: 0.6846 - val_loss: 0.8432 - val_accuracy: 0.6828\n",
      "Epoch 239/1500\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.84322 to 0.83241, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.8043 - accuracy: 0.7107 - val_loss: 0.8324 - val_accuracy: 0.7009\n",
      "Epoch 240/1500\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.8340 - accuracy: 0.6976 - val_loss: 0.8871 - val_accuracy: 0.6737\n",
      "Epoch 241/1500\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7683 - accuracy: 0.7066 - val_loss: 0.8610 - val_accuracy: 0.6737\n",
      "Epoch 242/1500\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.83241\n",
      "39/38 - 14s - loss: 0.7993 - accuracy: 0.6846 - val_loss: 0.8780 - val_accuracy: 0.6798\n",
      "Epoch 243/1500\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7822 - accuracy: 0.7042 - val_loss: 0.8639 - val_accuracy: 0.6828\n",
      "Epoch 244/1500\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7856 - accuracy: 0.7074 - val_loss: 0.8787 - val_accuracy: 0.6616\n",
      "Epoch 246/1500\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7920 - accuracy: 0.7025 - val_loss: 0.8348 - val_accuracy: 0.6586\n",
      "Epoch 247/1500\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7997 - accuracy: 0.7025 - val_loss: 0.8383 - val_accuracy: 0.6677\n",
      "Epoch 248/1500\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7827 - accuracy: 0.7025 - val_loss: 0.8329 - val_accuracy: 0.6949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/1500\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7980 - accuracy: 0.7107 - val_loss: 0.8712 - val_accuracy: 0.6707\n",
      "Epoch 251/1500\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7717 - accuracy: 0.7050 - val_loss: 0.8509 - val_accuracy: 0.6737\n",
      "Epoch 252/1500\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7622 - accuracy: 0.7205 - val_loss: 0.8720 - val_accuracy: 0.6828\n",
      "Epoch 253/1500\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7446 - accuracy: 0.7123 - val_loss: 0.8441 - val_accuracy: 0.6949\n",
      "Epoch 254/1500\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7803 - accuracy: 0.6936 - val_loss: 0.8642 - val_accuracy: 0.6888\n",
      "Epoch 255/1500\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7555 - accuracy: 0.7229 - val_loss: 0.8729 - val_accuracy: 0.6737\n",
      "Epoch 256/1500\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7796 - accuracy: 0.7115 - val_loss: 0.8748 - val_accuracy: 0.6737\n",
      "Epoch 257/1500\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.83241\n",
      "39/38 - 13s - loss: 0.7581 - accuracy: 0.7172 - val_loss: 0.8566 - val_accuracy: 0.6707\n",
      "Epoch 258/1500\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.83241 to 0.81720, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.7551 - accuracy: 0.7156 - val_loss: 0.8172 - val_accuracy: 0.6798\n",
      "Epoch 259/1500\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7533 - accuracy: 0.7139 - val_loss: 0.8280 - val_accuracy: 0.6828\n",
      "Epoch 260/1500\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7442 - accuracy: 0.7099 - val_loss: 0.8634 - val_accuracy: 0.6767\n",
      "Epoch 261/1500\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.81720\n",
      "39/38 - 14s - loss: 0.7717 - accuracy: 0.7090 - val_loss: 0.8560 - val_accuracy: 0.6707\n",
      "Epoch 262/1500\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.81720\n",
      "39/38 - 14s - loss: 0.7630 - accuracy: 0.7156 - val_loss: 0.8354 - val_accuracy: 0.6737\n",
      "Epoch 263/1500\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7680 - accuracy: 0.7123 - val_loss: 0.8457 - val_accuracy: 0.6828\n",
      "Epoch 264/1500\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7782 - accuracy: 0.7221 - val_loss: 0.8280 - val_accuracy: 0.7009\n",
      "Epoch 265/1500\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7466 - accuracy: 0.7245 - val_loss: 0.8378 - val_accuracy: 0.6949\n",
      "Epoch 266/1500\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7373 - accuracy: 0.7148 - val_loss: 0.8414 - val_accuracy: 0.6858\n",
      "Epoch 267/1500\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7407 - accuracy: 0.7221 - val_loss: 0.8443 - val_accuracy: 0.6979\n",
      "Epoch 268/1500\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7277 - accuracy: 0.7164 - val_loss: 0.8345 - val_accuracy: 0.6918\n",
      "Epoch 269/1500\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.81720\n",
      "39/38 - 14s - loss: 0.7502 - accuracy: 0.7131 - val_loss: 0.8597 - val_accuracy: 0.6737\n",
      "Epoch 270/1500\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7581 - accuracy: 0.7115 - val_loss: 0.8304 - val_accuracy: 0.6798\n",
      "Epoch 271/1500\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7766 - accuracy: 0.7123 - val_loss: 0.8328 - val_accuracy: 0.6798\n",
      "Epoch 272/1500\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.81720\n",
      "39/38 - 13s - loss: 0.7483 - accuracy: 0.7229 - val_loss: 0.8288 - val_accuracy: 0.6888\n",
      "Epoch 273/1500\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.81720 to 0.79678, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.7379 - accuracy: 0.7107 - val_loss: 0.7968 - val_accuracy: 0.6979\n",
      "Epoch 274/1500\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.79678\n",
      "39/38 - 14s - loss: 0.7170 - accuracy: 0.7245 - val_loss: 0.8071 - val_accuracy: 0.7130\n",
      "Epoch 275/1500\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.79678 to 0.78757, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.6987 - accuracy: 0.7302 - val_loss: 0.7876 - val_accuracy: 0.6979\n",
      "Epoch 276/1500\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.78757\n",
      "39/38 - 13s - loss: 0.7700 - accuracy: 0.7172 - val_loss: 0.8078 - val_accuracy: 0.7009\n",
      "Epoch 277/1500\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.78757\n",
      "39/38 - 13s - loss: 0.7183 - accuracy: 0.7343 - val_loss: 0.8379 - val_accuracy: 0.6918\n",
      "Epoch 278/1500\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.78757\n",
      "39/38 - 13s - loss: 0.7668 - accuracy: 0.7025 - val_loss: 0.9093 - val_accuracy: 0.6888\n",
      "Epoch 279/1500\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.78757\n",
      "39/38 - 13s - loss: 0.7305 - accuracy: 0.7270 - val_loss: 0.8763 - val_accuracy: 0.6798\n",
      "Epoch 280/1500\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.78757\n",
      "39/38 - 14s - loss: 0.7327 - accuracy: 0.7245 - val_loss: 0.8290 - val_accuracy: 0.6918\n",
      "Epoch 281/1500\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.78757\n",
      "39/38 - 14s - loss: 0.7399 - accuracy: 0.7042 - val_loss: 0.7979 - val_accuracy: 0.6858\n",
      "Epoch 282/1500\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.78757\n",
      "39/38 - 13s - loss: 0.7348 - accuracy: 0.7205 - val_loss: 0.8507 - val_accuracy: 0.6979\n",
      "Epoch 283/1500\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.78757\n",
      "39/38 - 13s - loss: 0.7104 - accuracy: 0.7384 - val_loss: 0.7989 - val_accuracy: 0.7039\n",
      "Epoch 284/1500\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.78757\n",
      "39/38 - 13s - loss: 0.7174 - accuracy: 0.7302 - val_loss: 0.8017 - val_accuracy: 0.7009\n",
      "Epoch 285/1500\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.78757\n",
      "39/38 - 14s - loss: 0.7315 - accuracy: 0.7262 - val_loss: 0.8147 - val_accuracy: 0.6979\n",
      "Epoch 286/1500\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.78757\n",
      "39/38 - 13s - loss: 0.6902 - accuracy: 0.7498 - val_loss: 0.8263 - val_accuracy: 0.7039\n",
      "Epoch 287/1500\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.78757\n",
      "39/38 - 13s - loss: 0.7203 - accuracy: 0.7221 - val_loss: 0.7925 - val_accuracy: 0.7009\n",
      "Epoch 288/1500\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.78757 to 0.76475, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.7006 - accuracy: 0.7522 - val_loss: 0.7648 - val_accuracy: 0.7039\n",
      "Epoch 289/1500\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.76475\n",
      "39/38 - 13s - loss: 0.7277 - accuracy: 0.7188 - val_loss: 0.7687 - val_accuracy: 0.7100\n",
      "Epoch 290/1500\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.76475\n",
      "39/38 - 13s - loss: 0.6859 - accuracy: 0.7441 - val_loss: 0.7767 - val_accuracy: 0.6949\n",
      "Epoch 291/1500\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.76475\n",
      "39/38 - 13s - loss: 0.7243 - accuracy: 0.7229 - val_loss: 0.7818 - val_accuracy: 0.7100\n",
      "Epoch 292/1500\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.76475\n",
      "39/38 - 13s - loss: 0.6864 - accuracy: 0.7425 - val_loss: 0.7923 - val_accuracy: 0.7009\n",
      "Epoch 293/1500\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.76475\n",
      "39/38 - 13s - loss: 0.7176 - accuracy: 0.7319 - val_loss: 0.8259 - val_accuracy: 0.7190\n",
      "Epoch 294/1500\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.76475\n",
      "39/38 - 13s - loss: 0.6706 - accuracy: 0.7637 - val_loss: 0.8223 - val_accuracy: 0.6888\n",
      "Epoch 295/1500\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.76475\n",
      "39/38 - 13s - loss: 0.6925 - accuracy: 0.7294 - val_loss: 0.9116 - val_accuracy: 0.6798\n",
      "Epoch 296/1500\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.76475\n",
      "39/38 - 13s - loss: 0.7260 - accuracy: 0.7351 - val_loss: 0.8152 - val_accuracy: 0.6979\n",
      "Epoch 297/1500\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.76475 to 0.76153, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.7422 - accuracy: 0.7245 - val_loss: 0.7615 - val_accuracy: 0.7069\n",
      "Epoch 298/1500\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.76153\n",
      "39/38 - 14s - loss: 0.7259 - accuracy: 0.7205 - val_loss: 0.8185 - val_accuracy: 0.7160\n",
      "Epoch 299/1500\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.76153\n",
      "39/38 - 14s - loss: 0.7398 - accuracy: 0.7180 - val_loss: 0.8344 - val_accuracy: 0.7160\n",
      "Epoch 300/1500\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.76153\n",
      "39/38 - 13s - loss: 0.7380 - accuracy: 0.7253 - val_loss: 0.8748 - val_accuracy: 0.7100\n",
      "Epoch 301/1500\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.76153\n",
      "39/38 - 14s - loss: 0.6563 - accuracy: 0.7425 - val_loss: 0.9137 - val_accuracy: 0.6828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 302/1500\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.76153 to 0.76064, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.7137 - accuracy: 0.7311 - val_loss: 0.7606 - val_accuracy: 0.7130\n",
      "Epoch 303/1500\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.76064 to 0.73584, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.7322 - accuracy: 0.7311 - val_loss: 0.7358 - val_accuracy: 0.7130\n",
      "Epoch 304/1500\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.73584 to 0.72954, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.6787 - accuracy: 0.7319 - val_loss: 0.7295 - val_accuracy: 0.7160\n",
      "Epoch 305/1500\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.72954\n",
      "39/38 - 13s - loss: 0.7407 - accuracy: 0.7359 - val_loss: 0.7710 - val_accuracy: 0.7402\n",
      "Epoch 306/1500\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.72954\n",
      "39/38 - 13s - loss: 0.6789 - accuracy: 0.7612 - val_loss: 0.7817 - val_accuracy: 0.7160\n",
      "Epoch 307/1500\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.72954\n",
      "39/38 - 14s - loss: 0.7267 - accuracy: 0.7237 - val_loss: 0.7667 - val_accuracy: 0.7281\n",
      "Epoch 308/1500\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.72954\n",
      "39/38 - 14s - loss: 0.7005 - accuracy: 0.7351 - val_loss: 0.8409 - val_accuracy: 0.7130\n",
      "Epoch 309/1500\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.72954\n",
      "39/38 - 13s - loss: 0.6791 - accuracy: 0.7441 - val_loss: 0.8722 - val_accuracy: 0.7069\n",
      "Epoch 310/1500\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.72954\n",
      "39/38 - 13s - loss: 0.7311 - accuracy: 0.7302 - val_loss: 0.7960 - val_accuracy: 0.7160\n",
      "Epoch 311/1500\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.72954\n",
      "39/38 - 14s - loss: 0.6949 - accuracy: 0.7327 - val_loss: 0.8330 - val_accuracy: 0.7009\n",
      "Epoch 312/1500\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.72954\n",
      "39/38 - 13s - loss: 0.7341 - accuracy: 0.7286 - val_loss: 0.7739 - val_accuracy: 0.7009\n",
      "Epoch 313/1500\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.72954\n",
      "39/38 - 14s - loss: 0.6834 - accuracy: 0.7196 - val_loss: 0.7544 - val_accuracy: 0.7190\n",
      "Epoch 314/1500\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.72954\n",
      "39/38 - 14s - loss: 0.6971 - accuracy: 0.7514 - val_loss: 0.7460 - val_accuracy: 0.7160\n",
      "Epoch 315/1500\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.72954\n",
      "39/38 - 14s - loss: 0.6567 - accuracy: 0.7522 - val_loss: 0.7383 - val_accuracy: 0.7251\n",
      "Epoch 316/1500\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.72954\n",
      "39/38 - 13s - loss: 0.6849 - accuracy: 0.7612 - val_loss: 0.7345 - val_accuracy: 0.7221\n",
      "Epoch 317/1500\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.72954 to 0.71973, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.6522 - accuracy: 0.7555 - val_loss: 0.7197 - val_accuracy: 0.7432\n",
      "Epoch 318/1500\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.71973\n",
      "39/38 - 13s - loss: 0.7151 - accuracy: 0.7327 - val_loss: 0.7661 - val_accuracy: 0.7100\n",
      "Epoch 319/1500\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.71973\n",
      "39/38 - 13s - loss: 0.7440 - accuracy: 0.7188 - val_loss: 0.7266 - val_accuracy: 0.7341\n",
      "Epoch 320/1500\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.71973 to 0.70946, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.6860 - accuracy: 0.7311 - val_loss: 0.7095 - val_accuracy: 0.7432\n",
      "Epoch 321/1500\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.6676 - accuracy: 0.7514 - val_loss: 0.7458 - val_accuracy: 0.7281\n",
      "Epoch 322/1500\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.6619 - accuracy: 0.7474 - val_loss: 0.7759 - val_accuracy: 0.7311\n",
      "Epoch 323/1500\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.6432 - accuracy: 0.7490 - val_loss: 0.7417 - val_accuracy: 0.7462\n",
      "Epoch 324/1500\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.7319 - accuracy: 0.7221 - val_loss: 0.7609 - val_accuracy: 0.7251\n",
      "Epoch 325/1500\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.6965 - accuracy: 0.7400 - val_loss: 0.7812 - val_accuracy: 0.7039\n",
      "Epoch 326/1500\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.6624 - accuracy: 0.7302 - val_loss: 0.7722 - val_accuracy: 0.7190\n",
      "Epoch 327/1500\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.7110 - accuracy: 0.7359 - val_loss: 0.7929 - val_accuracy: 0.7311\n",
      "Epoch 328/1500\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.6675 - accuracy: 0.7571 - val_loss: 0.7535 - val_accuracy: 0.7341\n",
      "Epoch 329/1500\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.6806 - accuracy: 0.7392 - val_loss: 0.7906 - val_accuracy: 0.7281\n",
      "Epoch 330/1500\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.70946\n",
      "39/38 - 14s - loss: 0.6457 - accuracy: 0.7490 - val_loss: 0.8198 - val_accuracy: 0.7160\n",
      "Epoch 331/1500\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.70946\n",
      "39/38 - 14s - loss: 0.6986 - accuracy: 0.7376 - val_loss: 0.7714 - val_accuracy: 0.7251\n",
      "Epoch 332/1500\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.70946\n",
      "39/38 - 14s - loss: 0.6023 - accuracy: 0.7783 - val_loss: 0.7368 - val_accuracy: 0.7462\n",
      "Epoch 333/1500\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.6303 - accuracy: 0.7596 - val_loss: 0.7817 - val_accuracy: 0.7190\n",
      "Epoch 334/1500\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.5890 - accuracy: 0.7734 - val_loss: 0.7128 - val_accuracy: 0.7432\n",
      "Epoch 335/1500\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.70946\n",
      "39/38 - 14s - loss: 0.6548 - accuracy: 0.7490 - val_loss: 0.7778 - val_accuracy: 0.7190\n",
      "Epoch 336/1500\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.70946\n",
      "39/38 - 13s - loss: 0.6325 - accuracy: 0.7620 - val_loss: 0.7125 - val_accuracy: 0.7372\n",
      "Epoch 337/1500\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.70946 to 0.68847, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.6873 - accuracy: 0.7408 - val_loss: 0.6885 - val_accuracy: 0.7583\n",
      "Epoch 338/1500\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.68847\n",
      "39/38 - 13s - loss: 0.6407 - accuracy: 0.7514 - val_loss: 0.7179 - val_accuracy: 0.7402\n",
      "Epoch 339/1500\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.68847\n",
      "39/38 - 13s - loss: 0.6391 - accuracy: 0.7498 - val_loss: 0.6972 - val_accuracy: 0.7492\n",
      "Epoch 340/1500\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.68847\n",
      "39/38 - 13s - loss: 0.6576 - accuracy: 0.7547 - val_loss: 0.7234 - val_accuracy: 0.7523\n",
      "Epoch 341/1500\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.68847\n",
      "39/38 - 13s - loss: 0.6497 - accuracy: 0.7498 - val_loss: 0.7568 - val_accuracy: 0.7281\n",
      "Epoch 342/1500\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.68847\n",
      "39/38 - 13s - loss: 0.6889 - accuracy: 0.7376 - val_loss: 0.7046 - val_accuracy: 0.7432\n",
      "Epoch 343/1500\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.68847\n",
      "39/38 - 13s - loss: 0.6167 - accuracy: 0.7694 - val_loss: 0.6908 - val_accuracy: 0.7613\n",
      "Epoch 344/1500\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.68847\n",
      "39/38 - 13s - loss: 0.6248 - accuracy: 0.7588 - val_loss: 0.7505 - val_accuracy: 0.7341\n",
      "Epoch 345/1500\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.68847\n",
      "39/38 - 13s - loss: 0.6379 - accuracy: 0.7547 - val_loss: 0.6942 - val_accuracy: 0.7462\n",
      "Epoch 346/1500\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.68847\n",
      "39/38 - 14s - loss: 0.6605 - accuracy: 0.7482 - val_loss: 0.7187 - val_accuracy: 0.7372\n",
      "Epoch 347/1500\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.68847 to 0.68655, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.6290 - accuracy: 0.7628 - val_loss: 0.6865 - val_accuracy: 0.7613\n",
      "Epoch 348/1500\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.68655\n",
      "39/38 - 14s - loss: 0.6439 - accuracy: 0.7441 - val_loss: 0.7702 - val_accuracy: 0.7311\n",
      "Epoch 349/1500\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.68655\n",
      "39/38 - 13s - loss: 0.6592 - accuracy: 0.7522 - val_loss: 0.7162 - val_accuracy: 0.7553\n",
      "Epoch 350/1500\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.68655\n",
      "39/38 - 14s - loss: 0.6588 - accuracy: 0.7588 - val_loss: 0.7007 - val_accuracy: 0.7553\n",
      "Epoch 351/1500\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.68655\n",
      "39/38 - 13s - loss: 0.6108 - accuracy: 0.7759 - val_loss: 0.7257 - val_accuracy: 0.7402\n",
      "Epoch 352/1500\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.68655 to 0.65767, saving model to piece_model1.weights.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/38 - 14s - loss: 0.6166 - accuracy: 0.7571 - val_loss: 0.6577 - val_accuracy: 0.7674\n",
      "Epoch 353/1500\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6381 - accuracy: 0.7620 - val_loss: 0.6699 - val_accuracy: 0.7583\n",
      "Epoch 354/1500\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.65767\n",
      "39/38 - 14s - loss: 0.7068 - accuracy: 0.7449 - val_loss: 0.6912 - val_accuracy: 0.7372\n",
      "Epoch 355/1500\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6167 - accuracy: 0.7669 - val_loss: 0.7164 - val_accuracy: 0.7613\n",
      "Epoch 356/1500\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6694 - accuracy: 0.7416 - val_loss: 0.7307 - val_accuracy: 0.7583\n",
      "Epoch 357/1500\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6120 - accuracy: 0.7685 - val_loss: 0.7262 - val_accuracy: 0.7462\n",
      "Epoch 358/1500\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.65767\n",
      "39/38 - 14s - loss: 0.6182 - accuracy: 0.7759 - val_loss: 0.7484 - val_accuracy: 0.7613\n",
      "Epoch 359/1500\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6223 - accuracy: 0.7579 - val_loss: 0.7488 - val_accuracy: 0.7402\n",
      "Epoch 360/1500\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6369 - accuracy: 0.7555 - val_loss: 0.7086 - val_accuracy: 0.7492\n",
      "Epoch 361/1500\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6853 - accuracy: 0.7547 - val_loss: 0.6871 - val_accuracy: 0.7734\n",
      "Epoch 362/1500\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6052 - accuracy: 0.7702 - val_loss: 0.7024 - val_accuracy: 0.7432\n",
      "Epoch 363/1500\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6061 - accuracy: 0.7767 - val_loss: 0.7217 - val_accuracy: 0.7492\n",
      "Epoch 364/1500\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6049 - accuracy: 0.7726 - val_loss: 0.7170 - val_accuracy: 0.7311\n",
      "Epoch 365/1500\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6195 - accuracy: 0.7694 - val_loss: 0.6850 - val_accuracy: 0.7613\n",
      "Epoch 366/1500\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6151 - accuracy: 0.7824 - val_loss: 0.7075 - val_accuracy: 0.7553\n",
      "Epoch 367/1500\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.5928 - accuracy: 0.7808 - val_loss: 0.7917 - val_accuracy: 0.7130\n",
      "Epoch 368/1500\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6163 - accuracy: 0.7718 - val_loss: 0.6903 - val_accuracy: 0.7734\n",
      "Epoch 369/1500\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6358 - accuracy: 0.7555 - val_loss: 0.6805 - val_accuracy: 0.7583\n",
      "Epoch 370/1500\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6294 - accuracy: 0.7604 - val_loss: 0.7006 - val_accuracy: 0.7523\n",
      "Epoch 371/1500\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.65767\n",
      "39/38 - 14s - loss: 0.6189 - accuracy: 0.7653 - val_loss: 0.6711 - val_accuracy: 0.7553\n",
      "Epoch 372/1500\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6273 - accuracy: 0.7612 - val_loss: 0.6877 - val_accuracy: 0.7462\n",
      "Epoch 373/1500\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.5995 - accuracy: 0.7800 - val_loss: 0.6910 - val_accuracy: 0.7523\n",
      "Epoch 374/1500\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6111 - accuracy: 0.7612 - val_loss: 0.6669 - val_accuracy: 0.7492\n",
      "Epoch 375/1500\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6473 - accuracy: 0.7628 - val_loss: 0.7738 - val_accuracy: 0.7221\n",
      "Epoch 376/1500\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.5960 - accuracy: 0.7742 - val_loss: 0.6986 - val_accuracy: 0.7432\n",
      "Epoch 377/1500\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.65767\n",
      "39/38 - 14s - loss: 0.6594 - accuracy: 0.7392 - val_loss: 0.8222 - val_accuracy: 0.7069\n",
      "Epoch 378/1500\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.65767\n",
      "39/38 - 14s - loss: 0.6333 - accuracy: 0.7685 - val_loss: 0.7133 - val_accuracy: 0.7523\n",
      "Epoch 379/1500\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.65767\n",
      "39/38 - 14s - loss: 0.5994 - accuracy: 0.7824 - val_loss: 0.6771 - val_accuracy: 0.7734\n",
      "Epoch 380/1500\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.5941 - accuracy: 0.7718 - val_loss: 0.6830 - val_accuracy: 0.7583\n",
      "Epoch 381/1500\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.65767\n",
      "39/38 - 13s - loss: 0.6073 - accuracy: 0.7596 - val_loss: 0.6580 - val_accuracy: 0.7613\n",
      "Epoch 382/1500\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.65767 to 0.64570, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.6066 - accuracy: 0.7685 - val_loss: 0.6457 - val_accuracy: 0.7613\n",
      "Epoch 383/1500\n",
      "\n",
      "Epoch 00383: val_loss improved from 0.64570 to 0.62777, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.6027 - accuracy: 0.7669 - val_loss: 0.6278 - val_accuracy: 0.7674\n",
      "Epoch 384/1500\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.62777\n",
      "39/38 - 13s - loss: 0.5865 - accuracy: 0.7751 - val_loss: 0.6719 - val_accuracy: 0.7825\n",
      "Epoch 385/1500\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.62777\n",
      "39/38 - 13s - loss: 0.6065 - accuracy: 0.7751 - val_loss: 0.6730 - val_accuracy: 0.7583\n",
      "Epoch 386/1500\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.62777\n",
      "39/38 - 13s - loss: 0.5851 - accuracy: 0.7848 - val_loss: 0.6410 - val_accuracy: 0.7795\n",
      "Epoch 387/1500\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.62777\n",
      "39/38 - 14s - loss: 0.6106 - accuracy: 0.7628 - val_loss: 0.6400 - val_accuracy: 0.7825\n",
      "Epoch 388/1500\n",
      "\n",
      "Epoch 00388: val_loss improved from 0.62777 to 0.62376, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.6101 - accuracy: 0.7596 - val_loss: 0.6238 - val_accuracy: 0.7855\n",
      "Epoch 389/1500\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.5810 - accuracy: 0.7783 - val_loss: 0.6521 - val_accuracy: 0.7734\n",
      "Epoch 390/1500\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.6023 - accuracy: 0.7775 - val_loss: 0.6439 - val_accuracy: 0.7825\n",
      "Epoch 391/1500\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.62376\n",
      "39/38 - 14s - loss: 0.6370 - accuracy: 0.7767 - val_loss: 0.6509 - val_accuracy: 0.7704\n",
      "Epoch 392/1500\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.5753 - accuracy: 0.7954 - val_loss: 0.6463 - val_accuracy: 0.7704\n",
      "Epoch 393/1500\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.5924 - accuracy: 0.7791 - val_loss: 0.6780 - val_accuracy: 0.7553\n",
      "Epoch 394/1500\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.6579 - accuracy: 0.7531 - val_loss: 0.6588 - val_accuracy: 0.7613\n",
      "Epoch 395/1500\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.5722 - accuracy: 0.7848 - val_loss: 0.6788 - val_accuracy: 0.7492\n",
      "Epoch 396/1500\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.6056 - accuracy: 0.7816 - val_loss: 0.6609 - val_accuracy: 0.7644\n",
      "Epoch 397/1500\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.5805 - accuracy: 0.7767 - val_loss: 0.6651 - val_accuracy: 0.7583\n",
      "Epoch 398/1500\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.5708 - accuracy: 0.7742 - val_loss: 0.6621 - val_accuracy: 0.7734\n",
      "Epoch 399/1500\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.6206 - accuracy: 0.7490 - val_loss: 0.7251 - val_accuracy: 0.7432\n",
      "Epoch 400/1500\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.62376\n",
      "39/38 - 13s - loss: 0.5847 - accuracy: 0.7791 - val_loss: 0.7289 - val_accuracy: 0.7492\n",
      "Epoch 401/1500\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.62376\n",
      "39/38 - 14s - loss: 0.5786 - accuracy: 0.7954 - val_loss: 0.6787 - val_accuracy: 0.7825\n",
      "Epoch 402/1500\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.62376\n",
      "39/38 - 14s - loss: 0.5976 - accuracy: 0.7800 - val_loss: 0.6597 - val_accuracy: 0.7734\n",
      "Epoch 403/1500\n",
      "\n",
      "Epoch 00403: val_loss improved from 0.62376 to 0.61106, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.6062 - accuracy: 0.7588 - val_loss: 0.6111 - val_accuracy: 0.7885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 404/1500\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.61106\n",
      "39/38 - 13s - loss: 0.5596 - accuracy: 0.7881 - val_loss: 0.6701 - val_accuracy: 0.7583\n",
      "Epoch 405/1500\n",
      "\n",
      "Epoch 00405: val_loss improved from 0.61106 to 0.60826, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.5794 - accuracy: 0.7759 - val_loss: 0.6083 - val_accuracy: 0.7915\n",
      "Epoch 406/1500\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.60826\n",
      "39/38 - 13s - loss: 0.5818 - accuracy: 0.7832 - val_loss: 0.6091 - val_accuracy: 0.7734\n",
      "Epoch 407/1500\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.60826\n",
      "39/38 - 13s - loss: 0.5957 - accuracy: 0.7832 - val_loss: 0.6214 - val_accuracy: 0.7915\n",
      "Epoch 408/1500\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.60826\n",
      "39/38 - 14s - loss: 0.5411 - accuracy: 0.7922 - val_loss: 0.6238 - val_accuracy: 0.7855\n",
      "Epoch 409/1500\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.60826\n",
      "39/38 - 13s - loss: 0.6142 - accuracy: 0.7620 - val_loss: 0.6354 - val_accuracy: 0.7734\n",
      "Epoch 410/1500\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.60826\n",
      "39/38 - 14s - loss: 0.5876 - accuracy: 0.7702 - val_loss: 0.6463 - val_accuracy: 0.7795\n",
      "Epoch 411/1500\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.60826\n",
      "39/38 - 13s - loss: 0.5703 - accuracy: 0.7783 - val_loss: 0.6366 - val_accuracy: 0.7795\n",
      "Epoch 412/1500\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.60826\n",
      "39/38 - 14s - loss: 0.5507 - accuracy: 0.7971 - val_loss: 0.6474 - val_accuracy: 0.7795\n",
      "Epoch 413/1500\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.60826\n",
      "39/38 - 13s - loss: 0.5886 - accuracy: 0.7767 - val_loss: 0.6235 - val_accuracy: 0.7764\n",
      "Epoch 414/1500\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.60826\n",
      "39/38 - 13s - loss: 0.5959 - accuracy: 0.7808 - val_loss: 0.6340 - val_accuracy: 0.7825\n",
      "Epoch 415/1500\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.60826\n",
      "39/38 - 13s - loss: 0.5910 - accuracy: 0.7791 - val_loss: 0.6238 - val_accuracy: 0.7825\n",
      "Epoch 416/1500\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.60826\n",
      "39/38 - 14s - loss: 0.5835 - accuracy: 0.7840 - val_loss: 0.6176 - val_accuracy: 0.7885\n",
      "Epoch 417/1500\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.60826\n",
      "39/38 - 13s - loss: 0.5769 - accuracy: 0.7857 - val_loss: 0.6198 - val_accuracy: 0.7795\n",
      "Epoch 418/1500\n",
      "\n",
      "Epoch 00418: val_loss improved from 0.60826 to 0.60328, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.5799 - accuracy: 0.7726 - val_loss: 0.6033 - val_accuracy: 0.7764\n",
      "Epoch 419/1500\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.60328\n",
      "39/38 - 13s - loss: 0.5611 - accuracy: 0.7914 - val_loss: 0.8628 - val_accuracy: 0.6949\n",
      "Epoch 420/1500\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.60328\n",
      "39/38 - 13s - loss: 0.5666 - accuracy: 0.7865 - val_loss: 0.6774 - val_accuracy: 0.7674\n",
      "Epoch 421/1500\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.60328\n",
      "39/38 - 13s - loss: 0.5598 - accuracy: 0.7995 - val_loss: 0.6199 - val_accuracy: 0.7855\n",
      "Epoch 422/1500\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.60328\n",
      "39/38 - 13s - loss: 0.5514 - accuracy: 0.7987 - val_loss: 0.6355 - val_accuracy: 0.7523\n",
      "Epoch 423/1500\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.60328\n",
      "39/38 - 14s - loss: 0.5243 - accuracy: 0.8028 - val_loss: 0.6165 - val_accuracy: 0.7764\n",
      "Epoch 424/1500\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.60328\n",
      "39/38 - 14s - loss: 0.5513 - accuracy: 0.7971 - val_loss: 0.6273 - val_accuracy: 0.7795\n",
      "Epoch 425/1500\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.60328\n",
      "39/38 - 13s - loss: 0.5839 - accuracy: 0.7848 - val_loss: 0.6944 - val_accuracy: 0.7674\n",
      "Epoch 426/1500\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.60328\n",
      "39/38 - 14s - loss: 0.5829 - accuracy: 0.7824 - val_loss: 0.6585 - val_accuracy: 0.7583\n",
      "Epoch 427/1500\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.60328\n",
      "39/38 - 13s - loss: 0.5718 - accuracy: 0.7677 - val_loss: 0.6643 - val_accuracy: 0.7644\n",
      "Epoch 428/1500\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.60328\n",
      "39/38 - 14s - loss: 0.5425 - accuracy: 0.7930 - val_loss: 0.6102 - val_accuracy: 0.7704\n",
      "Epoch 429/1500\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.60328\n",
      "39/38 - 13s - loss: 0.5592 - accuracy: 0.7922 - val_loss: 0.6100 - val_accuracy: 0.7885\n",
      "Epoch 430/1500\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.60328\n",
      "39/38 - 14s - loss: 0.5446 - accuracy: 0.8068 - val_loss: 0.7164 - val_accuracy: 0.7402\n",
      "Epoch 431/1500\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.60328\n",
      "39/38 - 14s - loss: 0.5327 - accuracy: 0.8011 - val_loss: 0.6250 - val_accuracy: 0.7885\n",
      "Epoch 432/1500\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.60328\n",
      "39/38 - 14s - loss: 0.5670 - accuracy: 0.7873 - val_loss: 0.6639 - val_accuracy: 0.7674\n",
      "Epoch 433/1500\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.60328\n",
      "39/38 - 14s - loss: 0.5488 - accuracy: 0.7971 - val_loss: 0.6576 - val_accuracy: 0.7644\n",
      "Epoch 434/1500\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.60328\n",
      "39/38 - 14s - loss: 0.5478 - accuracy: 0.7946 - val_loss: 0.6316 - val_accuracy: 0.7764\n",
      "Epoch 435/1500\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.60328 to 0.59978, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.5324 - accuracy: 0.8036 - val_loss: 0.5998 - val_accuracy: 0.7825\n",
      "Epoch 436/1500\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.59978\n",
      "39/38 - 13s - loss: 0.5618 - accuracy: 0.7979 - val_loss: 0.6105 - val_accuracy: 0.7795\n",
      "Epoch 437/1500\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.59978\n",
      "39/38 - 14s - loss: 0.5487 - accuracy: 0.7857 - val_loss: 0.6192 - val_accuracy: 0.7976\n",
      "Epoch 438/1500\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.59978\n",
      "39/38 - 13s - loss: 0.5485 - accuracy: 0.7905 - val_loss: 0.6475 - val_accuracy: 0.7915\n",
      "Epoch 439/1500\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.59978\n",
      "39/38 - 13s - loss: 0.5652 - accuracy: 0.7938 - val_loss: 0.6208 - val_accuracy: 0.8036\n",
      "Epoch 440/1500\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.59978\n",
      "39/38 - 13s - loss: 0.5414 - accuracy: 0.7979 - val_loss: 0.6391 - val_accuracy: 0.8006\n",
      "Epoch 441/1500\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.59978\n",
      "39/38 - 13s - loss: 0.5525 - accuracy: 0.7873 - val_loss: 0.6255 - val_accuracy: 0.8006\n",
      "Epoch 442/1500\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.59978\n",
      "39/38 - 13s - loss: 0.5079 - accuracy: 0.8060 - val_loss: 0.6648 - val_accuracy: 0.7764\n",
      "Epoch 443/1500\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.59978\n",
      "39/38 - 14s - loss: 0.5772 - accuracy: 0.7914 - val_loss: 0.6072 - val_accuracy: 0.8066\n",
      "Epoch 444/1500\n",
      "\n",
      "Epoch 00444: val_loss improved from 0.59978 to 0.57482, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.5698 - accuracy: 0.8003 - val_loss: 0.5748 - val_accuracy: 0.8097\n",
      "Epoch 445/1500\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.57482\n",
      "39/38 - 14s - loss: 0.5164 - accuracy: 0.8109 - val_loss: 0.6366 - val_accuracy: 0.7734\n",
      "Epoch 446/1500\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.57482\n",
      "39/38 - 14s - loss: 0.5322 - accuracy: 0.7963 - val_loss: 0.5955 - val_accuracy: 0.8127\n",
      "Epoch 447/1500\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.57482\n",
      "39/38 - 13s - loss: 0.5277 - accuracy: 0.7922 - val_loss: 0.6072 - val_accuracy: 0.8006\n",
      "Epoch 448/1500\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.57482\n",
      "39/38 - 13s - loss: 0.5320 - accuracy: 0.8142 - val_loss: 0.6792 - val_accuracy: 0.7704\n",
      "Epoch 449/1500\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.57482\n",
      "39/38 - 14s - loss: 0.6124 - accuracy: 0.7759 - val_loss: 0.6602 - val_accuracy: 0.7704\n",
      "Epoch 450/1500\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.57482\n",
      "39/38 - 14s - loss: 0.5123 - accuracy: 0.8020 - val_loss: 0.6030 - val_accuracy: 0.7855\n",
      "Epoch 451/1500\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.57482\n",
      "39/38 - 13s - loss: 0.5324 - accuracy: 0.8060 - val_loss: 0.6102 - val_accuracy: 0.7734\n",
      "Epoch 452/1500\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.57482\n",
      "39/38 - 14s - loss: 0.5425 - accuracy: 0.7938 - val_loss: 0.6093 - val_accuracy: 0.7795\n",
      "Epoch 453/1500\n",
      "\n",
      "Epoch 00453: val_loss improved from 0.57482 to 0.55786, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.5255 - accuracy: 0.7995 - val_loss: 0.5579 - val_accuracy: 0.8036\n",
      "Epoch 454/1500\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.55786\n",
      "39/38 - 13s - loss: 0.5708 - accuracy: 0.7930 - val_loss: 0.5838 - val_accuracy: 0.8097\n",
      "Epoch 455/1500\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.55786\n",
      "39/38 - 13s - loss: 0.5527 - accuracy: 0.8011 - val_loss: 0.5627 - val_accuracy: 0.8006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/1500\n",
      "\n",
      "Epoch 00456: val_loss improved from 0.55786 to 0.55300, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.5535 - accuracy: 0.7881 - val_loss: 0.5530 - val_accuracy: 0.7946\n",
      "Epoch 457/1500\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.55300\n",
      "39/38 - 13s - loss: 0.5084 - accuracy: 0.8011 - val_loss: 0.5667 - val_accuracy: 0.7976\n",
      "Epoch 458/1500\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.55300\n",
      "39/38 - 14s - loss: 0.5196 - accuracy: 0.8109 - val_loss: 0.5708 - val_accuracy: 0.7885\n",
      "Epoch 459/1500\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.55300\n",
      "39/38 - 13s - loss: 0.5293 - accuracy: 0.8077 - val_loss: 0.6127 - val_accuracy: 0.7704\n",
      "Epoch 460/1500\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.55300\n",
      "39/38 - 13s - loss: 0.4996 - accuracy: 0.8093 - val_loss: 0.5849 - val_accuracy: 0.8036\n",
      "Epoch 461/1500\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.55300\n",
      "39/38 - 13s - loss: 0.5386 - accuracy: 0.7865 - val_loss: 0.5602 - val_accuracy: 0.7976\n",
      "Epoch 462/1500\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.55300\n",
      "39/38 - 13s - loss: 0.5785 - accuracy: 0.7897 - val_loss: 0.6041 - val_accuracy: 0.8066\n",
      "Epoch 463/1500\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.55300\n",
      "39/38 - 13s - loss: 0.5184 - accuracy: 0.8044 - val_loss: 0.6228 - val_accuracy: 0.7855\n",
      "Epoch 464/1500\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.55300\n",
      "39/38 - 13s - loss: 0.5371 - accuracy: 0.7963 - val_loss: 0.6083 - val_accuracy: 0.7855\n",
      "Epoch 465/1500\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.55300\n",
      "39/38 - 14s - loss: 0.5416 - accuracy: 0.8158 - val_loss: 0.5772 - val_accuracy: 0.8097\n",
      "Epoch 466/1500\n",
      "\n",
      "Epoch 00466: val_loss improved from 0.55300 to 0.55140, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.5268 - accuracy: 0.8060 - val_loss: 0.5514 - val_accuracy: 0.8097\n",
      "Epoch 467/1500\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.55140\n",
      "39/38 - 13s - loss: 0.5098 - accuracy: 0.8060 - val_loss: 0.5683 - val_accuracy: 0.7795\n",
      "Epoch 468/1500\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.55140\n",
      "39/38 - 13s - loss: 0.5301 - accuracy: 0.8044 - val_loss: 0.5956 - val_accuracy: 0.8066\n",
      "Epoch 469/1500\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.55140\n",
      "39/38 - 13s - loss: 0.5266 - accuracy: 0.8126 - val_loss: 0.5575 - val_accuracy: 0.8157\n",
      "Epoch 470/1500\n",
      "\n",
      "Epoch 00470: val_loss improved from 0.55140 to 0.55127, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4991 - accuracy: 0.8077 - val_loss: 0.5513 - val_accuracy: 0.8006\n",
      "Epoch 471/1500\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.55127\n",
      "39/38 - 13s - loss: 0.5432 - accuracy: 0.8011 - val_loss: 0.5595 - val_accuracy: 0.8157\n",
      "Epoch 472/1500\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.55127\n",
      "39/38 - 13s - loss: 0.4795 - accuracy: 0.8207 - val_loss: 0.5654 - val_accuracy: 0.8218\n",
      "Epoch 473/1500\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.55127\n",
      "39/38 - 14s - loss: 0.5397 - accuracy: 0.8101 - val_loss: 0.5689 - val_accuracy: 0.8066\n",
      "Epoch 474/1500\n",
      "\n",
      "Epoch 00474: val_loss improved from 0.55127 to 0.55123, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.5460 - accuracy: 0.7938 - val_loss: 0.5512 - val_accuracy: 0.8006\n",
      "Epoch 475/1500\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.55123\n",
      "39/38 - 13s - loss: 0.5172 - accuracy: 0.8077 - val_loss: 0.5806 - val_accuracy: 0.8066\n",
      "Epoch 476/1500\n",
      "\n",
      "Epoch 00476: val_loss improved from 0.55123 to 0.54134, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 13s - loss: 0.5506 - accuracy: 0.7979 - val_loss: 0.5413 - val_accuracy: 0.8187\n",
      "Epoch 477/1500\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5361 - accuracy: 0.8028 - val_loss: 0.5574 - val_accuracy: 0.8066\n",
      "Epoch 478/1500\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5089 - accuracy: 0.8036 - val_loss: 0.5652 - val_accuracy: 0.8157\n",
      "Epoch 479/1500\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5501 - accuracy: 0.7897 - val_loss: 0.6281 - val_accuracy: 0.8006\n",
      "Epoch 480/1500\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5392 - accuracy: 0.7881 - val_loss: 0.5546 - val_accuracy: 0.8036\n",
      "Epoch 481/1500\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5068 - accuracy: 0.8117 - val_loss: 0.5837 - val_accuracy: 0.7976\n",
      "Epoch 482/1500\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5177 - accuracy: 0.8207 - val_loss: 0.5688 - val_accuracy: 0.8127\n",
      "Epoch 483/1500\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5388 - accuracy: 0.7954 - val_loss: 0.5760 - val_accuracy: 0.7976\n",
      "Epoch 484/1500\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.54134\n",
      "39/38 - 14s - loss: 0.5256 - accuracy: 0.8020 - val_loss: 0.6325 - val_accuracy: 0.7795\n",
      "Epoch 485/1500\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5155 - accuracy: 0.8060 - val_loss: 0.5965 - val_accuracy: 0.7825\n",
      "Epoch 486/1500\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.54134\n",
      "39/38 - 14s - loss: 0.5266 - accuracy: 0.8166 - val_loss: 0.6485 - val_accuracy: 0.7795\n",
      "Epoch 487/1500\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5174 - accuracy: 0.8174 - val_loss: 0.5743 - val_accuracy: 0.7915\n",
      "Epoch 488/1500\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5644 - accuracy: 0.7963 - val_loss: 0.5612 - val_accuracy: 0.7946\n",
      "Epoch 489/1500\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.4726 - accuracy: 0.8272 - val_loss: 0.5915 - val_accuracy: 0.7946\n",
      "Epoch 490/1500\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5099 - accuracy: 0.8068 - val_loss: 0.6477 - val_accuracy: 0.7825\n",
      "Epoch 491/1500\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5487 - accuracy: 0.7930 - val_loss: 0.5658 - val_accuracy: 0.8036\n",
      "Epoch 492/1500\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.54134\n",
      "39/38 - 13s - loss: 0.5168 - accuracy: 0.8109 - val_loss: 0.5570 - val_accuracy: 0.8097\n",
      "Epoch 493/1500\n",
      "\n",
      "Epoch 00493: val_loss improved from 0.54134 to 0.53367, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4943 - accuracy: 0.8003 - val_loss: 0.5337 - val_accuracy: 0.8187\n",
      "Epoch 494/1500\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5910 - accuracy: 0.8044 - val_loss: 0.5415 - val_accuracy: 0.8187\n",
      "Epoch 495/1500\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5172 - accuracy: 0.8093 - val_loss: 0.5744 - val_accuracy: 0.8066\n",
      "Epoch 496/1500\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5508 - accuracy: 0.7816 - val_loss: 0.5734 - val_accuracy: 0.8006\n",
      "Epoch 497/1500\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.53367\n",
      "39/38 - 14s - loss: 0.4889 - accuracy: 0.8199 - val_loss: 0.5660 - val_accuracy: 0.8127\n",
      "Epoch 498/1500\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5142 - accuracy: 0.8134 - val_loss: 0.5758 - val_accuracy: 0.8066\n",
      "Epoch 499/1500\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5274 - accuracy: 0.8207 - val_loss: 0.5751 - val_accuracy: 0.7855\n",
      "Epoch 500/1500\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.4816 - accuracy: 0.8183 - val_loss: 0.5752 - val_accuracy: 0.8127\n",
      "Epoch 501/1500\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5428 - accuracy: 0.8158 - val_loss: 0.5686 - val_accuracy: 0.7946\n",
      "Epoch 502/1500\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.4830 - accuracy: 0.8231 - val_loss: 0.5588 - val_accuracy: 0.8097\n",
      "Epoch 503/1500\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.4900 - accuracy: 0.8191 - val_loss: 0.6463 - val_accuracy: 0.7885\n",
      "Epoch 504/1500\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5055 - accuracy: 0.8134 - val_loss: 0.5810 - val_accuracy: 0.8127\n",
      "Epoch 505/1500\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.53367\n",
      "39/38 - 14s - loss: 0.5170 - accuracy: 0.8101 - val_loss: 0.5567 - val_accuracy: 0.8218\n",
      "Epoch 506/1500\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.4674 - accuracy: 0.8231 - val_loss: 0.5475 - val_accuracy: 0.8097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 507/1500\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5047 - accuracy: 0.8011 - val_loss: 0.5719 - val_accuracy: 0.7946\n",
      "Epoch 508/1500\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.4917 - accuracy: 0.8174 - val_loss: 0.5629 - val_accuracy: 0.8097\n",
      "Epoch 509/1500\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5597 - accuracy: 0.8068 - val_loss: 0.5363 - val_accuracy: 0.8097\n",
      "Epoch 510/1500\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.4958 - accuracy: 0.8223 - val_loss: 0.5352 - val_accuracy: 0.8097\n",
      "Epoch 511/1500\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.4759 - accuracy: 0.8223 - val_loss: 0.5494 - val_accuracy: 0.8066\n",
      "Epoch 512/1500\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5036 - accuracy: 0.8052 - val_loss: 0.5850 - val_accuracy: 0.8097\n",
      "Epoch 513/1500\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.4784 - accuracy: 0.8166 - val_loss: 0.5598 - val_accuracy: 0.8036\n",
      "Epoch 514/1500\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.4949 - accuracy: 0.8126 - val_loss: 0.5726 - val_accuracy: 0.8187\n",
      "Epoch 515/1500\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.4852 - accuracy: 0.8223 - val_loss: 0.5841 - val_accuracy: 0.8066\n",
      "Epoch 516/1500\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.53367\n",
      "39/38 - 14s - loss: 0.5171 - accuracy: 0.8134 - val_loss: 0.5890 - val_accuracy: 0.8187\n",
      "Epoch 517/1500\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.53367\n",
      "39/38 - 14s - loss: 0.5197 - accuracy: 0.7995 - val_loss: 0.5620 - val_accuracy: 0.8248\n",
      "Epoch 518/1500\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.53367\n",
      "39/38 - 13s - loss: 0.5394 - accuracy: 0.7930 - val_loss: 0.5402 - val_accuracy: 0.8006\n",
      "Epoch 519/1500\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.53367\n",
      "39/38 - 14s - loss: 0.4768 - accuracy: 0.8191 - val_loss: 0.5415 - val_accuracy: 0.8127\n",
      "Epoch 520/1500\n",
      "\n",
      "Epoch 00520: val_loss improved from 0.53367 to 0.53212, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4796 - accuracy: 0.8346 - val_loss: 0.5321 - val_accuracy: 0.8157\n",
      "Epoch 521/1500\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.53212\n",
      "39/38 - 13s - loss: 0.4601 - accuracy: 0.8174 - val_loss: 0.5342 - val_accuracy: 0.8157\n",
      "Epoch 522/1500\n",
      "\n",
      "Epoch 00522: val_loss improved from 0.53212 to 0.52242, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4313 - accuracy: 0.8403 - val_loss: 0.5224 - val_accuracy: 0.8157\n",
      "Epoch 523/1500\n",
      "\n",
      "Epoch 00523: val_loss improved from 0.52242 to 0.51787, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4830 - accuracy: 0.8256 - val_loss: 0.5179 - val_accuracy: 0.8187\n",
      "Epoch 524/1500\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4490 - accuracy: 0.8386 - val_loss: 0.6414 - val_accuracy: 0.7764\n",
      "Epoch 525/1500\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.51787\n",
      "39/38 - 14s - loss: 0.5136 - accuracy: 0.8093 - val_loss: 0.5460 - val_accuracy: 0.8097\n",
      "Epoch 526/1500\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4912 - accuracy: 0.8109 - val_loss: 0.5621 - val_accuracy: 0.8127\n",
      "Epoch 527/1500\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4793 - accuracy: 0.8207 - val_loss: 0.7151 - val_accuracy: 0.7644\n",
      "Epoch 528/1500\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4772 - accuracy: 0.8297 - val_loss: 0.6812 - val_accuracy: 0.7704\n",
      "Epoch 529/1500\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4761 - accuracy: 0.8231 - val_loss: 0.5754 - val_accuracy: 0.8157\n",
      "Epoch 530/1500\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.5109 - accuracy: 0.8044 - val_loss: 0.6826 - val_accuracy: 0.7674\n",
      "Epoch 531/1500\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4867 - accuracy: 0.8126 - val_loss: 0.5834 - val_accuracy: 0.8127\n",
      "Epoch 532/1500\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.51787\n",
      "39/38 - 14s - loss: 0.4948 - accuracy: 0.8052 - val_loss: 0.5713 - val_accuracy: 0.8127\n",
      "Epoch 533/1500\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4673 - accuracy: 0.8215 - val_loss: 0.5589 - val_accuracy: 0.8157\n",
      "Epoch 534/1500\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4648 - accuracy: 0.8240 - val_loss: 0.5276 - val_accuracy: 0.8218\n",
      "Epoch 535/1500\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.51787\n",
      "39/38 - 14s - loss: 0.4725 - accuracy: 0.8150 - val_loss: 0.5227 - val_accuracy: 0.8127\n",
      "Epoch 536/1500\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.51787\n",
      "39/38 - 14s - loss: 0.4812 - accuracy: 0.8077 - val_loss: 0.5334 - val_accuracy: 0.8127\n",
      "Epoch 537/1500\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4962 - accuracy: 0.8280 - val_loss: 0.5350 - val_accuracy: 0.8248\n",
      "Epoch 538/1500\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4937 - accuracy: 0.8109 - val_loss: 0.5257 - val_accuracy: 0.8187\n",
      "Epoch 539/1500\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.51787\n",
      "39/38 - 14s - loss: 0.4617 - accuracy: 0.8394 - val_loss: 0.5260 - val_accuracy: 0.8218\n",
      "Epoch 540/1500\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4566 - accuracy: 0.8280 - val_loss: 0.5395 - val_accuracy: 0.8127\n",
      "Epoch 541/1500\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4906 - accuracy: 0.8183 - val_loss: 0.5615 - val_accuracy: 0.8218\n",
      "Epoch 542/1500\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4770 - accuracy: 0.8231 - val_loss: 0.5491 - val_accuracy: 0.8157\n",
      "Epoch 543/1500\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4738 - accuracy: 0.8329 - val_loss: 0.5362 - val_accuracy: 0.8157\n",
      "Epoch 544/1500\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4669 - accuracy: 0.8191 - val_loss: 0.5818 - val_accuracy: 0.8248\n",
      "Epoch 545/1500\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.51787\n",
      "39/38 - 13s - loss: 0.4632 - accuracy: 0.8199 - val_loss: 0.6401 - val_accuracy: 0.8006\n",
      "Epoch 546/1500\n",
      "\n",
      "Epoch 00546: val_loss improved from 0.51787 to 0.51067, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4395 - accuracy: 0.8394 - val_loss: 0.5107 - val_accuracy: 0.8248\n",
      "Epoch 547/1500\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4874 - accuracy: 0.8207 - val_loss: 0.5852 - val_accuracy: 0.8278\n",
      "Epoch 548/1500\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4441 - accuracy: 0.8313 - val_loss: 0.5182 - val_accuracy: 0.8248\n",
      "Epoch 549/1500\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4968 - accuracy: 0.8183 - val_loss: 0.5278 - val_accuracy: 0.8187\n",
      "Epoch 550/1500\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4441 - accuracy: 0.8394 - val_loss: 0.5551 - val_accuracy: 0.8157\n",
      "Epoch 551/1500\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.5349 - accuracy: 0.8093 - val_loss: 0.6858 - val_accuracy: 0.8097\n",
      "Epoch 552/1500\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4561 - accuracy: 0.8264 - val_loss: 0.6187 - val_accuracy: 0.8157\n",
      "Epoch 553/1500\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.5077 - accuracy: 0.8174 - val_loss: 0.6359 - val_accuracy: 0.8248\n",
      "Epoch 554/1500\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.5000 - accuracy: 0.8215 - val_loss: 0.6481 - val_accuracy: 0.8248\n",
      "Epoch 555/1500\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4827 - accuracy: 0.8191 - val_loss: 0.5533 - val_accuracy: 0.8036\n",
      "Epoch 556/1500\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4461 - accuracy: 0.8337 - val_loss: 0.5139 - val_accuracy: 0.8157\n",
      "Epoch 557/1500\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4552 - accuracy: 0.8240 - val_loss: 0.7703 - val_accuracy: 0.7795\n",
      "Epoch 558/1500\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4769 - accuracy: 0.8199 - val_loss: 0.5340 - val_accuracy: 0.8187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 559/1500\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4519 - accuracy: 0.8386 - val_loss: 0.5614 - val_accuracy: 0.8248\n",
      "Epoch 560/1500\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4471 - accuracy: 0.8435 - val_loss: 0.6188 - val_accuracy: 0.8218\n",
      "Epoch 561/1500\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4697 - accuracy: 0.8199 - val_loss: 0.6141 - val_accuracy: 0.8218\n",
      "Epoch 562/1500\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4374 - accuracy: 0.8370 - val_loss: 0.5167 - val_accuracy: 0.8278\n",
      "Epoch 563/1500\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4639 - accuracy: 0.8321 - val_loss: 0.5156 - val_accuracy: 0.8278\n",
      "Epoch 564/1500\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4705 - accuracy: 0.8289 - val_loss: 0.5236 - val_accuracy: 0.8248\n",
      "Epoch 565/1500\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4709 - accuracy: 0.8313 - val_loss: 0.5525 - val_accuracy: 0.8218\n",
      "Epoch 566/1500\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4772 - accuracy: 0.8354 - val_loss: 0.5720 - val_accuracy: 0.8127\n",
      "Epoch 567/1500\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4678 - accuracy: 0.8346 - val_loss: 0.5621 - val_accuracy: 0.8097\n",
      "Epoch 568/1500\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4928 - accuracy: 0.8158 - val_loss: 0.5770 - val_accuracy: 0.8127\n",
      "Epoch 569/1500\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4580 - accuracy: 0.8362 - val_loss: 0.5424 - val_accuracy: 0.8097\n",
      "Epoch 570/1500\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4469 - accuracy: 0.8240 - val_loss: 0.5316 - val_accuracy: 0.8187\n",
      "Epoch 571/1500\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4639 - accuracy: 0.8386 - val_loss: 0.5555 - val_accuracy: 0.8006\n",
      "Epoch 572/1500\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4512 - accuracy: 0.8419 - val_loss: 0.5347 - val_accuracy: 0.8097\n",
      "Epoch 573/1500\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4812 - accuracy: 0.8346 - val_loss: 0.5266 - val_accuracy: 0.8066\n",
      "Epoch 574/1500\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.5009 - accuracy: 0.8305 - val_loss: 0.5249 - val_accuracy: 0.8278\n",
      "Epoch 575/1500\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4430 - accuracy: 0.8272 - val_loss: 0.5485 - val_accuracy: 0.8066\n",
      "Epoch 576/1500\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4603 - accuracy: 0.8313 - val_loss: 0.5324 - val_accuracy: 0.8157\n",
      "Epoch 577/1500\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4737 - accuracy: 0.8223 - val_loss: 0.5301 - val_accuracy: 0.8278\n",
      "Epoch 578/1500\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4321 - accuracy: 0.8394 - val_loss: 0.5264 - val_accuracy: 0.8157\n",
      "Epoch 579/1500\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4234 - accuracy: 0.8337 - val_loss: 0.5112 - val_accuracy: 0.8248\n",
      "Epoch 580/1500\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4333 - accuracy: 0.8452 - val_loss: 0.5171 - val_accuracy: 0.8338\n",
      "Epoch 581/1500\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.3973 - accuracy: 0.8460 - val_loss: 0.6118 - val_accuracy: 0.8066\n",
      "Epoch 582/1500\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4323 - accuracy: 0.8419 - val_loss: 0.5818 - val_accuracy: 0.8097\n",
      "Epoch 583/1500\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4504 - accuracy: 0.8354 - val_loss: 0.5362 - val_accuracy: 0.8097\n",
      "Epoch 584/1500\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4588 - accuracy: 0.8346 - val_loss: 0.5136 - val_accuracy: 0.8218\n",
      "Epoch 585/1500\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4124 - accuracy: 0.8509 - val_loss: 0.5139 - val_accuracy: 0.8278\n",
      "Epoch 586/1500\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4438 - accuracy: 0.8272 - val_loss: 0.5423 - val_accuracy: 0.8248\n",
      "Epoch 587/1500\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.51067\n",
      "39/38 - 13s - loss: 0.4486 - accuracy: 0.8354 - val_loss: 0.5324 - val_accuracy: 0.8097\n",
      "Epoch 588/1500\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.51067\n",
      "39/38 - 14s - loss: 0.4625 - accuracy: 0.8329 - val_loss: 0.5145 - val_accuracy: 0.8278\n",
      "Epoch 589/1500\n",
      "\n",
      "Epoch 00589: val_loss improved from 0.51067 to 0.50597, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4513 - accuracy: 0.8256 - val_loss: 0.5060 - val_accuracy: 0.8157\n",
      "Epoch 590/1500\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.50597\n",
      "39/38 - 13s - loss: 0.4497 - accuracy: 0.8264 - val_loss: 0.5273 - val_accuracy: 0.8097\n",
      "Epoch 591/1500\n",
      "\n",
      "Epoch 00591: val_loss improved from 0.50597 to 0.50044, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.3936 - accuracy: 0.8500 - val_loss: 0.5004 - val_accuracy: 0.8248\n",
      "Epoch 592/1500\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.50044\n",
      "39/38 - 14s - loss: 0.4364 - accuracy: 0.8370 - val_loss: 0.6799 - val_accuracy: 0.7946\n",
      "Epoch 593/1500\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.50044\n",
      "39/38 - 13s - loss: 0.4425 - accuracy: 0.8354 - val_loss: 0.5073 - val_accuracy: 0.8308\n",
      "Epoch 594/1500\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.50044\n",
      "39/38 - 13s - loss: 0.4177 - accuracy: 0.8443 - val_loss: 0.5126 - val_accuracy: 0.8308\n",
      "Epoch 595/1500\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.50044\n",
      "39/38 - 14s - loss: 0.4690 - accuracy: 0.8354 - val_loss: 0.5533 - val_accuracy: 0.8127\n",
      "Epoch 596/1500\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.50044\n",
      "39/38 - 13s - loss: 0.4338 - accuracy: 0.8411 - val_loss: 0.5739 - val_accuracy: 0.8218\n",
      "Epoch 597/1500\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.50044\n",
      "39/38 - 13s - loss: 0.4371 - accuracy: 0.8403 - val_loss: 0.6880 - val_accuracy: 0.7976\n",
      "Epoch 598/1500\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.50044\n",
      "39/38 - 13s - loss: 0.4260 - accuracy: 0.8460 - val_loss: 0.5586 - val_accuracy: 0.8157\n",
      "Epoch 599/1500\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.50044\n",
      "39/38 - 13s - loss: 0.4755 - accuracy: 0.8329 - val_loss: 0.5523 - val_accuracy: 0.8248\n",
      "Epoch 600/1500\n",
      "\n",
      "Epoch 00600: val_loss improved from 0.50044 to 0.49835, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4673 - accuracy: 0.8378 - val_loss: 0.4983 - val_accuracy: 0.8248\n",
      "Epoch 601/1500\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4002 - accuracy: 0.8476 - val_loss: 0.5882 - val_accuracy: 0.8036\n",
      "Epoch 602/1500\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4688 - accuracy: 0.8272 - val_loss: 0.5078 - val_accuracy: 0.8066\n",
      "Epoch 603/1500\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4320 - accuracy: 0.8297 - val_loss: 0.5644 - val_accuracy: 0.7885\n",
      "Epoch 604/1500\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4678 - accuracy: 0.8256 - val_loss: 0.5553 - val_accuracy: 0.8218\n",
      "Epoch 605/1500\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4094 - accuracy: 0.8517 - val_loss: 0.5857 - val_accuracy: 0.8338\n",
      "Epoch 606/1500\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4550 - accuracy: 0.8362 - val_loss: 0.5170 - val_accuracy: 0.8248\n",
      "Epoch 607/1500\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4525 - accuracy: 0.8264 - val_loss: 0.5251 - val_accuracy: 0.8308\n",
      "Epoch 608/1500\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.49835\n",
      "39/38 - 14s - loss: 0.4277 - accuracy: 0.8394 - val_loss: 0.5252 - val_accuracy: 0.8248\n",
      "Epoch 609/1500\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4342 - accuracy: 0.8509 - val_loss: 0.5003 - val_accuracy: 0.8369\n",
      "Epoch 610/1500\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.3903 - accuracy: 0.8541 - val_loss: 0.5170 - val_accuracy: 0.8218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 611/1500\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4183 - accuracy: 0.8435 - val_loss: 0.6157 - val_accuracy: 0.7976\n",
      "Epoch 612/1500\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4373 - accuracy: 0.8403 - val_loss: 0.6138 - val_accuracy: 0.8006\n",
      "Epoch 613/1500\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4488 - accuracy: 0.8394 - val_loss: 0.5428 - val_accuracy: 0.8278\n",
      "Epoch 614/1500\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4040 - accuracy: 0.8517 - val_loss: 0.5430 - val_accuracy: 0.8338\n",
      "Epoch 615/1500\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4260 - accuracy: 0.8517 - val_loss: 0.5688 - val_accuracy: 0.8308\n",
      "Epoch 616/1500\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4037 - accuracy: 0.8403 - val_loss: 0.5926 - val_accuracy: 0.8187\n",
      "Epoch 617/1500\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.49835\n",
      "39/38 - 14s - loss: 0.4534 - accuracy: 0.8492 - val_loss: 0.5598 - val_accuracy: 0.8248\n",
      "Epoch 618/1500\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4201 - accuracy: 0.8452 - val_loss: 0.4989 - val_accuracy: 0.8308\n",
      "Epoch 619/1500\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.3893 - accuracy: 0.8443 - val_loss: 0.5532 - val_accuracy: 0.8218\n",
      "Epoch 620/1500\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.49835\n",
      "39/38 - 13s - loss: 0.4367 - accuracy: 0.8411 - val_loss: 0.5037 - val_accuracy: 0.8429\n",
      "Epoch 621/1500\n",
      "\n",
      "Epoch 00621: val_loss improved from 0.49835 to 0.46415, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4145 - accuracy: 0.8492 - val_loss: 0.4642 - val_accuracy: 0.8369\n",
      "Epoch 622/1500\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4249 - accuracy: 0.8394 - val_loss: 0.4685 - val_accuracy: 0.8399\n",
      "Epoch 623/1500\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4306 - accuracy: 0.8321 - val_loss: 0.4700 - val_accuracy: 0.8429\n",
      "Epoch 624/1500\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4141 - accuracy: 0.8517 - val_loss: 0.4697 - val_accuracy: 0.8248\n",
      "Epoch 625/1500\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4336 - accuracy: 0.8280 - val_loss: 0.5347 - val_accuracy: 0.8187\n",
      "Epoch 626/1500\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4425 - accuracy: 0.8354 - val_loss: 0.5363 - val_accuracy: 0.8338\n",
      "Epoch 627/1500\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4076 - accuracy: 0.8386 - val_loss: 0.5481 - val_accuracy: 0.8338\n",
      "Epoch 628/1500\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4502 - accuracy: 0.8443 - val_loss: 0.5536 - val_accuracy: 0.8338\n",
      "Epoch 629/1500\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4224 - accuracy: 0.8435 - val_loss: 0.4998 - val_accuracy: 0.8338\n",
      "Epoch 630/1500\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.3902 - accuracy: 0.8460 - val_loss: 0.5360 - val_accuracy: 0.8248\n",
      "Epoch 631/1500\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4340 - accuracy: 0.8411 - val_loss: 0.4870 - val_accuracy: 0.8278\n",
      "Epoch 632/1500\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4156 - accuracy: 0.8500 - val_loss: 0.4747 - val_accuracy: 0.8308\n",
      "Epoch 633/1500\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4282 - accuracy: 0.8452 - val_loss: 0.5291 - val_accuracy: 0.8308\n",
      "Epoch 634/1500\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4316 - accuracy: 0.8280 - val_loss: 0.5392 - val_accuracy: 0.8278\n",
      "Epoch 635/1500\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4346 - accuracy: 0.8386 - val_loss: 0.5283 - val_accuracy: 0.8218\n",
      "Epoch 636/1500\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4145 - accuracy: 0.8443 - val_loss: 0.5209 - val_accuracy: 0.8369\n",
      "Epoch 637/1500\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4155 - accuracy: 0.8476 - val_loss: 0.4945 - val_accuracy: 0.8308\n",
      "Epoch 638/1500\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4361 - accuracy: 0.8468 - val_loss: 0.5181 - val_accuracy: 0.8218\n",
      "Epoch 639/1500\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4240 - accuracy: 0.8403 - val_loss: 0.5325 - val_accuracy: 0.8308\n",
      "Epoch 640/1500\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4439 - accuracy: 0.8419 - val_loss: 0.5608 - val_accuracy: 0.8338\n",
      "Epoch 641/1500\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4121 - accuracy: 0.8476 - val_loss: 0.5171 - val_accuracy: 0.8369\n",
      "Epoch 642/1500\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4155 - accuracy: 0.8549 - val_loss: 0.5340 - val_accuracy: 0.8278\n",
      "Epoch 643/1500\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4225 - accuracy: 0.8386 - val_loss: 0.5754 - val_accuracy: 0.8187\n",
      "Epoch 644/1500\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.3987 - accuracy: 0.8419 - val_loss: 0.5101 - val_accuracy: 0.8187\n",
      "Epoch 645/1500\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4237 - accuracy: 0.8525 - val_loss: 0.5172 - val_accuracy: 0.8308\n",
      "Epoch 646/1500\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4059 - accuracy: 0.8582 - val_loss: 0.5159 - val_accuracy: 0.8399\n",
      "Epoch 647/1500\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4151 - accuracy: 0.8435 - val_loss: 0.4860 - val_accuracy: 0.8218\n",
      "Epoch 648/1500\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4490 - accuracy: 0.8460 - val_loss: 0.5122 - val_accuracy: 0.8036\n",
      "Epoch 649/1500\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.3991 - accuracy: 0.8582 - val_loss: 0.4929 - val_accuracy: 0.8278\n",
      "Epoch 650/1500\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4068 - accuracy: 0.8476 - val_loss: 0.5261 - val_accuracy: 0.8218\n",
      "Epoch 651/1500\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4102 - accuracy: 0.8329 - val_loss: 0.5045 - val_accuracy: 0.8369\n",
      "Epoch 652/1500\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4061 - accuracy: 0.8476 - val_loss: 0.4861 - val_accuracy: 0.8369\n",
      "Epoch 653/1500\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.3877 - accuracy: 0.8663 - val_loss: 0.5336 - val_accuracy: 0.8187\n",
      "Epoch 654/1500\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4269 - accuracy: 0.8378 - val_loss: 0.5158 - val_accuracy: 0.8157\n",
      "Epoch 655/1500\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4401 - accuracy: 0.8460 - val_loss: 0.4849 - val_accuracy: 0.8308\n",
      "Epoch 656/1500\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.3848 - accuracy: 0.8672 - val_loss: 0.5043 - val_accuracy: 0.8036\n",
      "Epoch 657/1500\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4545 - accuracy: 0.8280 - val_loss: 0.4978 - val_accuracy: 0.8187\n",
      "Epoch 658/1500\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4144 - accuracy: 0.8460 - val_loss: 0.5524 - val_accuracy: 0.8308\n",
      "Epoch 659/1500\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.3902 - accuracy: 0.8403 - val_loss: 0.5698 - val_accuracy: 0.8338\n",
      "Epoch 660/1500\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4058 - accuracy: 0.8476 - val_loss: 0.5826 - val_accuracy: 0.8036\n",
      "Epoch 661/1500\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4247 - accuracy: 0.8378 - val_loss: 0.5517 - val_accuracy: 0.8248\n",
      "Epoch 662/1500\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4045 - accuracy: 0.8468 - val_loss: 0.5571 - val_accuracy: 0.8218\n",
      "Epoch 663/1500\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4422 - accuracy: 0.8346 - val_loss: 0.4839 - val_accuracy: 0.8248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/1500\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4250 - accuracy: 0.8370 - val_loss: 0.5526 - val_accuracy: 0.8036\n",
      "Epoch 665/1500\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4161 - accuracy: 0.8460 - val_loss: 0.4985 - val_accuracy: 0.8429\n",
      "Epoch 666/1500\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4225 - accuracy: 0.8484 - val_loss: 0.4673 - val_accuracy: 0.8459\n",
      "Epoch 667/1500\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4414 - accuracy: 0.8394 - val_loss: 0.5014 - val_accuracy: 0.8338\n",
      "Epoch 668/1500\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4049 - accuracy: 0.8427 - val_loss: 0.4856 - val_accuracy: 0.8399\n",
      "Epoch 669/1500\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4130 - accuracy: 0.8435 - val_loss: 0.5129 - val_accuracy: 0.8369\n",
      "Epoch 670/1500\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4274 - accuracy: 0.8452 - val_loss: 0.4986 - val_accuracy: 0.8338\n",
      "Epoch 671/1500\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4233 - accuracy: 0.8435 - val_loss: 0.4922 - val_accuracy: 0.8308\n",
      "Epoch 672/1500\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4119 - accuracy: 0.8378 - val_loss: 0.5513 - val_accuracy: 0.8369\n",
      "Epoch 673/1500\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4216 - accuracy: 0.8476 - val_loss: 0.5433 - val_accuracy: 0.8459\n",
      "Epoch 674/1500\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4305 - accuracy: 0.8443 - val_loss: 0.5588 - val_accuracy: 0.8218\n",
      "Epoch 675/1500\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4202 - accuracy: 0.8354 - val_loss: 0.5556 - val_accuracy: 0.8097\n",
      "Epoch 676/1500\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4113 - accuracy: 0.8533 - val_loss: 0.5608 - val_accuracy: 0.8278\n",
      "Epoch 677/1500\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.3930 - accuracy: 0.8525 - val_loss: 0.5120 - val_accuracy: 0.8369\n",
      "Epoch 678/1500\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.3778 - accuracy: 0.8639 - val_loss: 0.6029 - val_accuracy: 0.8278\n",
      "Epoch 679/1500\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4012 - accuracy: 0.8541 - val_loss: 0.5145 - val_accuracy: 0.8369\n",
      "Epoch 680/1500\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.3918 - accuracy: 0.8557 - val_loss: 0.5153 - val_accuracy: 0.8369\n",
      "Epoch 681/1500\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.4014 - accuracy: 0.8566 - val_loss: 0.4897 - val_accuracy: 0.8369\n",
      "Epoch 682/1500\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.46415\n",
      "39/38 - 14s - loss: 0.3974 - accuracy: 0.8598 - val_loss: 0.5137 - val_accuracy: 0.8520\n",
      "Epoch 683/1500\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.3845 - accuracy: 0.8615 - val_loss: 0.4841 - val_accuracy: 0.8459\n",
      "Epoch 684/1500\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4233 - accuracy: 0.8460 - val_loss: 0.4984 - val_accuracy: 0.8369\n",
      "Epoch 685/1500\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.3914 - accuracy: 0.8598 - val_loss: 0.5610 - val_accuracy: 0.8157\n",
      "Epoch 686/1500\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.46415\n",
      "39/38 - 13s - loss: 0.4266 - accuracy: 0.8419 - val_loss: 0.5139 - val_accuracy: 0.8308\n",
      "Epoch 687/1500\n",
      "\n",
      "Epoch 00687: val_loss improved from 0.46415 to 0.46229, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 13s - loss: 0.3894 - accuracy: 0.8460 - val_loss: 0.4623 - val_accuracy: 0.8429\n",
      "Epoch 688/1500\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.46229\n",
      "39/38 - 13s - loss: 0.3981 - accuracy: 0.8533 - val_loss: 0.4912 - val_accuracy: 0.8429\n",
      "Epoch 689/1500\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.46229\n",
      "39/38 - 13s - loss: 0.3637 - accuracy: 0.8672 - val_loss: 0.5678 - val_accuracy: 0.8036\n",
      "Epoch 690/1500\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.46229\n",
      "39/38 - 13s - loss: 0.4015 - accuracy: 0.8419 - val_loss: 0.5009 - val_accuracy: 0.8429\n",
      "Epoch 691/1500\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.46229\n",
      "39/38 - 13s - loss: 0.3890 - accuracy: 0.8639 - val_loss: 0.4920 - val_accuracy: 0.8429\n",
      "Epoch 692/1500\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.46229\n",
      "39/38 - 14s - loss: 0.3639 - accuracy: 0.8688 - val_loss: 0.4807 - val_accuracy: 0.8338\n",
      "Epoch 693/1500\n",
      "\n",
      "Epoch 00693: val_loss improved from 0.46229 to 0.45715, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4125 - accuracy: 0.8557 - val_loss: 0.4572 - val_accuracy: 0.8429\n",
      "Epoch 694/1500\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3954 - accuracy: 0.8468 - val_loss: 0.4996 - val_accuracy: 0.8278\n",
      "Epoch 695/1500\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3680 - accuracy: 0.8655 - val_loss: 0.5180 - val_accuracy: 0.8369\n",
      "Epoch 696/1500\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3849 - accuracy: 0.8647 - val_loss: 0.5640 - val_accuracy: 0.8338\n",
      "Epoch 697/1500\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3984 - accuracy: 0.8574 - val_loss: 0.5523 - val_accuracy: 0.8308\n",
      "Epoch 698/1500\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3629 - accuracy: 0.8672 - val_loss: 0.5257 - val_accuracy: 0.8278\n",
      "Epoch 699/1500\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3863 - accuracy: 0.8639 - val_loss: 0.5175 - val_accuracy: 0.8429\n",
      "Epoch 700/1500\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3789 - accuracy: 0.8680 - val_loss: 0.4993 - val_accuracy: 0.8550\n",
      "Epoch 701/1500\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.45715\n",
      "39/38 - 14s - loss: 0.3826 - accuracy: 0.8500 - val_loss: 0.4728 - val_accuracy: 0.8459\n",
      "Epoch 702/1500\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3680 - accuracy: 0.8557 - val_loss: 0.4643 - val_accuracy: 0.8429\n",
      "Epoch 703/1500\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3547 - accuracy: 0.8769 - val_loss: 0.5473 - val_accuracy: 0.8338\n",
      "Epoch 704/1500\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.45715\n",
      "39/38 - 14s - loss: 0.3560 - accuracy: 0.8688 - val_loss: 0.6025 - val_accuracy: 0.8036\n",
      "Epoch 705/1500\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.45715\n",
      "39/38 - 14s - loss: 0.3939 - accuracy: 0.8541 - val_loss: 0.4685 - val_accuracy: 0.8429\n",
      "Epoch 706/1500\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.45715\n",
      "39/38 - 14s - loss: 0.3548 - accuracy: 0.8663 - val_loss: 0.4744 - val_accuracy: 0.8520\n",
      "Epoch 707/1500\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3455 - accuracy: 0.8745 - val_loss: 0.4981 - val_accuracy: 0.8489\n",
      "Epoch 708/1500\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.4012 - accuracy: 0.8484 - val_loss: 0.5246 - val_accuracy: 0.8429\n",
      "Epoch 709/1500\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3905 - accuracy: 0.8566 - val_loss: 0.5491 - val_accuracy: 0.8338\n",
      "Epoch 710/1500\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.4188 - accuracy: 0.8443 - val_loss: 0.4872 - val_accuracy: 0.8459\n",
      "Epoch 711/1500\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3860 - accuracy: 0.8574 - val_loss: 0.4981 - val_accuracy: 0.8218\n",
      "Epoch 712/1500\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3833 - accuracy: 0.8525 - val_loss: 0.4914 - val_accuracy: 0.8429\n",
      "Epoch 713/1500\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3585 - accuracy: 0.8696 - val_loss: 0.4808 - val_accuracy: 0.8399\n",
      "Epoch 714/1500\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3824 - accuracy: 0.8696 - val_loss: 0.5213 - val_accuracy: 0.8248\n",
      "Epoch 715/1500\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.45715\n",
      "39/38 - 13s - loss: 0.3756 - accuracy: 0.8598 - val_loss: 0.5029 - val_accuracy: 0.8489\n",
      "Epoch 716/1500\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.45715\n",
      "39/38 - 14s - loss: 0.3790 - accuracy: 0.8549 - val_loss: 0.5300 - val_accuracy: 0.8308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 717/1500\n",
      "\n",
      "Epoch 00717: val_loss improved from 0.45715 to 0.43516, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.3816 - accuracy: 0.8525 - val_loss: 0.4352 - val_accuracy: 0.8429\n",
      "Epoch 718/1500\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3593 - accuracy: 0.8541 - val_loss: 0.4708 - val_accuracy: 0.8459\n",
      "Epoch 719/1500\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3973 - accuracy: 0.8452 - val_loss: 0.4451 - val_accuracy: 0.8429\n",
      "Epoch 720/1500\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3851 - accuracy: 0.8509 - val_loss: 0.4913 - val_accuracy: 0.8489\n",
      "Epoch 721/1500\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3488 - accuracy: 0.8704 - val_loss: 0.6073 - val_accuracy: 0.7976\n",
      "Epoch 722/1500\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3933 - accuracy: 0.8598 - val_loss: 0.4839 - val_accuracy: 0.8308\n",
      "Epoch 723/1500\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3894 - accuracy: 0.8533 - val_loss: 0.4779 - val_accuracy: 0.8338\n",
      "Epoch 724/1500\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3661 - accuracy: 0.8639 - val_loss: 0.5679 - val_accuracy: 0.8157\n",
      "Epoch 725/1500\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3947 - accuracy: 0.8696 - val_loss: 0.4858 - val_accuracy: 0.8338\n",
      "Epoch 726/1500\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.4234 - accuracy: 0.8615 - val_loss: 0.5397 - val_accuracy: 0.8308\n",
      "Epoch 727/1500\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3979 - accuracy: 0.8615 - val_loss: 0.5731 - val_accuracy: 0.8459\n",
      "Epoch 728/1500\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.4047 - accuracy: 0.8606 - val_loss: 0.5332 - val_accuracy: 0.8459\n",
      "Epoch 729/1500\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3515 - accuracy: 0.8802 - val_loss: 0.5573 - val_accuracy: 0.8248\n",
      "Epoch 730/1500\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3776 - accuracy: 0.8631 - val_loss: 0.5127 - val_accuracy: 0.8520\n",
      "Epoch 731/1500\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3660 - accuracy: 0.8623 - val_loss: 0.4955 - val_accuracy: 0.8459\n",
      "Epoch 732/1500\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3859 - accuracy: 0.8672 - val_loss: 0.4997 - val_accuracy: 0.8248\n",
      "Epoch 733/1500\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3590 - accuracy: 0.8672 - val_loss: 0.4751 - val_accuracy: 0.8489\n",
      "Epoch 734/1500\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3493 - accuracy: 0.8680 - val_loss: 0.4569 - val_accuracy: 0.8459\n",
      "Epoch 735/1500\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3959 - accuracy: 0.8476 - val_loss: 0.5066 - val_accuracy: 0.8550\n",
      "Epoch 736/1500\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3934 - accuracy: 0.8541 - val_loss: 0.5760 - val_accuracy: 0.8489\n",
      "Epoch 737/1500\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3619 - accuracy: 0.8680 - val_loss: 0.5084 - val_accuracy: 0.8369\n",
      "Epoch 738/1500\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3736 - accuracy: 0.8615 - val_loss: 0.4937 - val_accuracy: 0.8429\n",
      "Epoch 739/1500\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3929 - accuracy: 0.8517 - val_loss: 0.4876 - val_accuracy: 0.8399\n",
      "Epoch 740/1500\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3382 - accuracy: 0.8696 - val_loss: 0.5352 - val_accuracy: 0.8369\n",
      "Epoch 741/1500\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3491 - accuracy: 0.8794 - val_loss: 0.4831 - val_accuracy: 0.8640\n",
      "Epoch 742/1500\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3882 - accuracy: 0.8598 - val_loss: 0.4401 - val_accuracy: 0.8610\n",
      "Epoch 743/1500\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3613 - accuracy: 0.8598 - val_loss: 0.5370 - val_accuracy: 0.8308\n",
      "Epoch 744/1500\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3640 - accuracy: 0.8672 - val_loss: 0.6221 - val_accuracy: 0.7976\n",
      "Epoch 745/1500\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3818 - accuracy: 0.8663 - val_loss: 0.5041 - val_accuracy: 0.8429\n",
      "Epoch 746/1500\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3388 - accuracy: 0.8688 - val_loss: 0.4943 - val_accuracy: 0.8429\n",
      "Epoch 747/1500\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3762 - accuracy: 0.8639 - val_loss: 0.5436 - val_accuracy: 0.8369\n",
      "Epoch 748/1500\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3822 - accuracy: 0.8623 - val_loss: 0.5642 - val_accuracy: 0.8127\n",
      "Epoch 749/1500\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.4112 - accuracy: 0.8541 - val_loss: 0.5007 - val_accuracy: 0.8459\n",
      "Epoch 750/1500\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.43516\n",
      "39/38 - 13s - loss: 0.3688 - accuracy: 0.8615 - val_loss: 0.5264 - val_accuracy: 0.8520\n",
      "Epoch 751/1500\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3403 - accuracy: 0.8688 - val_loss: 0.5052 - val_accuracy: 0.8520\n",
      "Epoch 752/1500\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.43516\n",
      "39/38 - 14s - loss: 0.3960 - accuracy: 0.8566 - val_loss: 0.5419 - val_accuracy: 0.8369\n",
      "Epoch 753/1500\n",
      "\n",
      "Epoch 00753: val_loss improved from 0.43516 to 0.43108, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.4066 - accuracy: 0.8500 - val_loss: 0.4311 - val_accuracy: 0.8520\n",
      "Epoch 754/1500\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.43108\n",
      "39/38 - 13s - loss: 0.3656 - accuracy: 0.8623 - val_loss: 0.4762 - val_accuracy: 0.8640\n",
      "Epoch 755/1500\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.43108\n",
      "39/38 - 13s - loss: 0.3822 - accuracy: 0.8574 - val_loss: 0.4576 - val_accuracy: 0.8520\n",
      "Epoch 756/1500\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.43108\n",
      "39/38 - 13s - loss: 0.3490 - accuracy: 0.8712 - val_loss: 0.4465 - val_accuracy: 0.8580\n",
      "Epoch 757/1500\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.43108\n",
      "39/38 - 13s - loss: 0.3755 - accuracy: 0.8631 - val_loss: 0.5336 - val_accuracy: 0.8550\n",
      "Epoch 758/1500\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.43108\n",
      "39/38 - 13s - loss: 0.3803 - accuracy: 0.8623 - val_loss: 0.4626 - val_accuracy: 0.8459\n",
      "Epoch 759/1500\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.43108\n",
      "39/38 - 13s - loss: 0.3478 - accuracy: 0.8753 - val_loss: 0.4567 - val_accuracy: 0.8489\n",
      "Epoch 760/1500\n",
      "\n",
      "Epoch 00760: val_loss improved from 0.43108 to 0.41516, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.3655 - accuracy: 0.8663 - val_loss: 0.4152 - val_accuracy: 0.8731\n",
      "Epoch 761/1500\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.41516\n",
      "39/38 - 14s - loss: 0.3617 - accuracy: 0.8672 - val_loss: 0.4640 - val_accuracy: 0.8520\n",
      "Epoch 762/1500\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.41516\n",
      "39/38 - 13s - loss: 0.3878 - accuracy: 0.8566 - val_loss: 0.4835 - val_accuracy: 0.8520\n",
      "Epoch 763/1500\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.41516\n",
      "39/38 - 13s - loss: 0.3667 - accuracy: 0.8647 - val_loss: 0.4824 - val_accuracy: 0.8369\n",
      "Epoch 764/1500\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.41516\n",
      "39/38 - 13s - loss: 0.3533 - accuracy: 0.8631 - val_loss: 0.4796 - val_accuracy: 0.8429\n",
      "Epoch 765/1500\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.41516\n",
      "39/38 - 13s - loss: 0.3948 - accuracy: 0.8655 - val_loss: 0.5087 - val_accuracy: 0.8338\n",
      "Epoch 766/1500\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.41516\n",
      "39/38 - 13s - loss: 0.3537 - accuracy: 0.8704 - val_loss: 0.4655 - val_accuracy: 0.8520\n",
      "Epoch 767/1500\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.41516\n",
      "39/38 - 13s - loss: 0.3447 - accuracy: 0.8615 - val_loss: 0.6080 - val_accuracy: 0.7976\n",
      "Epoch 768/1500\n",
      "\n",
      "Epoch 00768: val_loss improved from 0.41516 to 0.40312, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.3737 - accuracy: 0.8582 - val_loss: 0.4031 - val_accuracy: 0.8792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 769/1500\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3224 - accuracy: 0.8818 - val_loss: 0.5350 - val_accuracy: 0.7976\n",
      "Epoch 770/1500\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3856 - accuracy: 0.8606 - val_loss: 0.4929 - val_accuracy: 0.8489\n",
      "Epoch 771/1500\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3642 - accuracy: 0.8598 - val_loss: 0.4607 - val_accuracy: 0.8459\n",
      "Epoch 772/1500\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3542 - accuracy: 0.8712 - val_loss: 0.4940 - val_accuracy: 0.8399\n",
      "Epoch 773/1500\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3720 - accuracy: 0.8655 - val_loss: 0.4660 - val_accuracy: 0.8580\n",
      "Epoch 774/1500\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3498 - accuracy: 0.8598 - val_loss: 0.4561 - val_accuracy: 0.8489\n",
      "Epoch 775/1500\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3482 - accuracy: 0.8647 - val_loss: 0.4521 - val_accuracy: 0.8520\n",
      "Epoch 776/1500\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3802 - accuracy: 0.8598 - val_loss: 0.4907 - val_accuracy: 0.8399\n",
      "Epoch 777/1500\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3205 - accuracy: 0.8843 - val_loss: 0.4311 - val_accuracy: 0.8610\n",
      "Epoch 778/1500\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3145 - accuracy: 0.8835 - val_loss: 0.4919 - val_accuracy: 0.8338\n",
      "Epoch 779/1500\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3150 - accuracy: 0.8875 - val_loss: 0.4906 - val_accuracy: 0.8338\n",
      "Epoch 780/1500\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3737 - accuracy: 0.8598 - val_loss: 0.6488 - val_accuracy: 0.8218\n",
      "Epoch 781/1500\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.4177 - accuracy: 0.8509 - val_loss: 0.6595 - val_accuracy: 0.8308\n",
      "Epoch 782/1500\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3703 - accuracy: 0.8623 - val_loss: 0.5839 - val_accuracy: 0.8520\n",
      "Epoch 783/1500\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3660 - accuracy: 0.8729 - val_loss: 0.5470 - val_accuracy: 0.8369\n",
      "Epoch 784/1500\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3698 - accuracy: 0.8647 - val_loss: 0.4961 - val_accuracy: 0.8489\n",
      "Epoch 785/1500\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3772 - accuracy: 0.8663 - val_loss: 0.5027 - val_accuracy: 0.8459\n",
      "Epoch 786/1500\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3797 - accuracy: 0.8663 - val_loss: 0.4436 - val_accuracy: 0.8459\n",
      "Epoch 787/1500\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3536 - accuracy: 0.8810 - val_loss: 0.4493 - val_accuracy: 0.8459\n",
      "Epoch 788/1500\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3773 - accuracy: 0.8704 - val_loss: 0.4422 - val_accuracy: 0.8399\n",
      "Epoch 789/1500\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3402 - accuracy: 0.8704 - val_loss: 0.4490 - val_accuracy: 0.8489\n",
      "Epoch 790/1500\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3136 - accuracy: 0.8810 - val_loss: 0.4400 - val_accuracy: 0.8610\n",
      "Epoch 791/1500\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3361 - accuracy: 0.8745 - val_loss: 0.4678 - val_accuracy: 0.8399\n",
      "Epoch 792/1500\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3069 - accuracy: 0.8826 - val_loss: 0.4345 - val_accuracy: 0.8369\n",
      "Epoch 793/1500\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3505 - accuracy: 0.8623 - val_loss: 0.5394 - val_accuracy: 0.8218\n",
      "Epoch 794/1500\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3468 - accuracy: 0.8655 - val_loss: 0.5146 - val_accuracy: 0.8550\n",
      "Epoch 795/1500\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3256 - accuracy: 0.8794 - val_loss: 0.4986 - val_accuracy: 0.8459\n",
      "Epoch 796/1500\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3382 - accuracy: 0.8729 - val_loss: 0.6971 - val_accuracy: 0.7885\n",
      "Epoch 797/1500\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3270 - accuracy: 0.8859 - val_loss: 0.4374 - val_accuracy: 0.8520\n",
      "Epoch 798/1500\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3238 - accuracy: 0.8761 - val_loss: 0.4475 - val_accuracy: 0.8610\n",
      "Epoch 799/1500\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3468 - accuracy: 0.8704 - val_loss: 0.4523 - val_accuracy: 0.8550\n",
      "Epoch 800/1500\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3353 - accuracy: 0.8778 - val_loss: 0.4541 - val_accuracy: 0.8520\n",
      "Epoch 801/1500\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.4065 - accuracy: 0.8623 - val_loss: 0.4623 - val_accuracy: 0.8489\n",
      "Epoch 802/1500\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3338 - accuracy: 0.8794 - val_loss: 0.4813 - val_accuracy: 0.8640\n",
      "Epoch 803/1500\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3504 - accuracy: 0.8737 - val_loss: 0.5055 - val_accuracy: 0.8520\n",
      "Epoch 804/1500\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3003 - accuracy: 0.8802 - val_loss: 0.5235 - val_accuracy: 0.8520\n",
      "Epoch 805/1500\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3440 - accuracy: 0.8786 - val_loss: 0.4461 - val_accuracy: 0.8550\n",
      "Epoch 806/1500\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3352 - accuracy: 0.8720 - val_loss: 0.4877 - val_accuracy: 0.8429\n",
      "Epoch 807/1500\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3386 - accuracy: 0.8802 - val_loss: 0.4858 - val_accuracy: 0.8489\n",
      "Epoch 808/1500\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3451 - accuracy: 0.8598 - val_loss: 0.4374 - val_accuracy: 0.8610\n",
      "Epoch 809/1500\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3644 - accuracy: 0.8647 - val_loss: 0.4292 - val_accuracy: 0.8580\n",
      "Epoch 810/1500\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3471 - accuracy: 0.8704 - val_loss: 0.4574 - val_accuracy: 0.8550\n",
      "Epoch 811/1500\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3379 - accuracy: 0.8688 - val_loss: 0.4368 - val_accuracy: 0.8640\n",
      "Epoch 812/1500\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3334 - accuracy: 0.8753 - val_loss: 0.4428 - val_accuracy: 0.8580\n",
      "Epoch 813/1500\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3068 - accuracy: 0.8835 - val_loss: 0.4339 - val_accuracy: 0.8640\n",
      "Epoch 814/1500\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3371 - accuracy: 0.8704 - val_loss: 0.5196 - val_accuracy: 0.8248\n",
      "Epoch 815/1500\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3875 - accuracy: 0.8615 - val_loss: 0.4565 - val_accuracy: 0.8520\n",
      "Epoch 816/1500\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3102 - accuracy: 0.8843 - val_loss: 0.4550 - val_accuracy: 0.8520\n",
      "Epoch 817/1500\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3338 - accuracy: 0.8835 - val_loss: 0.4281 - val_accuracy: 0.8610\n",
      "Epoch 818/1500\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3531 - accuracy: 0.8745 - val_loss: 0.5443 - val_accuracy: 0.8248\n",
      "Epoch 819/1500\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3616 - accuracy: 0.8631 - val_loss: 0.4609 - val_accuracy: 0.8429\n",
      "Epoch 820/1500\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3185 - accuracy: 0.8712 - val_loss: 0.5604 - val_accuracy: 0.8248\n",
      "Epoch 821/1500\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3318 - accuracy: 0.8753 - val_loss: 0.4798 - val_accuracy: 0.8550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 822/1500\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3846 - accuracy: 0.8623 - val_loss: 0.4781 - val_accuracy: 0.8429\n",
      "Epoch 823/1500\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3401 - accuracy: 0.8712 - val_loss: 0.4560 - val_accuracy: 0.8610\n",
      "Epoch 824/1500\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3347 - accuracy: 0.8737 - val_loss: 0.4151 - val_accuracy: 0.8640\n",
      "Epoch 825/1500\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3233 - accuracy: 0.8778 - val_loss: 0.4593 - val_accuracy: 0.8580\n",
      "Epoch 826/1500\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3501 - accuracy: 0.8802 - val_loss: 0.4376 - val_accuracy: 0.8701\n",
      "Epoch 827/1500\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3494 - accuracy: 0.8696 - val_loss: 0.4375 - val_accuracy: 0.8550\n",
      "Epoch 828/1500\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3319 - accuracy: 0.8875 - val_loss: 0.4727 - val_accuracy: 0.8248\n",
      "Epoch 829/1500\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3273 - accuracy: 0.8753 - val_loss: 0.4104 - val_accuracy: 0.8640\n",
      "Epoch 830/1500\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3546 - accuracy: 0.8639 - val_loss: 0.4518 - val_accuracy: 0.8429\n",
      "Epoch 831/1500\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3413 - accuracy: 0.8745 - val_loss: 0.4226 - val_accuracy: 0.8520\n",
      "Epoch 832/1500\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3314 - accuracy: 0.8851 - val_loss: 0.4427 - val_accuracy: 0.8550\n",
      "Epoch 833/1500\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.40312\n",
      "39/38 - 14s - loss: 0.3407 - accuracy: 0.8761 - val_loss: 0.4851 - val_accuracy: 0.8248\n",
      "Epoch 834/1500\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.40312\n",
      "39/38 - 13s - loss: 0.3339 - accuracy: 0.8769 - val_loss: 0.4410 - val_accuracy: 0.8520\n",
      "Epoch 835/1500\n",
      "\n",
      "Epoch 00835: val_loss improved from 0.40312 to 0.39831, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.3051 - accuracy: 0.8900 - val_loss: 0.3983 - val_accuracy: 0.8761\n",
      "Epoch 836/1500\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.39831\n",
      "39/38 - 13s - loss: 0.3212 - accuracy: 0.8704 - val_loss: 0.4346 - val_accuracy: 0.8580\n",
      "Epoch 837/1500\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.39831\n",
      "39/38 - 14s - loss: 0.3279 - accuracy: 0.8753 - val_loss: 0.4201 - val_accuracy: 0.8610\n",
      "Epoch 838/1500\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.39831\n",
      "39/38 - 14s - loss: 0.2989 - accuracy: 0.8875 - val_loss: 0.4181 - val_accuracy: 0.8580\n",
      "Epoch 839/1500\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.39831\n",
      "39/38 - 14s - loss: 0.3496 - accuracy: 0.8778 - val_loss: 0.4284 - val_accuracy: 0.8640\n",
      "Epoch 840/1500\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 0.39831\n",
      "39/38 - 14s - loss: 0.3421 - accuracy: 0.8696 - val_loss: 0.4373 - val_accuracy: 0.8580\n",
      "Epoch 841/1500\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.39831\n",
      "39/38 - 14s - loss: 0.3578 - accuracy: 0.8680 - val_loss: 0.4665 - val_accuracy: 0.8459\n",
      "Epoch 842/1500\n",
      "\n",
      "Epoch 00842: val_loss improved from 0.39831 to 0.38944, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.3705 - accuracy: 0.8729 - val_loss: 0.3894 - val_accuracy: 0.8701\n",
      "Epoch 843/1500\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3520 - accuracy: 0.8778 - val_loss: 0.4191 - val_accuracy: 0.8761\n",
      "Epoch 844/1500\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3324 - accuracy: 0.8761 - val_loss: 0.4325 - val_accuracy: 0.8520\n",
      "Epoch 845/1500\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.38944\n",
      "39/38 - 14s - loss: 0.2860 - accuracy: 0.8941 - val_loss: 0.4176 - val_accuracy: 0.8550\n",
      "Epoch 846/1500\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.38944\n",
      "39/38 - 14s - loss: 0.3521 - accuracy: 0.8786 - val_loss: 0.4206 - val_accuracy: 0.8580\n",
      "Epoch 847/1500\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.2932 - accuracy: 0.8989 - val_loss: 0.4243 - val_accuracy: 0.8580\n",
      "Epoch 848/1500\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.38944\n",
      "39/38 - 14s - loss: 0.3625 - accuracy: 0.8704 - val_loss: 0.4430 - val_accuracy: 0.8610\n",
      "Epoch 849/1500\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.38944\n",
      "39/38 - 14s - loss: 0.3712 - accuracy: 0.8672 - val_loss: 0.4305 - val_accuracy: 0.8550\n",
      "Epoch 850/1500\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3420 - accuracy: 0.8761 - val_loss: 0.4020 - val_accuracy: 0.8640\n",
      "Epoch 851/1500\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3247 - accuracy: 0.8818 - val_loss: 0.4615 - val_accuracy: 0.8761\n",
      "Epoch 852/1500\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3100 - accuracy: 0.8949 - val_loss: 0.4913 - val_accuracy: 0.8580\n",
      "Epoch 853/1500\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.38944\n",
      "39/38 - 14s - loss: 0.3225 - accuracy: 0.8761 - val_loss: 0.4161 - val_accuracy: 0.8640\n",
      "Epoch 854/1500\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3296 - accuracy: 0.8818 - val_loss: 0.4475 - val_accuracy: 0.8550\n",
      "Epoch 855/1500\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3248 - accuracy: 0.8786 - val_loss: 0.4483 - val_accuracy: 0.8489\n",
      "Epoch 856/1500\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3430 - accuracy: 0.8778 - val_loss: 0.4784 - val_accuracy: 0.8550\n",
      "Epoch 857/1500\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3320 - accuracy: 0.8663 - val_loss: 0.4439 - val_accuracy: 0.8489\n",
      "Epoch 858/1500\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3310 - accuracy: 0.8696 - val_loss: 0.4320 - val_accuracy: 0.8671\n",
      "Epoch 859/1500\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3018 - accuracy: 0.8965 - val_loss: 0.4407 - val_accuracy: 0.8489\n",
      "Epoch 860/1500\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3629 - accuracy: 0.8745 - val_loss: 0.4117 - val_accuracy: 0.8671\n",
      "Epoch 861/1500\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3620 - accuracy: 0.8729 - val_loss: 0.4265 - val_accuracy: 0.8701\n",
      "Epoch 862/1500\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.38944\n",
      "39/38 - 14s - loss: 0.3681 - accuracy: 0.8672 - val_loss: 0.4878 - val_accuracy: 0.8338\n",
      "Epoch 863/1500\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3325 - accuracy: 0.8769 - val_loss: 0.4562 - val_accuracy: 0.8520\n",
      "Epoch 864/1500\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3519 - accuracy: 0.8663 - val_loss: 0.4548 - val_accuracy: 0.8671\n",
      "Epoch 865/1500\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3082 - accuracy: 0.8810 - val_loss: 0.4398 - val_accuracy: 0.8520\n",
      "Epoch 866/1500\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 0.38944\n",
      "39/38 - 14s - loss: 0.3631 - accuracy: 0.8704 - val_loss: 0.4324 - val_accuracy: 0.8640\n",
      "Epoch 867/1500\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.2842 - accuracy: 0.8981 - val_loss: 0.5261 - val_accuracy: 0.8127\n",
      "Epoch 868/1500\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3342 - accuracy: 0.8769 - val_loss: 0.4265 - val_accuracy: 0.8550\n",
      "Epoch 869/1500\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.38944\n",
      "39/38 - 14s - loss: 0.3265 - accuracy: 0.8826 - val_loss: 0.5645 - val_accuracy: 0.8157\n",
      "Epoch 870/1500\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3188 - accuracy: 0.8875 - val_loss: 0.5507 - val_accuracy: 0.8187\n",
      "Epoch 871/1500\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3310 - accuracy: 0.8875 - val_loss: 0.4885 - val_accuracy: 0.8278\n",
      "Epoch 872/1500\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3492 - accuracy: 0.8647 - val_loss: 0.4200 - val_accuracy: 0.8580\n",
      "Epoch 873/1500\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.38944\n",
      "39/38 - 14s - loss: 0.3258 - accuracy: 0.8712 - val_loss: 0.4061 - val_accuracy: 0.8671\n",
      "Epoch 874/1500\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3340 - accuracy: 0.8818 - val_loss: 0.5061 - val_accuracy: 0.8218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 875/1500\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3379 - accuracy: 0.8761 - val_loss: 0.4616 - val_accuracy: 0.8610\n",
      "Epoch 876/1500\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3461 - accuracy: 0.8802 - val_loss: 0.5073 - val_accuracy: 0.8278\n",
      "Epoch 877/1500\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.3627 - accuracy: 0.8623 - val_loss: 0.4454 - val_accuracy: 0.8429\n",
      "Epoch 878/1500\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.38944\n",
      "39/38 - 13s - loss: 0.2952 - accuracy: 0.8916 - val_loss: 0.5538 - val_accuracy: 0.8157\n",
      "Epoch 879/1500\n",
      "\n",
      "Epoch 00879: val_loss improved from 0.38944 to 0.38438, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.3396 - accuracy: 0.8802 - val_loss: 0.3844 - val_accuracy: 0.8610\n",
      "Epoch 880/1500\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.38438\n",
      "39/38 - 14s - loss: 0.3298 - accuracy: 0.8900 - val_loss: 0.4773 - val_accuracy: 0.8248\n",
      "Epoch 881/1500\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3291 - accuracy: 0.8696 - val_loss: 0.4034 - val_accuracy: 0.8731\n",
      "Epoch 882/1500\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.38438\n",
      "39/38 - 14s - loss: 0.3447 - accuracy: 0.8753 - val_loss: 0.4028 - val_accuracy: 0.8610\n",
      "Epoch 883/1500\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.2829 - accuracy: 0.8924 - val_loss: 0.4171 - val_accuracy: 0.8550\n",
      "Epoch 884/1500\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3528 - accuracy: 0.8647 - val_loss: 0.4050 - val_accuracy: 0.8520\n",
      "Epoch 885/1500\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.38438\n",
      "39/38 - 14s - loss: 0.3109 - accuracy: 0.8859 - val_loss: 0.4063 - val_accuracy: 0.8580\n",
      "Epoch 886/1500\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.38438\n",
      "39/38 - 14s - loss: 0.3035 - accuracy: 0.8851 - val_loss: 0.4128 - val_accuracy: 0.8550\n",
      "Epoch 887/1500\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3338 - accuracy: 0.8818 - val_loss: 0.3955 - val_accuracy: 0.8580\n",
      "Epoch 888/1500\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3010 - accuracy: 0.8843 - val_loss: 0.4513 - val_accuracy: 0.8399\n",
      "Epoch 889/1500\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.38438\n",
      "39/38 - 14s - loss: 0.3152 - accuracy: 0.8802 - val_loss: 0.4401 - val_accuracy: 0.8640\n",
      "Epoch 890/1500\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3529 - accuracy: 0.8704 - val_loss: 0.4591 - val_accuracy: 0.8731\n",
      "Epoch 891/1500\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.38438\n",
      "39/38 - 14s - loss: 0.3393 - accuracy: 0.8883 - val_loss: 0.4168 - val_accuracy: 0.8671\n",
      "Epoch 892/1500\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3353 - accuracy: 0.8769 - val_loss: 0.3972 - val_accuracy: 0.8671\n",
      "Epoch 893/1500\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.38438\n",
      "39/38 - 14s - loss: 0.3186 - accuracy: 0.8826 - val_loss: 0.4134 - val_accuracy: 0.8731\n",
      "Epoch 894/1500\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3110 - accuracy: 0.8859 - val_loss: 0.4245 - val_accuracy: 0.8520\n",
      "Epoch 895/1500\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3419 - accuracy: 0.8623 - val_loss: 0.4259 - val_accuracy: 0.8520\n",
      "Epoch 896/1500\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.38438\n",
      "39/38 - 14s - loss: 0.3825 - accuracy: 0.8867 - val_loss: 0.4289 - val_accuracy: 0.8429\n",
      "Epoch 897/1500\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3349 - accuracy: 0.8802 - val_loss: 0.4232 - val_accuracy: 0.8489\n",
      "Epoch 898/1500\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3039 - accuracy: 0.8932 - val_loss: 0.4077 - val_accuracy: 0.8550\n",
      "Epoch 899/1500\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3199 - accuracy: 0.8859 - val_loss: 0.4925 - val_accuracy: 0.8399\n",
      "Epoch 900/1500\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.2968 - accuracy: 0.8859 - val_loss: 0.5325 - val_accuracy: 0.8187\n",
      "Epoch 901/1500\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3155 - accuracy: 0.8761 - val_loss: 0.5273 - val_accuracy: 0.8338\n",
      "Epoch 902/1500\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.3066 - accuracy: 0.8859 - val_loss: 0.5669 - val_accuracy: 0.8278\n",
      "Epoch 903/1500\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.38438\n",
      "39/38 - 13s - loss: 0.2916 - accuracy: 0.8941 - val_loss: 0.3945 - val_accuracy: 0.8610\n",
      "Epoch 904/1500\n",
      "\n",
      "Epoch 00904: val_loss improved from 0.38438 to 0.38241, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.3138 - accuracy: 0.8794 - val_loss: 0.3824 - val_accuracy: 0.8671\n",
      "Epoch 905/1500\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.2880 - accuracy: 0.8908 - val_loss: 0.4279 - val_accuracy: 0.8489\n",
      "Epoch 906/1500\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.2999 - accuracy: 0.8867 - val_loss: 0.4611 - val_accuracy: 0.8520\n",
      "Epoch 907/1500\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3271 - accuracy: 0.8867 - val_loss: 0.4803 - val_accuracy: 0.8489\n",
      "Epoch 908/1500\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3223 - accuracy: 0.8778 - val_loss: 0.4873 - val_accuracy: 0.8550\n",
      "Epoch 909/1500\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.38241\n",
      "39/38 - 14s - loss: 0.3438 - accuracy: 0.8737 - val_loss: 0.4712 - val_accuracy: 0.8520\n",
      "Epoch 910/1500\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3159 - accuracy: 0.8745 - val_loss: 0.4320 - val_accuracy: 0.8580\n",
      "Epoch 911/1500\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3049 - accuracy: 0.8875 - val_loss: 0.4178 - val_accuracy: 0.8459\n",
      "Epoch 912/1500\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.38241\n",
      "39/38 - 14s - loss: 0.2767 - accuracy: 0.8932 - val_loss: 0.4085 - val_accuracy: 0.8580\n",
      "Epoch 913/1500\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.2833 - accuracy: 0.9022 - val_loss: 0.4433 - val_accuracy: 0.8489\n",
      "Epoch 914/1500\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3183 - accuracy: 0.8851 - val_loss: 0.3867 - val_accuracy: 0.8640\n",
      "Epoch 915/1500\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.2823 - accuracy: 0.8851 - val_loss: 0.4108 - val_accuracy: 0.8701\n",
      "Epoch 916/1500\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3524 - accuracy: 0.8712 - val_loss: 0.4301 - val_accuracy: 0.8520\n",
      "Epoch 917/1500\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3201 - accuracy: 0.8892 - val_loss: 0.4109 - val_accuracy: 0.8671\n",
      "Epoch 918/1500\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3098 - accuracy: 0.8875 - val_loss: 0.4844 - val_accuracy: 0.8520\n",
      "Epoch 919/1500\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.38241\n",
      "39/38 - 14s - loss: 0.3001 - accuracy: 0.8957 - val_loss: 0.4504 - val_accuracy: 0.8550\n",
      "Epoch 920/1500\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3053 - accuracy: 0.8916 - val_loss: 0.4198 - val_accuracy: 0.8671\n",
      "Epoch 921/1500\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.2856 - accuracy: 0.9046 - val_loss: 0.4382 - val_accuracy: 0.8610\n",
      "Epoch 922/1500\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3201 - accuracy: 0.8810 - val_loss: 0.5216 - val_accuracy: 0.8369\n",
      "Epoch 923/1500\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3374 - accuracy: 0.8818 - val_loss: 0.7022 - val_accuracy: 0.7976\n",
      "Epoch 924/1500\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.2897 - accuracy: 0.8957 - val_loss: 0.4864 - val_accuracy: 0.8550\n",
      "Epoch 925/1500\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3482 - accuracy: 0.8680 - val_loss: 0.3953 - val_accuracy: 0.8822\n",
      "Epoch 926/1500\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.38241\n",
      "39/38 - 14s - loss: 0.3146 - accuracy: 0.8769 - val_loss: 0.4590 - val_accuracy: 0.8399\n",
      "Epoch 927/1500\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.38241\n",
      "39/38 - 14s - loss: 0.3089 - accuracy: 0.9006 - val_loss: 0.4295 - val_accuracy: 0.8580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 928/1500\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.38241\n",
      "39/38 - 13s - loss: 0.3101 - accuracy: 0.8867 - val_loss: 0.4585 - val_accuracy: 0.8550\n",
      "Epoch 929/1500\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.38241\n",
      "39/38 - 14s - loss: 0.2939 - accuracy: 0.8924 - val_loss: 0.4463 - val_accuracy: 0.8580\n",
      "Epoch 930/1500\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.38241\n",
      "39/38 - 14s - loss: 0.3221 - accuracy: 0.8794 - val_loss: 0.4004 - val_accuracy: 0.8792\n",
      "Epoch 931/1500\n",
      "\n",
      "Epoch 00931: val_loss improved from 0.38241 to 0.37791, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.2924 - accuracy: 0.8916 - val_loss: 0.3779 - val_accuracy: 0.8852\n",
      "Epoch 932/1500\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.37791\n",
      "39/38 - 14s - loss: 0.3043 - accuracy: 0.8941 - val_loss: 0.4194 - val_accuracy: 0.8580\n",
      "Epoch 933/1500\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.37791\n",
      "39/38 - 13s - loss: 0.3126 - accuracy: 0.8965 - val_loss: 0.4647 - val_accuracy: 0.8580\n",
      "Epoch 934/1500\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.37791\n",
      "39/38 - 13s - loss: 0.3223 - accuracy: 0.8916 - val_loss: 0.4432 - val_accuracy: 0.8459\n",
      "Epoch 935/1500\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.37791\n",
      "39/38 - 13s - loss: 0.2985 - accuracy: 0.8941 - val_loss: 0.4171 - val_accuracy: 0.8429\n",
      "Epoch 936/1500\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.37791\n",
      "39/38 - 13s - loss: 0.3444 - accuracy: 0.8729 - val_loss: 0.3900 - val_accuracy: 0.8640\n",
      "Epoch 937/1500\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.37791\n",
      "39/38 - 13s - loss: 0.2771 - accuracy: 0.9022 - val_loss: 0.4173 - val_accuracy: 0.8429\n",
      "Epoch 938/1500\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.37791\n",
      "39/38 - 14s - loss: 0.3229 - accuracy: 0.8908 - val_loss: 0.3921 - val_accuracy: 0.8610\n",
      "Epoch 939/1500\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.37791\n",
      "39/38 - 13s - loss: 0.3287 - accuracy: 0.8949 - val_loss: 0.4483 - val_accuracy: 0.8429\n",
      "Epoch 940/1500\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.37791\n",
      "39/38 - 13s - loss: 0.3132 - accuracy: 0.8794 - val_loss: 0.4695 - val_accuracy: 0.8429\n",
      "Epoch 941/1500\n",
      "\n",
      "Epoch 00941: val_loss improved from 0.37791 to 0.36948, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.2689 - accuracy: 0.8932 - val_loss: 0.3695 - val_accuracy: 0.8731\n",
      "Epoch 942/1500\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.2969 - accuracy: 0.8908 - val_loss: 0.4888 - val_accuracy: 0.8278\n",
      "Epoch 943/1500\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3172 - accuracy: 0.8843 - val_loss: 0.4150 - val_accuracy: 0.8640\n",
      "Epoch 944/1500\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.3136 - accuracy: 0.8851 - val_loss: 0.4413 - val_accuracy: 0.8671\n",
      "Epoch 945/1500\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.3154 - accuracy: 0.8883 - val_loss: 0.4892 - val_accuracy: 0.8489\n",
      "Epoch 946/1500\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.2733 - accuracy: 0.9038 - val_loss: 0.4129 - val_accuracy: 0.8520\n",
      "Epoch 947/1500\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3055 - accuracy: 0.8810 - val_loss: 0.4077 - val_accuracy: 0.8671\n",
      "Epoch 948/1500\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3586 - accuracy: 0.8745 - val_loss: 0.4133 - val_accuracy: 0.8731\n",
      "Epoch 949/1500\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3018 - accuracy: 0.8973 - val_loss: 0.4567 - val_accuracy: 0.8429\n",
      "Epoch 950/1500\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3094 - accuracy: 0.8957 - val_loss: 0.4052 - val_accuracy: 0.8731\n",
      "Epoch 951/1500\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.3159 - accuracy: 0.8737 - val_loss: 0.5119 - val_accuracy: 0.8338\n",
      "Epoch 952/1500\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.2705 - accuracy: 0.9046 - val_loss: 0.4317 - val_accuracy: 0.8520\n",
      "Epoch 953/1500\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.2847 - accuracy: 0.9055 - val_loss: 0.4855 - val_accuracy: 0.8429\n",
      "Epoch 954/1500\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3002 - accuracy: 0.8957 - val_loss: 0.4313 - val_accuracy: 0.8701\n",
      "Epoch 955/1500\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.2704 - accuracy: 0.9055 - val_loss: 0.3927 - val_accuracy: 0.8852\n",
      "Epoch 956/1500\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.2781 - accuracy: 0.8981 - val_loss: 0.4604 - val_accuracy: 0.8610\n",
      "Epoch 957/1500\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3029 - accuracy: 0.8957 - val_loss: 0.4080 - val_accuracy: 0.8671\n",
      "Epoch 958/1500\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.3149 - accuracy: 0.8794 - val_loss: 0.4150 - val_accuracy: 0.8792\n",
      "Epoch 959/1500\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3226 - accuracy: 0.8753 - val_loss: 0.4225 - val_accuracy: 0.8520\n",
      "Epoch 960/1500\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.2971 - accuracy: 0.8957 - val_loss: 0.4753 - val_accuracy: 0.8248\n",
      "Epoch 961/1500\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3111 - accuracy: 0.8900 - val_loss: 0.4295 - val_accuracy: 0.8640\n",
      "Epoch 962/1500\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3410 - accuracy: 0.8859 - val_loss: 0.4704 - val_accuracy: 0.8489\n",
      "Epoch 963/1500\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3039 - accuracy: 0.8941 - val_loss: 0.4250 - val_accuracy: 0.8459\n",
      "Epoch 964/1500\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.2626 - accuracy: 0.9006 - val_loss: 0.4021 - val_accuracy: 0.8580\n",
      "Epoch 965/1500\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.2743 - accuracy: 0.9022 - val_loss: 0.4484 - val_accuracy: 0.8308\n",
      "Epoch 966/1500\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.2997 - accuracy: 0.8826 - val_loss: 0.3889 - val_accuracy: 0.8640\n",
      "Epoch 967/1500\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.2664 - accuracy: 0.8973 - val_loss: 0.4167 - val_accuracy: 0.8550\n",
      "Epoch 968/1500\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.3174 - accuracy: 0.8875 - val_loss: 0.4232 - val_accuracy: 0.8550\n",
      "Epoch 969/1500\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.3267 - accuracy: 0.8875 - val_loss: 0.3908 - val_accuracy: 0.8731\n",
      "Epoch 970/1500\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.2942 - accuracy: 0.8900 - val_loss: 0.3911 - val_accuracy: 0.8610\n",
      "Epoch 971/1500\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.2921 - accuracy: 0.8924 - val_loss: 0.4408 - val_accuracy: 0.8610\n",
      "Epoch 972/1500\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.36948\n",
      "39/38 - 14s - loss: 0.2832 - accuracy: 0.9038 - val_loss: 0.4175 - val_accuracy: 0.8640\n",
      "Epoch 973/1500\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.2847 - accuracy: 0.8932 - val_loss: 0.4442 - val_accuracy: 0.8459\n",
      "Epoch 974/1500\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.36948\n",
      "39/38 - 13s - loss: 0.3028 - accuracy: 0.8916 - val_loss: 0.4301 - val_accuracy: 0.8489\n",
      "Epoch 975/1500\n",
      "\n",
      "Epoch 00975: val_loss improved from 0.36948 to 0.36254, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.3081 - accuracy: 0.8851 - val_loss: 0.3625 - val_accuracy: 0.8731\n",
      "Epoch 976/1500\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2938 - accuracy: 0.8965 - val_loss: 0.3858 - val_accuracy: 0.8520\n",
      "Epoch 977/1500\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2790 - accuracy: 0.8949 - val_loss: 0.3914 - val_accuracy: 0.8399\n",
      "Epoch 978/1500\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2770 - accuracy: 0.8998 - val_loss: 0.5250 - val_accuracy: 0.8157\n",
      "Epoch 979/1500\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3062 - accuracy: 0.8826 - val_loss: 0.4245 - val_accuracy: 0.8580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 980/1500\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2906 - accuracy: 0.8989 - val_loss: 0.3891 - val_accuracy: 0.8701\n",
      "Epoch 981/1500\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2937 - accuracy: 0.8851 - val_loss: 0.4568 - val_accuracy: 0.8520\n",
      "Epoch 982/1500\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2694 - accuracy: 0.8973 - val_loss: 0.4143 - val_accuracy: 0.8610\n",
      "Epoch 983/1500\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2785 - accuracy: 0.8981 - val_loss: 0.4129 - val_accuracy: 0.8671\n",
      "Epoch 984/1500\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2829 - accuracy: 0.8957 - val_loss: 0.4241 - val_accuracy: 0.8671\n",
      "Epoch 985/1500\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3146 - accuracy: 0.8924 - val_loss: 0.3992 - val_accuracy: 0.8640\n",
      "Epoch 986/1500\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2831 - accuracy: 0.8965 - val_loss: 0.4110 - val_accuracy: 0.8701\n",
      "Epoch 987/1500\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2799 - accuracy: 0.8908 - val_loss: 0.4000 - val_accuracy: 0.8640\n",
      "Epoch 988/1500\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.3227 - accuracy: 0.8786 - val_loss: 0.4340 - val_accuracy: 0.8610\n",
      "Epoch 989/1500\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3254 - accuracy: 0.8859 - val_loss: 0.4121 - val_accuracy: 0.8671\n",
      "Epoch 990/1500\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2940 - accuracy: 0.8973 - val_loss: 0.4141 - val_accuracy: 0.8610\n",
      "Epoch 991/1500\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2739 - accuracy: 0.9055 - val_loss: 0.4145 - val_accuracy: 0.8580\n",
      "Epoch 992/1500\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2956 - accuracy: 0.9006 - val_loss: 0.3937 - val_accuracy: 0.8701\n",
      "Epoch 993/1500\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3017 - accuracy: 0.8818 - val_loss: 0.4118 - val_accuracy: 0.8580\n",
      "Epoch 994/1500\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2809 - accuracy: 0.8957 - val_loss: 0.3751 - val_accuracy: 0.8822\n",
      "Epoch 995/1500\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2690 - accuracy: 0.9022 - val_loss: 0.4088 - val_accuracy: 0.8550\n",
      "Epoch 996/1500\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2976 - accuracy: 0.8859 - val_loss: 0.5256 - val_accuracy: 0.8308\n",
      "Epoch 997/1500\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2786 - accuracy: 0.8965 - val_loss: 0.3960 - val_accuracy: 0.8429\n",
      "Epoch 998/1500\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2621 - accuracy: 0.8981 - val_loss: 0.4644 - val_accuracy: 0.8248\n",
      "Epoch 999/1500\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2898 - accuracy: 0.8941 - val_loss: 0.3761 - val_accuracy: 0.8852\n",
      "Epoch 1000/1500\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2650 - accuracy: 0.9071 - val_loss: 0.3873 - val_accuracy: 0.8701\n",
      "Epoch 1001/1500\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2705 - accuracy: 0.8989 - val_loss: 0.4448 - val_accuracy: 0.8218\n",
      "Epoch 1002/1500\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3272 - accuracy: 0.8778 - val_loss: 0.4080 - val_accuracy: 0.8610\n",
      "Epoch 1003/1500\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3150 - accuracy: 0.8867 - val_loss: 0.4042 - val_accuracy: 0.8610\n",
      "Epoch 1004/1500\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2850 - accuracy: 0.8998 - val_loss: 0.4124 - val_accuracy: 0.8610\n",
      "Epoch 1005/1500\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2955 - accuracy: 0.8941 - val_loss: 0.3854 - val_accuracy: 0.8671\n",
      "Epoch 1006/1500\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2908 - accuracy: 0.8810 - val_loss: 0.3709 - val_accuracy: 0.8792\n",
      "Epoch 1007/1500\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2598 - accuracy: 0.9030 - val_loss: 0.3956 - val_accuracy: 0.8671\n",
      "Epoch 1008/1500\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2811 - accuracy: 0.8924 - val_loss: 0.4289 - val_accuracy: 0.8520\n",
      "Epoch 1009/1500\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2734 - accuracy: 0.8989 - val_loss: 0.5615 - val_accuracy: 0.8097\n",
      "Epoch 1010/1500\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.3128 - accuracy: 0.8835 - val_loss: 0.3933 - val_accuracy: 0.8640\n",
      "Epoch 1011/1500\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2839 - accuracy: 0.8875 - val_loss: 0.5661 - val_accuracy: 0.8097\n",
      "Epoch 1012/1500\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2983 - accuracy: 0.8810 - val_loss: 0.5026 - val_accuracy: 0.8187\n",
      "Epoch 1013/1500\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2815 - accuracy: 0.9022 - val_loss: 0.4297 - val_accuracy: 0.8399\n",
      "Epoch 1014/1500\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2599 - accuracy: 0.8949 - val_loss: 0.5447 - val_accuracy: 0.8248\n",
      "Epoch 1015/1500\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2942 - accuracy: 0.8932 - val_loss: 0.4022 - val_accuracy: 0.8610\n",
      "Epoch 1016/1500\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2511 - accuracy: 0.9006 - val_loss: 0.4025 - val_accuracy: 0.8671\n",
      "Epoch 1017/1500\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2768 - accuracy: 0.8932 - val_loss: 0.3761 - val_accuracy: 0.8580\n",
      "Epoch 1018/1500\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3131 - accuracy: 0.9030 - val_loss: 0.4253 - val_accuracy: 0.8550\n",
      "Epoch 1019/1500\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3078 - accuracy: 0.8908 - val_loss: 0.4553 - val_accuracy: 0.8550\n",
      "Epoch 1020/1500\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2883 - accuracy: 0.8949 - val_loss: 0.4082 - val_accuracy: 0.8701\n",
      "Epoch 1021/1500\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2321 - accuracy: 0.9177 - val_loss: 0.5195 - val_accuracy: 0.8489\n",
      "Epoch 1022/1500\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2640 - accuracy: 0.9055 - val_loss: 0.5134 - val_accuracy: 0.8580\n",
      "Epoch 1023/1500\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2849 - accuracy: 0.9006 - val_loss: 0.4904 - val_accuracy: 0.8550\n",
      "Epoch 1024/1500\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2873 - accuracy: 0.8973 - val_loss: 0.3889 - val_accuracy: 0.8701\n",
      "Epoch 1025/1500\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2899 - accuracy: 0.9006 - val_loss: 0.3705 - val_accuracy: 0.8761\n",
      "Epoch 1026/1500\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2987 - accuracy: 0.8818 - val_loss: 0.4101 - val_accuracy: 0.8580\n",
      "Epoch 1027/1500\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2648 - accuracy: 0.9006 - val_loss: 0.4031 - val_accuracy: 0.8761\n",
      "Epoch 1028/1500\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3117 - accuracy: 0.8924 - val_loss: 0.5257 - val_accuracy: 0.8520\n",
      "Epoch 1029/1500\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2987 - accuracy: 0.8883 - val_loss: 0.4645 - val_accuracy: 0.8761\n",
      "Epoch 1030/1500\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2432 - accuracy: 0.9055 - val_loss: 0.4284 - val_accuracy: 0.8610\n",
      "Epoch 1031/1500\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2781 - accuracy: 0.9063 - val_loss: 0.4425 - val_accuracy: 0.8520\n",
      "Epoch 1032/1500\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2694 - accuracy: 0.9046 - val_loss: 0.4103 - val_accuracy: 0.8550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1033/1500\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2536 - accuracy: 0.9063 - val_loss: 0.4392 - val_accuracy: 0.8640\n",
      "Epoch 1034/1500\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2592 - accuracy: 0.9152 - val_loss: 0.4218 - val_accuracy: 0.8610\n",
      "Epoch 1035/1500\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3002 - accuracy: 0.8973 - val_loss: 0.4420 - val_accuracy: 0.8550\n",
      "Epoch 1036/1500\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3010 - accuracy: 0.8981 - val_loss: 0.4155 - val_accuracy: 0.8610\n",
      "Epoch 1037/1500\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3069 - accuracy: 0.8859 - val_loss: 0.3968 - val_accuracy: 0.8610\n",
      "Epoch 1038/1500\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2691 - accuracy: 0.8989 - val_loss: 0.4372 - val_accuracy: 0.8399\n",
      "Epoch 1039/1500\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2623 - accuracy: 0.9087 - val_loss: 0.4106 - val_accuracy: 0.8550\n",
      "Epoch 1040/1500\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2582 - accuracy: 0.9063 - val_loss: 0.4813 - val_accuracy: 0.8187\n",
      "Epoch 1041/1500\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2622 - accuracy: 0.9063 - val_loss: 0.4427 - val_accuracy: 0.8550\n",
      "Epoch 1042/1500\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2839 - accuracy: 0.8998 - val_loss: 0.4244 - val_accuracy: 0.8580\n",
      "Epoch 1043/1500\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2737 - accuracy: 0.9038 - val_loss: 0.4293 - val_accuracy: 0.8550\n",
      "Epoch 1044/1500\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2505 - accuracy: 0.9038 - val_loss: 0.3894 - val_accuracy: 0.8671\n",
      "Epoch 1045/1500\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2774 - accuracy: 0.8989 - val_loss: 0.3896 - val_accuracy: 0.8671\n",
      "Epoch 1046/1500\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2941 - accuracy: 0.9022 - val_loss: 0.4226 - val_accuracy: 0.8580\n",
      "Epoch 1047/1500\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2473 - accuracy: 0.9071 - val_loss: 0.4222 - val_accuracy: 0.8550\n",
      "Epoch 1048/1500\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2842 - accuracy: 0.8932 - val_loss: 0.4430 - val_accuracy: 0.8550\n",
      "Epoch 1049/1500\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2699 - accuracy: 0.8998 - val_loss: 0.4540 - val_accuracy: 0.8459\n",
      "Epoch 1050/1500\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.3027 - accuracy: 0.8949 - val_loss: 0.4347 - val_accuracy: 0.8520\n",
      "Epoch 1051/1500\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2744 - accuracy: 0.9055 - val_loss: 0.3976 - val_accuracy: 0.8792\n",
      "Epoch 1052/1500\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3076 - accuracy: 0.8883 - val_loss: 0.4007 - val_accuracy: 0.8671\n",
      "Epoch 1053/1500\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2834 - accuracy: 0.9120 - val_loss: 0.3830 - val_accuracy: 0.8761\n",
      "Epoch 1054/1500\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2348 - accuracy: 0.9087 - val_loss: 0.4074 - val_accuracy: 0.8550\n",
      "Epoch 1055/1500\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2687 - accuracy: 0.8949 - val_loss: 0.4491 - val_accuracy: 0.8489\n",
      "Epoch 1056/1500\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2823 - accuracy: 0.9055 - val_loss: 0.4037 - val_accuracy: 0.8520\n",
      "Epoch 1057/1500\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2882 - accuracy: 0.8989 - val_loss: 0.4277 - val_accuracy: 0.8489\n",
      "Epoch 1058/1500\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2761 - accuracy: 0.9022 - val_loss: 0.4286 - val_accuracy: 0.8761\n",
      "Epoch 1059/1500\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.3010 - accuracy: 0.9006 - val_loss: 0.4986 - val_accuracy: 0.8520\n",
      "Epoch 1060/1500\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2699 - accuracy: 0.9152 - val_loss: 0.4567 - val_accuracy: 0.8520\n",
      "Epoch 1061/1500\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2576 - accuracy: 0.9079 - val_loss: 0.4089 - val_accuracy: 0.8671\n",
      "Epoch 1062/1500\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2680 - accuracy: 0.8965 - val_loss: 0.3910 - val_accuracy: 0.8671\n",
      "Epoch 1063/1500\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2602 - accuracy: 0.9144 - val_loss: 0.4229 - val_accuracy: 0.8731\n",
      "Epoch 1064/1500\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2749 - accuracy: 0.9030 - val_loss: 0.3956 - val_accuracy: 0.8701\n",
      "Epoch 1065/1500\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2498 - accuracy: 0.9095 - val_loss: 0.4664 - val_accuracy: 0.8550\n",
      "Epoch 1066/1500\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2637 - accuracy: 0.9022 - val_loss: 0.4345 - val_accuracy: 0.8459\n",
      "Epoch 1067/1500\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2636 - accuracy: 0.9095 - val_loss: 0.4359 - val_accuracy: 0.8580\n",
      "Epoch 1068/1500\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2705 - accuracy: 0.8932 - val_loss: 0.5999 - val_accuracy: 0.8278\n",
      "Epoch 1069/1500\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2398 - accuracy: 0.9152 - val_loss: 0.4331 - val_accuracy: 0.8550\n",
      "Epoch 1070/1500\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2804 - accuracy: 0.8941 - val_loss: 0.5015 - val_accuracy: 0.8610\n",
      "Epoch 1071/1500\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2908 - accuracy: 0.8916 - val_loss: 0.4471 - val_accuracy: 0.8520\n",
      "Epoch 1072/1500\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2389 - accuracy: 0.9144 - val_loss: 0.4082 - val_accuracy: 0.8580\n",
      "Epoch 1073/1500\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2355 - accuracy: 0.9104 - val_loss: 0.4048 - val_accuracy: 0.8459\n",
      "Epoch 1074/1500\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2805 - accuracy: 0.8924 - val_loss: 0.3870 - val_accuracy: 0.8580\n",
      "Epoch 1075/1500\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2819 - accuracy: 0.8924 - val_loss: 0.3737 - val_accuracy: 0.8580\n",
      "Epoch 1076/1500\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2912 - accuracy: 0.8998 - val_loss: 0.4021 - val_accuracy: 0.8640\n",
      "Epoch 1077/1500\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2996 - accuracy: 0.8875 - val_loss: 0.5133 - val_accuracy: 0.8187\n",
      "Epoch 1078/1500\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2553 - accuracy: 0.9177 - val_loss: 0.4817 - val_accuracy: 0.8399\n",
      "Epoch 1079/1500\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2718 - accuracy: 0.8932 - val_loss: 0.4760 - val_accuracy: 0.8308\n",
      "Epoch 1080/1500\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2693 - accuracy: 0.9087 - val_loss: 0.4149 - val_accuracy: 0.8489\n",
      "Epoch 1081/1500\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2493 - accuracy: 0.9128 - val_loss: 0.4130 - val_accuracy: 0.8459\n",
      "Epoch 1082/1500\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2770 - accuracy: 0.9014 - val_loss: 0.4081 - val_accuracy: 0.8580\n",
      "Epoch 1083/1500\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2559 - accuracy: 0.9136 - val_loss: 0.4848 - val_accuracy: 0.8338\n",
      "Epoch 1084/1500\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2472 - accuracy: 0.9177 - val_loss: 0.4286 - val_accuracy: 0.8580\n",
      "Epoch 1085/1500\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2479 - accuracy: 0.9071 - val_loss: 0.5042 - val_accuracy: 0.8459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1086/1500\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2882 - accuracy: 0.9071 - val_loss: 0.4886 - val_accuracy: 0.8399\n",
      "Epoch 1087/1500\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2690 - accuracy: 0.9006 - val_loss: 0.5458 - val_accuracy: 0.8218\n",
      "Epoch 1088/1500\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2901 - accuracy: 0.8924 - val_loss: 0.5090 - val_accuracy: 0.8429\n",
      "Epoch 1089/1500\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2426 - accuracy: 0.9144 - val_loss: 0.4783 - val_accuracy: 0.8429\n",
      "Epoch 1090/1500\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2430 - accuracy: 0.9022 - val_loss: 0.4205 - val_accuracy: 0.8550\n",
      "Epoch 1091/1500\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2688 - accuracy: 0.9006 - val_loss: 0.3650 - val_accuracy: 0.8822\n",
      "Epoch 1092/1500\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2791 - accuracy: 0.9063 - val_loss: 0.4181 - val_accuracy: 0.8520\n",
      "Epoch 1093/1500\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 0.36254\n",
      "39/38 - 14s - loss: 0.2984 - accuracy: 0.8883 - val_loss: 0.4197 - val_accuracy: 0.8580\n",
      "Epoch 1094/1500\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2700 - accuracy: 0.9030 - val_loss: 0.4076 - val_accuracy: 0.8550\n",
      "Epoch 1095/1500\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2732 - accuracy: 0.9063 - val_loss: 0.3989 - val_accuracy: 0.8550\n",
      "Epoch 1096/1500\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 0.36254\n",
      "39/38 - 13s - loss: 0.2517 - accuracy: 0.9014 - val_loss: 0.4014 - val_accuracy: 0.8610\n",
      "Epoch 1097/1500\n",
      "\n",
      "Epoch 01097: val_loss improved from 0.36254 to 0.35336, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.2519 - accuracy: 0.9055 - val_loss: 0.3534 - val_accuracy: 0.8761\n",
      "Epoch 1098/1500\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2206 - accuracy: 0.9267 - val_loss: 0.3995 - val_accuracy: 0.8671\n",
      "Epoch 1099/1500\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2793 - accuracy: 0.9104 - val_loss: 0.4709 - val_accuracy: 0.8489\n",
      "Epoch 1100/1500\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 0.35336\n",
      "39/38 - 14s - loss: 0.2524 - accuracy: 0.9128 - val_loss: 0.3705 - val_accuracy: 0.8580\n",
      "Epoch 1101/1500\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2597 - accuracy: 0.9006 - val_loss: 0.4196 - val_accuracy: 0.8671\n",
      "Epoch 1102/1500\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2753 - accuracy: 0.8981 - val_loss: 0.4218 - val_accuracy: 0.8520\n",
      "Epoch 1103/1500\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2742 - accuracy: 0.9022 - val_loss: 0.4684 - val_accuracy: 0.8580\n",
      "Epoch 1104/1500\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2391 - accuracy: 0.9136 - val_loss: 0.4273 - val_accuracy: 0.8459\n",
      "Epoch 1105/1500\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2533 - accuracy: 0.8998 - val_loss: 0.3788 - val_accuracy: 0.8792\n",
      "Epoch 1106/1500\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2749 - accuracy: 0.9014 - val_loss: 0.4175 - val_accuracy: 0.8671\n",
      "Epoch 1107/1500\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2605 - accuracy: 0.9161 - val_loss: 0.3722 - val_accuracy: 0.8701\n",
      "Epoch 1108/1500\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2787 - accuracy: 0.8965 - val_loss: 0.3942 - val_accuracy: 0.8761\n",
      "Epoch 1109/1500\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2503 - accuracy: 0.9087 - val_loss: 0.4276 - val_accuracy: 0.8610\n",
      "Epoch 1110/1500\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 0.35336\n",
      "39/38 - 14s - loss: 0.2408 - accuracy: 0.9055 - val_loss: 0.3686 - val_accuracy: 0.8761\n",
      "Epoch 1111/1500\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 0.35336\n",
      "39/38 - 14s - loss: 0.2946 - accuracy: 0.8949 - val_loss: 0.3985 - val_accuracy: 0.8731\n",
      "Epoch 1112/1500\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2649 - accuracy: 0.9120 - val_loss: 0.4455 - val_accuracy: 0.8701\n",
      "Epoch 1113/1500\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.3020 - accuracy: 0.8892 - val_loss: 0.3893 - val_accuracy: 0.8731\n",
      "Epoch 1114/1500\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 0.35336\n",
      "39/38 - 13s - loss: 0.2542 - accuracy: 0.9144 - val_loss: 0.3799 - val_accuracy: 0.8822\n",
      "Epoch 1115/1500\n",
      "\n",
      "Epoch 01115: val_loss improved from 0.35336 to 0.34815, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.2615 - accuracy: 0.8989 - val_loss: 0.3482 - val_accuracy: 0.8792\n",
      "Epoch 1116/1500\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 0.34815\n",
      "39/38 - 14s - loss: 0.2322 - accuracy: 0.9104 - val_loss: 0.4246 - val_accuracy: 0.8610\n",
      "Epoch 1117/1500\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2864 - accuracy: 0.8932 - val_loss: 0.4316 - val_accuracy: 0.8640\n",
      "Epoch 1118/1500\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2441 - accuracy: 0.9161 - val_loss: 0.4948 - val_accuracy: 0.8580\n",
      "Epoch 1119/1500\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2669 - accuracy: 0.9055 - val_loss: 0.4349 - val_accuracy: 0.8610\n",
      "Epoch 1120/1500\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2823 - accuracy: 0.8949 - val_loss: 0.3973 - val_accuracy: 0.8671\n",
      "Epoch 1121/1500\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 0.34815\n",
      "39/38 - 14s - loss: 0.2499 - accuracy: 0.9169 - val_loss: 0.4363 - val_accuracy: 0.8731\n",
      "Epoch 1122/1500\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 0.34815\n",
      "39/38 - 14s - loss: 0.2774 - accuracy: 0.8973 - val_loss: 0.3899 - val_accuracy: 0.8761\n",
      "Epoch 1123/1500\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2571 - accuracy: 0.9038 - val_loss: 0.3817 - val_accuracy: 0.8792\n",
      "Epoch 1124/1500\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 0.34815\n",
      "39/38 - 14s - loss: 0.2442 - accuracy: 0.9193 - val_loss: 0.4418 - val_accuracy: 0.8580\n",
      "Epoch 1125/1500\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2692 - accuracy: 0.9006 - val_loss: 0.4491 - val_accuracy: 0.8671\n",
      "Epoch 1126/1500\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2422 - accuracy: 0.9087 - val_loss: 0.4155 - val_accuracy: 0.8701\n",
      "Epoch 1127/1500\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2742 - accuracy: 0.8989 - val_loss: 0.4597 - val_accuracy: 0.8489\n",
      "Epoch 1128/1500\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2503 - accuracy: 0.9071 - val_loss: 0.4461 - val_accuracy: 0.8701\n",
      "Epoch 1129/1500\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2479 - accuracy: 0.9152 - val_loss: 0.4731 - val_accuracy: 0.8369\n",
      "Epoch 1130/1500\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2634 - accuracy: 0.8957 - val_loss: 0.5451 - val_accuracy: 0.8278\n",
      "Epoch 1131/1500\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.3141 - accuracy: 0.9038 - val_loss: 0.4523 - val_accuracy: 0.8550\n",
      "Epoch 1132/1500\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2815 - accuracy: 0.8941 - val_loss: 0.4543 - val_accuracy: 0.8550\n",
      "Epoch 1133/1500\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 0.34815\n",
      "39/38 - 14s - loss: 0.2848 - accuracy: 0.9030 - val_loss: 0.4250 - val_accuracy: 0.8731\n",
      "Epoch 1134/1500\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2434 - accuracy: 0.9177 - val_loss: 0.4308 - val_accuracy: 0.8610\n",
      "Epoch 1135/1500\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 0.34815\n",
      "39/38 - 14s - loss: 0.2414 - accuracy: 0.9071 - val_loss: 0.4517 - val_accuracy: 0.8610\n",
      "Epoch 1136/1500\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2438 - accuracy: 0.9071 - val_loss: 0.5078 - val_accuracy: 0.8520\n",
      "Epoch 1137/1500\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2737 - accuracy: 0.8883 - val_loss: 0.3994 - val_accuracy: 0.8671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1138/1500\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2486 - accuracy: 0.9112 - val_loss: 0.4301 - val_accuracy: 0.8671\n",
      "Epoch 1139/1500\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2458 - accuracy: 0.9087 - val_loss: 0.4433 - val_accuracy: 0.8550\n",
      "Epoch 1140/1500\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 0.34815\n",
      "39/38 - 14s - loss: 0.2543 - accuracy: 0.9136 - val_loss: 0.4594 - val_accuracy: 0.8520\n",
      "Epoch 1141/1500\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2306 - accuracy: 0.9128 - val_loss: 0.4323 - val_accuracy: 0.8731\n",
      "Epoch 1142/1500\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 0.34815\n",
      "39/38 - 14s - loss: 0.2313 - accuracy: 0.9120 - val_loss: 0.4141 - val_accuracy: 0.8580\n",
      "Epoch 1143/1500\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.3020 - accuracy: 0.8932 - val_loss: 0.4483 - val_accuracy: 0.8671\n",
      "Epoch 1144/1500\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2603 - accuracy: 0.8900 - val_loss: 0.4528 - val_accuracy: 0.8671\n",
      "Epoch 1145/1500\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2289 - accuracy: 0.9169 - val_loss: 0.4120 - val_accuracy: 0.8761\n",
      "Epoch 1146/1500\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 0.34815\n",
      "39/38 - 14s - loss: 0.2346 - accuracy: 0.9144 - val_loss: 0.4691 - val_accuracy: 0.8610\n",
      "Epoch 1147/1500\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2382 - accuracy: 0.9169 - val_loss: 0.3924 - val_accuracy: 0.8701\n",
      "Epoch 1148/1500\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 0.34815\n",
      "39/38 - 14s - loss: 0.2648 - accuracy: 0.9038 - val_loss: 0.4318 - val_accuracy: 0.8489\n",
      "Epoch 1149/1500\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2541 - accuracy: 0.9112 - val_loss: 0.3861 - val_accuracy: 0.8640\n",
      "Epoch 1150/1500\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2157 - accuracy: 0.9193 - val_loss: 0.3637 - val_accuracy: 0.8912\n",
      "Epoch 1151/1500\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 0.34815\n",
      "39/38 - 13s - loss: 0.2529 - accuracy: 0.9169 - val_loss: 0.3513 - val_accuracy: 0.8852\n",
      "Epoch 1152/1500\n",
      "\n",
      "Epoch 01152: val_loss improved from 0.34815 to 0.33498, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.2401 - accuracy: 0.9087 - val_loss: 0.3350 - val_accuracy: 0.8912\n",
      "Epoch 1153/1500\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2685 - accuracy: 0.9022 - val_loss: 0.3775 - val_accuracy: 0.8943\n",
      "Epoch 1154/1500\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2990 - accuracy: 0.8867 - val_loss: 0.3579 - val_accuracy: 0.9003\n",
      "Epoch 1155/1500\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2367 - accuracy: 0.9120 - val_loss: 0.3875 - val_accuracy: 0.8792\n",
      "Epoch 1156/1500\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2490 - accuracy: 0.9152 - val_loss: 0.3669 - val_accuracy: 0.8822\n",
      "Epoch 1157/1500\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2678 - accuracy: 0.9038 - val_loss: 0.3955 - val_accuracy: 0.8761\n",
      "Epoch 1158/1500\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2466 - accuracy: 0.9071 - val_loss: 0.4550 - val_accuracy: 0.8640\n",
      "Epoch 1159/1500\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2407 - accuracy: 0.9169 - val_loss: 0.4470 - val_accuracy: 0.8701\n",
      "Epoch 1160/1500\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2453 - accuracy: 0.9152 - val_loss: 0.3833 - val_accuracy: 0.8731\n",
      "Epoch 1161/1500\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2586 - accuracy: 0.9128 - val_loss: 0.4507 - val_accuracy: 0.8580\n",
      "Epoch 1162/1500\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2767 - accuracy: 0.8998 - val_loss: 0.4858 - val_accuracy: 0.8399\n",
      "Epoch 1163/1500\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2422 - accuracy: 0.9087 - val_loss: 0.4319 - val_accuracy: 0.8731\n",
      "Epoch 1164/1500\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2397 - accuracy: 0.9152 - val_loss: 0.4381 - val_accuracy: 0.8610\n",
      "Epoch 1165/1500\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2961 - accuracy: 0.8973 - val_loss: 0.3643 - val_accuracy: 0.8943\n",
      "Epoch 1166/1500\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2413 - accuracy: 0.9144 - val_loss: 0.3872 - val_accuracy: 0.8731\n",
      "Epoch 1167/1500\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2743 - accuracy: 0.8949 - val_loss: 0.3953 - val_accuracy: 0.8731\n",
      "Epoch 1168/1500\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2627 - accuracy: 0.9055 - val_loss: 0.4162 - val_accuracy: 0.8550\n",
      "Epoch 1169/1500\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2480 - accuracy: 0.9136 - val_loss: 0.4740 - val_accuracy: 0.8429\n",
      "Epoch 1170/1500\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2319 - accuracy: 0.9201 - val_loss: 0.4781 - val_accuracy: 0.8792\n",
      "Epoch 1171/1500\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2501 - accuracy: 0.9136 - val_loss: 0.4239 - val_accuracy: 0.8761\n",
      "Epoch 1172/1500\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2419 - accuracy: 0.9120 - val_loss: 0.3922 - val_accuracy: 0.8882\n",
      "Epoch 1173/1500\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2287 - accuracy: 0.9087 - val_loss: 0.4084 - val_accuracy: 0.8822\n",
      "Epoch 1174/1500\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2475 - accuracy: 0.9087 - val_loss: 0.4298 - val_accuracy: 0.8822\n",
      "Epoch 1175/1500\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2785 - accuracy: 0.9071 - val_loss: 0.4505 - val_accuracy: 0.8459\n",
      "Epoch 1176/1500\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2604 - accuracy: 0.8998 - val_loss: 0.4283 - val_accuracy: 0.8520\n",
      "Epoch 1177/1500\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2492 - accuracy: 0.9071 - val_loss: 0.4341 - val_accuracy: 0.8610\n",
      "Epoch 1178/1500\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2577 - accuracy: 0.9038 - val_loss: 0.4565 - val_accuracy: 0.8580\n",
      "Epoch 1179/1500\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2499 - accuracy: 0.9152 - val_loss: 0.5103 - val_accuracy: 0.8640\n",
      "Epoch 1180/1500\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2059 - accuracy: 0.9169 - val_loss: 0.4165 - val_accuracy: 0.8550\n",
      "Epoch 1181/1500\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2278 - accuracy: 0.9169 - val_loss: 0.3863 - val_accuracy: 0.8671\n",
      "Epoch 1182/1500\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2400 - accuracy: 0.9144 - val_loss: 0.4356 - val_accuracy: 0.8701\n",
      "Epoch 1183/1500\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2545 - accuracy: 0.9128 - val_loss: 0.4396 - val_accuracy: 0.8550\n",
      "Epoch 1184/1500\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2611 - accuracy: 0.9087 - val_loss: 0.4170 - val_accuracy: 0.8731\n",
      "Epoch 1185/1500\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2531 - accuracy: 0.9128 - val_loss: 0.4039 - val_accuracy: 0.8610\n",
      "Epoch 1186/1500\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2106 - accuracy: 0.9209 - val_loss: 0.4004 - val_accuracy: 0.8882\n",
      "Epoch 1187/1500\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2424 - accuracy: 0.9161 - val_loss: 0.4813 - val_accuracy: 0.8550\n",
      "Epoch 1188/1500\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2377 - accuracy: 0.9169 - val_loss: 0.4598 - val_accuracy: 0.8640\n",
      "Epoch 1189/1500\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2473 - accuracy: 0.9120 - val_loss: 0.4206 - val_accuracy: 0.8731\n",
      "Epoch 1190/1500\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2013 - accuracy: 0.9283 - val_loss: 0.4186 - val_accuracy: 0.8731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1191/1500\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2685 - accuracy: 0.8957 - val_loss: 0.4317 - val_accuracy: 0.8550\n",
      "Epoch 1192/1500\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2065 - accuracy: 0.9226 - val_loss: 0.4077 - val_accuracy: 0.8671\n",
      "Epoch 1193/1500\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2011 - accuracy: 0.9258 - val_loss: 0.4154 - val_accuracy: 0.8761\n",
      "Epoch 1194/1500\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2280 - accuracy: 0.9258 - val_loss: 0.4183 - val_accuracy: 0.8701\n",
      "Epoch 1195/1500\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2296 - accuracy: 0.9275 - val_loss: 0.3933 - val_accuracy: 0.8761\n",
      "Epoch 1196/1500\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2548 - accuracy: 0.9161 - val_loss: 0.4068 - val_accuracy: 0.8610\n",
      "Epoch 1197/1500\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2510 - accuracy: 0.9193 - val_loss: 0.3860 - val_accuracy: 0.8731\n",
      "Epoch 1198/1500\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2561 - accuracy: 0.9128 - val_loss: 0.3712 - val_accuracy: 0.8792\n",
      "Epoch 1199/1500\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2472 - accuracy: 0.9128 - val_loss: 0.3971 - val_accuracy: 0.8640\n",
      "Epoch 1200/1500\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2629 - accuracy: 0.9087 - val_loss: 0.4101 - val_accuracy: 0.8701\n",
      "Epoch 1201/1500\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2999 - accuracy: 0.9071 - val_loss: 0.3796 - val_accuracy: 0.8852\n",
      "Epoch 1202/1500\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2537 - accuracy: 0.9120 - val_loss: 0.4064 - val_accuracy: 0.8822\n",
      "Epoch 1203/1500\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2116 - accuracy: 0.9234 - val_loss: 0.4628 - val_accuracy: 0.8520\n",
      "Epoch 1204/1500\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2178 - accuracy: 0.9209 - val_loss: 0.4804 - val_accuracy: 0.8489\n",
      "Epoch 1205/1500\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2341 - accuracy: 0.9193 - val_loss: 0.4174 - val_accuracy: 0.8640\n",
      "Epoch 1206/1500\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2420 - accuracy: 0.9234 - val_loss: 0.4163 - val_accuracy: 0.8792\n",
      "Epoch 1207/1500\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2484 - accuracy: 0.9112 - val_loss: 0.4118 - val_accuracy: 0.8640\n",
      "Epoch 1208/1500\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2375 - accuracy: 0.9120 - val_loss: 0.4325 - val_accuracy: 0.8671\n",
      "Epoch 1209/1500\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2044 - accuracy: 0.9275 - val_loss: 0.3774 - val_accuracy: 0.8822\n",
      "Epoch 1210/1500\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2434 - accuracy: 0.9144 - val_loss: 0.3788 - val_accuracy: 0.8761\n",
      "Epoch 1211/1500\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2245 - accuracy: 0.9234 - val_loss: 0.4078 - val_accuracy: 0.8882\n",
      "Epoch 1212/1500\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2195 - accuracy: 0.9161 - val_loss: 0.4212 - val_accuracy: 0.8761\n",
      "Epoch 1213/1500\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2351 - accuracy: 0.9095 - val_loss: 0.4258 - val_accuracy: 0.8671\n",
      "Epoch 1214/1500\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2350 - accuracy: 0.9169 - val_loss: 0.4150 - val_accuracy: 0.8671\n",
      "Epoch 1215/1500\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2084 - accuracy: 0.9299 - val_loss: 0.4093 - val_accuracy: 0.8640\n",
      "Epoch 1216/1500\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2654 - accuracy: 0.9144 - val_loss: 0.4054 - val_accuracy: 0.8792\n",
      "Epoch 1217/1500\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2084 - accuracy: 0.9258 - val_loss: 0.3902 - val_accuracy: 0.8731\n",
      "Epoch 1218/1500\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2482 - accuracy: 0.9120 - val_loss: 0.3750 - val_accuracy: 0.8701\n",
      "Epoch 1219/1500\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2437 - accuracy: 0.9136 - val_loss: 0.4046 - val_accuracy: 0.8822\n",
      "Epoch 1220/1500\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2067 - accuracy: 0.9291 - val_loss: 0.3889 - val_accuracy: 0.8761\n",
      "Epoch 1221/1500\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2410 - accuracy: 0.9242 - val_loss: 0.4692 - val_accuracy: 0.8489\n",
      "Epoch 1222/1500\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2532 - accuracy: 0.9136 - val_loss: 0.4328 - val_accuracy: 0.8580\n",
      "Epoch 1223/1500\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2594 - accuracy: 0.9095 - val_loss: 0.4322 - val_accuracy: 0.8792\n",
      "Epoch 1224/1500\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2495 - accuracy: 0.9209 - val_loss: 0.4231 - val_accuracy: 0.8671\n",
      "Epoch 1225/1500\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2215 - accuracy: 0.9185 - val_loss: 0.4629 - val_accuracy: 0.8369\n",
      "Epoch 1226/1500\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2285 - accuracy: 0.9087 - val_loss: 0.4407 - val_accuracy: 0.8610\n",
      "Epoch 1227/1500\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2270 - accuracy: 0.9087 - val_loss: 0.4907 - val_accuracy: 0.8248\n",
      "Epoch 1228/1500\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2308 - accuracy: 0.9201 - val_loss: 0.4242 - val_accuracy: 0.8640\n",
      "Epoch 1229/1500\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2699 - accuracy: 0.9193 - val_loss: 0.5070 - val_accuracy: 0.8308\n",
      "Epoch 1230/1500\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2398 - accuracy: 0.9185 - val_loss: 0.3990 - val_accuracy: 0.8792\n",
      "Epoch 1231/1500\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2684 - accuracy: 0.9079 - val_loss: 0.5059 - val_accuracy: 0.8308\n",
      "Epoch 1232/1500\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2752 - accuracy: 0.9022 - val_loss: 0.4151 - val_accuracy: 0.8701\n",
      "Epoch 1233/1500\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2075 - accuracy: 0.9242 - val_loss: 0.4320 - val_accuracy: 0.8731\n",
      "Epoch 1234/1500\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2386 - accuracy: 0.9030 - val_loss: 0.4150 - val_accuracy: 0.8701\n",
      "Epoch 1235/1500\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2202 - accuracy: 0.9209 - val_loss: 0.4336 - val_accuracy: 0.8731\n",
      "Epoch 1236/1500\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2417 - accuracy: 0.9136 - val_loss: 0.3749 - val_accuracy: 0.9003\n",
      "Epoch 1237/1500\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2210 - accuracy: 0.9046 - val_loss: 0.4043 - val_accuracy: 0.8731\n",
      "Epoch 1238/1500\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2100 - accuracy: 0.9234 - val_loss: 0.3534 - val_accuracy: 0.8852\n",
      "Epoch 1239/1500\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2260 - accuracy: 0.9185 - val_loss: 0.4132 - val_accuracy: 0.8822\n",
      "Epoch 1240/1500\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2025 - accuracy: 0.9283 - val_loss: 0.4025 - val_accuracy: 0.8731\n",
      "Epoch 1241/1500\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2417 - accuracy: 0.9095 - val_loss: 0.3675 - val_accuracy: 0.8792\n",
      "Epoch 1242/1500\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2384 - accuracy: 0.9177 - val_loss: 0.3886 - val_accuracy: 0.8792\n",
      "Epoch 1243/1500\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2423 - accuracy: 0.9063 - val_loss: 0.4514 - val_accuracy: 0.8640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1244/1500\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.1954 - accuracy: 0.9299 - val_loss: 0.4059 - val_accuracy: 0.8792\n",
      "Epoch 1245/1500\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2391 - accuracy: 0.9201 - val_loss: 0.4081 - val_accuracy: 0.8761\n",
      "Epoch 1246/1500\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2250 - accuracy: 0.9218 - val_loss: 0.3981 - val_accuracy: 0.8822\n",
      "Epoch 1247/1500\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2618 - accuracy: 0.9030 - val_loss: 0.3505 - val_accuracy: 0.8852\n",
      "Epoch 1248/1500\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2197 - accuracy: 0.9177 - val_loss: 0.3628 - val_accuracy: 0.8912\n",
      "Epoch 1249/1500\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2281 - accuracy: 0.9185 - val_loss: 0.3509 - val_accuracy: 0.8912\n",
      "Epoch 1250/1500\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2115 - accuracy: 0.9242 - val_loss: 0.3635 - val_accuracy: 0.8882\n",
      "Epoch 1251/1500\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2564 - accuracy: 0.9161 - val_loss: 0.3699 - val_accuracy: 0.8912\n",
      "Epoch 1252/1500\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2090 - accuracy: 0.9315 - val_loss: 0.3676 - val_accuracy: 0.8882\n",
      "Epoch 1253/1500\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2359 - accuracy: 0.9120 - val_loss: 0.3682 - val_accuracy: 0.8973\n",
      "Epoch 1254/1500\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2228 - accuracy: 0.9144 - val_loss: 0.3862 - val_accuracy: 0.8822\n",
      "Epoch 1255/1500\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2655 - accuracy: 0.9071 - val_loss: 0.4306 - val_accuracy: 0.8701\n",
      "Epoch 1256/1500\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2247 - accuracy: 0.9226 - val_loss: 0.3768 - val_accuracy: 0.8882\n",
      "Epoch 1257/1500\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2081 - accuracy: 0.9267 - val_loss: 0.4009 - val_accuracy: 0.8761\n",
      "Epoch 1258/1500\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2328 - accuracy: 0.9242 - val_loss: 0.3958 - val_accuracy: 0.8852\n",
      "Epoch 1259/1500\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2240 - accuracy: 0.9095 - val_loss: 0.4201 - val_accuracy: 0.8852\n",
      "Epoch 1260/1500\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2164 - accuracy: 0.9193 - val_loss: 0.3714 - val_accuracy: 0.8912\n",
      "Epoch 1261/1500\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2191 - accuracy: 0.9218 - val_loss: 0.4665 - val_accuracy: 0.8761\n",
      "Epoch 1262/1500\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2443 - accuracy: 0.9144 - val_loss: 0.4313 - val_accuracy: 0.8731\n",
      "Epoch 1263/1500\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2207 - accuracy: 0.9307 - val_loss: 0.4161 - val_accuracy: 0.8761\n",
      "Epoch 1264/1500\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2339 - accuracy: 0.9218 - val_loss: 0.4314 - val_accuracy: 0.8731\n",
      "Epoch 1265/1500\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2458 - accuracy: 0.9128 - val_loss: 0.4314 - val_accuracy: 0.8792\n",
      "Epoch 1266/1500\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2196 - accuracy: 0.9193 - val_loss: 0.4295 - val_accuracy: 0.8852\n",
      "Epoch 1267/1500\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2319 - accuracy: 0.9161 - val_loss: 0.4421 - val_accuracy: 0.8761\n",
      "Epoch 1268/1500\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2228 - accuracy: 0.9258 - val_loss: 0.3925 - val_accuracy: 0.8822\n",
      "Epoch 1269/1500\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2365 - accuracy: 0.9226 - val_loss: 0.4606 - val_accuracy: 0.8520\n",
      "Epoch 1270/1500\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2099 - accuracy: 0.9267 - val_loss: 0.3904 - val_accuracy: 0.9003\n",
      "Epoch 1271/1500\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2223 - accuracy: 0.9120 - val_loss: 0.4062 - val_accuracy: 0.8792\n",
      "Epoch 1272/1500\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2226 - accuracy: 0.9242 - val_loss: 0.4144 - val_accuracy: 0.8912\n",
      "Epoch 1273/1500\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 0.33498\n",
      "39/38 - 13s - loss: 0.2360 - accuracy: 0.9193 - val_loss: 0.3691 - val_accuracy: 0.8912\n",
      "Epoch 1274/1500\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2508 - accuracy: 0.9095 - val_loss: 0.3890 - val_accuracy: 0.8761\n",
      "Epoch 1275/1500\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.1973 - accuracy: 0.9307 - val_loss: 0.3543 - val_accuracy: 0.8882\n",
      "Epoch 1276/1500\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2301 - accuracy: 0.9177 - val_loss: 0.4202 - val_accuracy: 0.8792\n",
      "Epoch 1277/1500\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2017 - accuracy: 0.9267 - val_loss: 0.3957 - val_accuracy: 0.8882\n",
      "Epoch 1278/1500\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2305 - accuracy: 0.9169 - val_loss: 0.4064 - val_accuracy: 0.8912\n",
      "Epoch 1279/1500\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2355 - accuracy: 0.9161 - val_loss: 0.3602 - val_accuracy: 0.8882\n",
      "Epoch 1280/1500\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 0.33498\n",
      "39/38 - 14s - loss: 0.2325 - accuracy: 0.9242 - val_loss: 0.3717 - val_accuracy: 0.8822\n",
      "Epoch 1281/1500\n",
      "\n",
      "Epoch 01281: val_loss improved from 0.33498 to 0.30269, saving model to piece_model1.weights.best.hdf5\n",
      "39/38 - 14s - loss: 0.2202 - accuracy: 0.9193 - val_loss: 0.3027 - val_accuracy: 0.9063\n",
      "Epoch 1282/1500\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.1966 - accuracy: 0.9291 - val_loss: 0.3789 - val_accuracy: 0.8792\n",
      "Epoch 1283/1500\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2605 - accuracy: 0.9128 - val_loss: 0.3847 - val_accuracy: 0.8912\n",
      "Epoch 1284/1500\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2288 - accuracy: 0.9136 - val_loss: 0.3755 - val_accuracy: 0.8852\n",
      "Epoch 1285/1500\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2066 - accuracy: 0.9226 - val_loss: 0.3979 - val_accuracy: 0.8459\n",
      "Epoch 1286/1500\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2155 - accuracy: 0.9283 - val_loss: 0.3836 - val_accuracy: 0.8731\n",
      "Epoch 1287/1500\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2215 - accuracy: 0.9218 - val_loss: 0.3857 - val_accuracy: 0.8761\n",
      "Epoch 1288/1500\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2197 - accuracy: 0.9234 - val_loss: 0.4001 - val_accuracy: 0.8822\n",
      "Epoch 1289/1500\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2185 - accuracy: 0.9185 - val_loss: 0.3885 - val_accuracy: 0.8912\n",
      "Epoch 1290/1500\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2538 - accuracy: 0.9201 - val_loss: 0.3423 - val_accuracy: 0.9063\n",
      "Epoch 1291/1500\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2526 - accuracy: 0.9079 - val_loss: 0.4014 - val_accuracy: 0.8852\n",
      "Epoch 1292/1500\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2255 - accuracy: 0.9201 - val_loss: 0.3621 - val_accuracy: 0.8852\n",
      "Epoch 1293/1500\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2473 - accuracy: 0.9055 - val_loss: 0.3822 - val_accuracy: 0.8882\n",
      "Epoch 1294/1500\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2418 - accuracy: 0.9128 - val_loss: 0.3809 - val_accuracy: 0.8822\n",
      "Epoch 1295/1500\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2068 - accuracy: 0.9218 - val_loss: 0.4024 - val_accuracy: 0.8822\n",
      "Epoch 1296/1500\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2095 - accuracy: 0.9226 - val_loss: 0.3937 - val_accuracy: 0.8882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1297/1500\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2364 - accuracy: 0.9193 - val_loss: 0.4332 - val_accuracy: 0.8701\n",
      "Epoch 1298/1500\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2061 - accuracy: 0.9218 - val_loss: 0.3710 - val_accuracy: 0.8822\n",
      "Epoch 1299/1500\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2376 - accuracy: 0.9234 - val_loss: 0.5154 - val_accuracy: 0.8399\n",
      "Epoch 1300/1500\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2119 - accuracy: 0.9169 - val_loss: 0.4091 - val_accuracy: 0.8610\n",
      "Epoch 1301/1500\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2033 - accuracy: 0.9226 - val_loss: 0.4067 - val_accuracy: 0.8701\n",
      "Epoch 1302/1500\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2477 - accuracy: 0.9169 - val_loss: 0.4030 - val_accuracy: 0.8640\n",
      "Epoch 1303/1500\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2126 - accuracy: 0.9258 - val_loss: 0.4033 - val_accuracy: 0.8792\n",
      "Epoch 1304/1500\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2154 - accuracy: 0.9218 - val_loss: 0.3414 - val_accuracy: 0.8852\n",
      "Epoch 1305/1500\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2367 - accuracy: 0.9169 - val_loss: 0.3790 - val_accuracy: 0.8792\n",
      "Epoch 1306/1500\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2221 - accuracy: 0.9144 - val_loss: 0.3681 - val_accuracy: 0.8822\n",
      "Epoch 1307/1500\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2317 - accuracy: 0.9185 - val_loss: 0.3621 - val_accuracy: 0.8852\n",
      "Epoch 1308/1500\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2017 - accuracy: 0.9389 - val_loss: 0.3904 - val_accuracy: 0.8822\n",
      "Epoch 1309/1500\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2327 - accuracy: 0.9267 - val_loss: 0.3591 - val_accuracy: 0.8852\n",
      "Epoch 1310/1500\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.1898 - accuracy: 0.9421 - val_loss: 0.3306 - val_accuracy: 0.8912\n",
      "Epoch 1311/1500\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2238 - accuracy: 0.9242 - val_loss: 0.3311 - val_accuracy: 0.8882\n",
      "Epoch 1312/1500\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2306 - accuracy: 0.9201 - val_loss: 0.3292 - val_accuracy: 0.8912\n",
      "Epoch 1313/1500\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2318 - accuracy: 0.9250 - val_loss: 0.3475 - val_accuracy: 0.8943\n",
      "Epoch 1314/1500\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2451 - accuracy: 0.9226 - val_loss: 0.3645 - val_accuracy: 0.8882\n",
      "Epoch 1315/1500\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2293 - accuracy: 0.9169 - val_loss: 0.3830 - val_accuracy: 0.8761\n",
      "Epoch 1316/1500\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2238 - accuracy: 0.9291 - val_loss: 0.4256 - val_accuracy: 0.8640\n",
      "Epoch 1317/1500\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2173 - accuracy: 0.9250 - val_loss: 0.3432 - val_accuracy: 0.9033\n",
      "Epoch 1318/1500\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2572 - accuracy: 0.9177 - val_loss: 0.4307 - val_accuracy: 0.8761\n",
      "Epoch 1319/1500\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2312 - accuracy: 0.9177 - val_loss: 0.3896 - val_accuracy: 0.8761\n",
      "Epoch 1320/1500\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2324 - accuracy: 0.9193 - val_loss: 0.3666 - val_accuracy: 0.8852\n",
      "Epoch 1321/1500\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2024 - accuracy: 0.9332 - val_loss: 0.3883 - val_accuracy: 0.8822\n",
      "Epoch 1322/1500\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.1953 - accuracy: 0.9209 - val_loss: 0.3566 - val_accuracy: 0.8852\n",
      "Epoch 1323/1500\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2227 - accuracy: 0.9193 - val_loss: 0.3558 - val_accuracy: 0.8912\n",
      "Epoch 1324/1500\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.1745 - accuracy: 0.9389 - val_loss: 0.3996 - val_accuracy: 0.8822\n",
      "Epoch 1325/1500\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2606 - accuracy: 0.9193 - val_loss: 0.4194 - val_accuracy: 0.8822\n",
      "Epoch 1326/1500\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2010 - accuracy: 0.9332 - val_loss: 0.3824 - val_accuracy: 0.8852\n",
      "Epoch 1327/1500\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2039 - accuracy: 0.9332 - val_loss: 0.4587 - val_accuracy: 0.8640\n",
      "Epoch 1328/1500\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2339 - accuracy: 0.9120 - val_loss: 0.3549 - val_accuracy: 0.8912\n",
      "Epoch 1329/1500\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2094 - accuracy: 0.9283 - val_loss: 0.3604 - val_accuracy: 0.8822\n",
      "Epoch 1330/1500\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2250 - accuracy: 0.9209 - val_loss: 0.3853 - val_accuracy: 0.8852\n",
      "Epoch 1331/1500\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2585 - accuracy: 0.9038 - val_loss: 0.4146 - val_accuracy: 0.8761\n",
      "Epoch 1332/1500\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2033 - accuracy: 0.9250 - val_loss: 0.4286 - val_accuracy: 0.8640\n",
      "Epoch 1333/1500\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2069 - accuracy: 0.9275 - val_loss: 0.4626 - val_accuracy: 0.8640\n",
      "Epoch 1334/1500\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2189 - accuracy: 0.9209 - val_loss: 0.3589 - val_accuracy: 0.9003\n",
      "Epoch 1335/1500\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2499 - accuracy: 0.9046 - val_loss: 0.4015 - val_accuracy: 0.8761\n",
      "Epoch 1336/1500\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2115 - accuracy: 0.9234 - val_loss: 0.4513 - val_accuracy: 0.8852\n",
      "Epoch 1337/1500\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.1880 - accuracy: 0.9250 - val_loss: 0.4257 - val_accuracy: 0.8882\n",
      "Epoch 1338/1500\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2109 - accuracy: 0.9152 - val_loss: 0.4368 - val_accuracy: 0.8671\n",
      "Epoch 1339/1500\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2161 - accuracy: 0.9193 - val_loss: 0.3538 - val_accuracy: 0.8822\n",
      "Epoch 1340/1500\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2195 - accuracy: 0.9218 - val_loss: 0.3626 - val_accuracy: 0.8912\n",
      "Epoch 1341/1500\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2264 - accuracy: 0.9275 - val_loss: 0.4119 - val_accuracy: 0.8610\n",
      "Epoch 1342/1500\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2561 - accuracy: 0.9112 - val_loss: 0.3798 - val_accuracy: 0.8852\n",
      "Epoch 1343/1500\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2309 - accuracy: 0.9120 - val_loss: 0.3382 - val_accuracy: 0.9003\n",
      "Epoch 1344/1500\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.1969 - accuracy: 0.9201 - val_loss: 0.3925 - val_accuracy: 0.8792\n",
      "Epoch 1345/1500\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2524 - accuracy: 0.9169 - val_loss: 0.3411 - val_accuracy: 0.8882\n",
      "Epoch 1346/1500\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2190 - accuracy: 0.9185 - val_loss: 0.3420 - val_accuracy: 0.8852\n",
      "Epoch 1347/1500\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2047 - accuracy: 0.9307 - val_loss: 0.3469 - val_accuracy: 0.8882\n",
      "Epoch 1348/1500\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2230 - accuracy: 0.9226 - val_loss: 0.3579 - val_accuracy: 0.8852\n",
      "Epoch 1349/1500\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2077 - accuracy: 0.9275 - val_loss: 0.3732 - val_accuracy: 0.8822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1350/1500\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2324 - accuracy: 0.9218 - val_loss: 0.4005 - val_accuracy: 0.8731\n",
      "Epoch 1351/1500\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.1876 - accuracy: 0.9275 - val_loss: 0.3820 - val_accuracy: 0.8852\n",
      "Epoch 1352/1500\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2114 - accuracy: 0.9234 - val_loss: 0.3713 - val_accuracy: 0.8852\n",
      "Epoch 1353/1500\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2235 - accuracy: 0.9267 - val_loss: 0.4083 - val_accuracy: 0.8701\n",
      "Epoch 1354/1500\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.1973 - accuracy: 0.9226 - val_loss: 0.4202 - val_accuracy: 0.8852\n",
      "Epoch 1355/1500\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2070 - accuracy: 0.9242 - val_loss: 0.3859 - val_accuracy: 0.9063\n",
      "Epoch 1356/1500\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2115 - accuracy: 0.9226 - val_loss: 0.3705 - val_accuracy: 0.9033\n",
      "Epoch 1357/1500\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2170 - accuracy: 0.9242 - val_loss: 0.3604 - val_accuracy: 0.9033\n",
      "Epoch 1358/1500\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2088 - accuracy: 0.9315 - val_loss: 0.4497 - val_accuracy: 0.8761\n",
      "Epoch 1359/1500\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2329 - accuracy: 0.9242 - val_loss: 0.3692 - val_accuracy: 0.8943\n",
      "Epoch 1360/1500\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2188 - accuracy: 0.9242 - val_loss: 0.3776 - val_accuracy: 0.8852\n",
      "Epoch 1361/1500\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2157 - accuracy: 0.9226 - val_loss: 0.3840 - val_accuracy: 0.8761\n",
      "Epoch 1362/1500\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2079 - accuracy: 0.9177 - val_loss: 0.3802 - val_accuracy: 0.8792\n",
      "Epoch 1363/1500\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2148 - accuracy: 0.9250 - val_loss: 0.4152 - val_accuracy: 0.8610\n",
      "Epoch 1364/1500\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2242 - accuracy: 0.9209 - val_loss: 0.3968 - val_accuracy: 0.8912\n",
      "Epoch 1365/1500\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.1898 - accuracy: 0.9372 - val_loss: 0.3222 - val_accuracy: 0.9003\n",
      "Epoch 1366/1500\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2120 - accuracy: 0.9226 - val_loss: 0.3376 - val_accuracy: 0.8973\n",
      "Epoch 1367/1500\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2158 - accuracy: 0.9185 - val_loss: 0.4276 - val_accuracy: 0.8640\n",
      "Epoch 1368/1500\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2102 - accuracy: 0.9283 - val_loss: 0.3949 - val_accuracy: 0.8852\n",
      "Epoch 1369/1500\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2020 - accuracy: 0.9307 - val_loss: 0.4389 - val_accuracy: 0.8731\n",
      "Epoch 1370/1500\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.1816 - accuracy: 0.9356 - val_loss: 0.4999 - val_accuracy: 0.8671\n",
      "Epoch 1371/1500\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2109 - accuracy: 0.9193 - val_loss: 0.3875 - val_accuracy: 0.8852\n",
      "Epoch 1372/1500\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2306 - accuracy: 0.9234 - val_loss: 0.3888 - val_accuracy: 0.8761\n",
      "Epoch 1373/1500\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2651 - accuracy: 0.9144 - val_loss: 0.3590 - val_accuracy: 0.8882\n",
      "Epoch 1374/1500\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2392 - accuracy: 0.9340 - val_loss: 0.3977 - val_accuracy: 0.8761\n",
      "Epoch 1375/1500\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2005 - accuracy: 0.9324 - val_loss: 0.3907 - val_accuracy: 0.8852\n",
      "Epoch 1376/1500\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2060 - accuracy: 0.9242 - val_loss: 0.4467 - val_accuracy: 0.8731\n",
      "Epoch 1377/1500\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.1908 - accuracy: 0.9356 - val_loss: 0.3929 - val_accuracy: 0.8973\n",
      "Epoch 1378/1500\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.1977 - accuracy: 0.9267 - val_loss: 0.4036 - val_accuracy: 0.8852\n",
      "Epoch 1379/1500\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2301 - accuracy: 0.9193 - val_loss: 0.3673 - val_accuracy: 0.8882\n",
      "Epoch 1380/1500\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.1735 - accuracy: 0.9372 - val_loss: 0.3996 - val_accuracy: 0.9063\n",
      "Epoch 1381/1500\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2159 - accuracy: 0.9185 - val_loss: 0.4689 - val_accuracy: 0.8671\n",
      "Epoch 1382/1500\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2361 - accuracy: 0.9250 - val_loss: 0.5086 - val_accuracy: 0.8701\n",
      "Epoch 1383/1500\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2619 - accuracy: 0.9120 - val_loss: 0.4148 - val_accuracy: 0.8731\n",
      "Epoch 1384/1500\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2313 - accuracy: 0.9087 - val_loss: 0.4071 - val_accuracy: 0.9033\n",
      "Epoch 1385/1500\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2126 - accuracy: 0.9299 - val_loss: 0.4636 - val_accuracy: 0.8731\n",
      "Epoch 1386/1500\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2531 - accuracy: 0.9144 - val_loss: 0.4028 - val_accuracy: 0.8943\n",
      "Epoch 1387/1500\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.1943 - accuracy: 0.9348 - val_loss: 0.4700 - val_accuracy: 0.8792\n",
      "Epoch 1388/1500\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2115 - accuracy: 0.9201 - val_loss: 0.4889 - val_accuracy: 0.8459\n",
      "Epoch 1389/1500\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2169 - accuracy: 0.9209 - val_loss: 0.4548 - val_accuracy: 0.8580\n",
      "Epoch 1390/1500\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2064 - accuracy: 0.9299 - val_loss: 0.3766 - val_accuracy: 0.8822\n",
      "Epoch 1391/1500\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.1799 - accuracy: 0.9299 - val_loss: 0.4243 - val_accuracy: 0.8822\n",
      "Epoch 1392/1500\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.1998 - accuracy: 0.9348 - val_loss: 0.3724 - val_accuracy: 0.8882\n",
      "Epoch 1393/1500\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2142 - accuracy: 0.9193 - val_loss: 0.4375 - val_accuracy: 0.8731\n",
      "Epoch 1394/1500\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.1951 - accuracy: 0.9226 - val_loss: 0.4184 - val_accuracy: 0.8792\n",
      "Epoch 1395/1500\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 0.30269\n",
      "39/38 - 14s - loss: 0.2012 - accuracy: 0.9356 - val_loss: 0.3605 - val_accuracy: 0.8882\n",
      "Epoch 1396/1500\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 0.30269\n",
      "39/38 - 13s - loss: 0.2297 - accuracy: 0.9144 - val_loss: 0.3920 - val_accuracy: 0.8912\n",
      "Epoch 1397/1500\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 0.30269\n",
      "39/38 - 10s - loss: 0.2181 - accuracy: 0.9218 - val_loss: 0.3586 - val_accuracy: 0.9003\n",
      "Epoch 1398/1500\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2061 - accuracy: 0.9307 - val_loss: 0.4169 - val_accuracy: 0.8852\n",
      "Epoch 1399/1500\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1820 - accuracy: 0.9315 - val_loss: 0.3536 - val_accuracy: 0.8973\n",
      "Epoch 1400/1500\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2290 - accuracy: 0.9242 - val_loss: 0.3554 - val_accuracy: 0.9003\n",
      "Epoch 1401/1500\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2090 - accuracy: 0.9242 - val_loss: 0.3893 - val_accuracy: 0.8882\n",
      "Epoch 1402/1500\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1941 - accuracy: 0.9332 - val_loss: 0.3933 - val_accuracy: 0.8822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1403/1500\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2128 - accuracy: 0.9144 - val_loss: 0.3994 - val_accuracy: 0.8882\n",
      "Epoch 1404/1500\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2228 - accuracy: 0.9177 - val_loss: 0.4321 - val_accuracy: 0.8852\n",
      "Epoch 1405/1500\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 0.30269\n",
      "39/38 - 9s - loss: 0.2232 - accuracy: 0.9185 - val_loss: 0.4392 - val_accuracy: 0.8792\n",
      "Epoch 1406/1500\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 0.30269\n",
      "39/38 - 9s - loss: 0.2375 - accuracy: 0.9144 - val_loss: 0.3239 - val_accuracy: 0.9063\n",
      "Epoch 1407/1500\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2039 - accuracy: 0.9291 - val_loss: 0.3522 - val_accuracy: 0.9003\n",
      "Epoch 1408/1500\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1968 - accuracy: 0.9332 - val_loss: 0.4031 - val_accuracy: 0.8792\n",
      "Epoch 1409/1500\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2054 - accuracy: 0.9307 - val_loss: 0.3637 - val_accuracy: 0.8822\n",
      "Epoch 1410/1500\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1935 - accuracy: 0.9299 - val_loss: 0.3843 - val_accuracy: 0.8882\n",
      "Epoch 1411/1500\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2148 - accuracy: 0.9340 - val_loss: 0.3503 - val_accuracy: 0.8912\n",
      "Epoch 1412/1500\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2102 - accuracy: 0.9250 - val_loss: 0.4242 - val_accuracy: 0.8822\n",
      "Epoch 1413/1500\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2150 - accuracy: 0.9356 - val_loss: 0.4388 - val_accuracy: 0.8792\n",
      "Epoch 1414/1500\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2015 - accuracy: 0.9356 - val_loss: 0.4484 - val_accuracy: 0.8701\n",
      "Epoch 1415/1500\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1977 - accuracy: 0.9267 - val_loss: 0.4198 - val_accuracy: 0.8852\n",
      "Epoch 1416/1500\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1736 - accuracy: 0.9324 - val_loss: 0.3979 - val_accuracy: 0.8852\n",
      "Epoch 1417/1500\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2153 - accuracy: 0.9161 - val_loss: 0.4068 - val_accuracy: 0.8912\n",
      "Epoch 1418/1500\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2096 - accuracy: 0.9136 - val_loss: 0.3606 - val_accuracy: 0.8882\n",
      "Epoch 1419/1500\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1730 - accuracy: 0.9291 - val_loss: 0.3781 - val_accuracy: 0.8943\n",
      "Epoch 1420/1500\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2171 - accuracy: 0.9193 - val_loss: 0.3922 - val_accuracy: 0.8792\n",
      "Epoch 1421/1500\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2180 - accuracy: 0.9226 - val_loss: 0.3772 - val_accuracy: 0.8882\n",
      "Epoch 1422/1500\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2051 - accuracy: 0.9283 - val_loss: 0.3471 - val_accuracy: 0.9033\n",
      "Epoch 1423/1500\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1829 - accuracy: 0.9291 - val_loss: 0.3220 - val_accuracy: 0.8973\n",
      "Epoch 1424/1500\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1988 - accuracy: 0.9315 - val_loss: 0.3669 - val_accuracy: 0.8973\n",
      "Epoch 1425/1500\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1720 - accuracy: 0.9446 - val_loss: 0.3818 - val_accuracy: 0.9003\n",
      "Epoch 1426/1500\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 0.30269\n",
      "39/38 - 9s - loss: 0.2125 - accuracy: 0.9234 - val_loss: 0.3420 - val_accuracy: 0.8943\n",
      "Epoch 1427/1500\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1836 - accuracy: 0.9356 - val_loss: 0.4103 - val_accuracy: 0.8459\n",
      "Epoch 1428/1500\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2012 - accuracy: 0.9275 - val_loss: 0.3571 - val_accuracy: 0.8912\n",
      "Epoch 1429/1500\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1917 - accuracy: 0.9250 - val_loss: 0.3716 - val_accuracy: 0.8912\n",
      "Epoch 1430/1500\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2011 - accuracy: 0.9332 - val_loss: 0.4081 - val_accuracy: 0.8852\n",
      "Epoch 1431/1500\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2358 - accuracy: 0.9104 - val_loss: 0.3467 - val_accuracy: 0.8973\n",
      "Epoch 1432/1500\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1934 - accuracy: 0.9291 - val_loss: 0.3593 - val_accuracy: 0.9033\n",
      "Epoch 1433/1500\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1795 - accuracy: 0.9315 - val_loss: 0.3481 - val_accuracy: 0.9033\n",
      "Epoch 1434/1500\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1811 - accuracy: 0.9315 - val_loss: 0.3647 - val_accuracy: 0.8852\n",
      "Epoch 1435/1500\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2160 - accuracy: 0.9209 - val_loss: 0.3625 - val_accuracy: 0.8912\n",
      "Epoch 1436/1500\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1770 - accuracy: 0.9324 - val_loss: 0.4113 - val_accuracy: 0.8671\n",
      "Epoch 1437/1500\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2233 - accuracy: 0.9242 - val_loss: 0.4826 - val_accuracy: 0.8671\n",
      "Epoch 1438/1500\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2539 - accuracy: 0.9218 - val_loss: 0.3985 - val_accuracy: 0.8761\n",
      "Epoch 1439/1500\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1828 - accuracy: 0.9299 - val_loss: 0.3740 - val_accuracy: 0.8882\n",
      "Epoch 1440/1500\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2452 - accuracy: 0.9226 - val_loss: 0.3491 - val_accuracy: 0.9003\n",
      "Epoch 1441/1500\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1964 - accuracy: 0.9291 - val_loss: 0.4196 - val_accuracy: 0.8882\n",
      "Epoch 1442/1500\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1854 - accuracy: 0.9242 - val_loss: 0.5070 - val_accuracy: 0.8610\n",
      "Epoch 1443/1500\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1954 - accuracy: 0.9299 - val_loss: 0.4010 - val_accuracy: 0.8731\n",
      "Epoch 1444/1500\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1883 - accuracy: 0.9348 - val_loss: 0.3696 - val_accuracy: 0.8912\n",
      "Epoch 1445/1500\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2122 - accuracy: 0.9299 - val_loss: 0.3668 - val_accuracy: 0.8943\n",
      "Epoch 1446/1500\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 0.30269\n",
      "39/38 - 9s - loss: 0.2152 - accuracy: 0.9275 - val_loss: 0.3506 - val_accuracy: 0.8912\n",
      "Epoch 1447/1500\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2021 - accuracy: 0.9250 - val_loss: 0.4401 - val_accuracy: 0.8580\n",
      "Epoch 1448/1500\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2110 - accuracy: 0.9324 - val_loss: 0.3564 - val_accuracy: 0.8882\n",
      "Epoch 1449/1500\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2027 - accuracy: 0.9315 - val_loss: 0.4133 - val_accuracy: 0.8943\n",
      "Epoch 1450/1500\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1643 - accuracy: 0.9372 - val_loss: 0.3694 - val_accuracy: 0.8882\n",
      "Epoch 1451/1500\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1876 - accuracy: 0.9267 - val_loss: 0.3880 - val_accuracy: 0.8852\n",
      "Epoch 1452/1500\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1839 - accuracy: 0.9381 - val_loss: 0.3801 - val_accuracy: 0.8882\n",
      "Epoch 1453/1500\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2022 - accuracy: 0.9299 - val_loss: 0.4091 - val_accuracy: 0.8640\n",
      "Epoch 1454/1500\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1994 - accuracy: 0.9283 - val_loss: 0.4986 - val_accuracy: 0.8610\n",
      "Epoch 1455/1500\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1938 - accuracy: 0.9462 - val_loss: 0.4488 - val_accuracy: 0.8671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1456/1500\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2270 - accuracy: 0.9250 - val_loss: 0.4052 - val_accuracy: 0.8761\n",
      "Epoch 1457/1500\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2010 - accuracy: 0.9283 - val_loss: 0.3524 - val_accuracy: 0.8882\n",
      "Epoch 1458/1500\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1877 - accuracy: 0.9389 - val_loss: 0.3753 - val_accuracy: 0.8882\n",
      "Epoch 1459/1500\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 0.30269\n",
      "39/38 - 9s - loss: 0.1847 - accuracy: 0.9381 - val_loss: 0.3592 - val_accuracy: 0.8882\n",
      "Epoch 1460/1500\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2339 - accuracy: 0.9258 - val_loss: 0.3427 - val_accuracy: 0.9003\n",
      "Epoch 1461/1500\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1771 - accuracy: 0.9307 - val_loss: 0.3797 - val_accuracy: 0.8852\n",
      "Epoch 1462/1500\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 0.30269\n",
      "39/38 - 9s - loss: 0.1891 - accuracy: 0.9372 - val_loss: 0.3563 - val_accuracy: 0.8852\n",
      "Epoch 1463/1500\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 0.30269\n",
      "39/38 - 9s - loss: 0.2282 - accuracy: 0.9177 - val_loss: 0.3558 - val_accuracy: 0.8912\n",
      "Epoch 1464/1500\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 0.30269\n",
      "39/38 - 9s - loss: 0.2245 - accuracy: 0.9250 - val_loss: 0.3668 - val_accuracy: 0.8852\n",
      "Epoch 1465/1500\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 0.30269\n",
      "39/38 - 9s - loss: 0.1945 - accuracy: 0.9283 - val_loss: 0.4652 - val_accuracy: 0.8640\n",
      "Epoch 1466/1500\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1920 - accuracy: 0.9242 - val_loss: 0.4065 - val_accuracy: 0.8792\n",
      "Epoch 1467/1500\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1975 - accuracy: 0.9307 - val_loss: 0.4349 - val_accuracy: 0.8701\n",
      "Epoch 1468/1500\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1739 - accuracy: 0.9372 - val_loss: 0.4173 - val_accuracy: 0.8761\n",
      "Epoch 1469/1500\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2046 - accuracy: 0.9299 - val_loss: 0.3844 - val_accuracy: 0.8792\n",
      "Epoch 1470/1500\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1802 - accuracy: 0.9340 - val_loss: 0.4165 - val_accuracy: 0.8761\n",
      "Epoch 1471/1500\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1791 - accuracy: 0.9348 - val_loss: 0.4143 - val_accuracy: 0.8792\n",
      "Epoch 1472/1500\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2175 - accuracy: 0.9226 - val_loss: 0.3817 - val_accuracy: 0.8852\n",
      "Epoch 1473/1500\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2141 - accuracy: 0.9242 - val_loss: 0.4319 - val_accuracy: 0.8852\n",
      "Epoch 1474/1500\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1784 - accuracy: 0.9348 - val_loss: 0.4714 - val_accuracy: 0.8671\n",
      "Epoch 1475/1500\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1909 - accuracy: 0.9291 - val_loss: 0.5195 - val_accuracy: 0.8489\n",
      "Epoch 1476/1500\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2012 - accuracy: 0.9275 - val_loss: 0.3629 - val_accuracy: 0.8943\n",
      "Epoch 1477/1500\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1875 - accuracy: 0.9267 - val_loss: 0.3428 - val_accuracy: 0.8852\n",
      "Epoch 1478/1500\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1572 - accuracy: 0.9462 - val_loss: 0.3410 - val_accuracy: 0.9003\n",
      "Epoch 1479/1500\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2336 - accuracy: 0.9193 - val_loss: 0.3628 - val_accuracy: 0.8852\n",
      "Epoch 1480/1500\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2432 - accuracy: 0.9161 - val_loss: 0.3362 - val_accuracy: 0.8912\n",
      "Epoch 1481/1500\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2430 - accuracy: 0.9242 - val_loss: 0.3525 - val_accuracy: 0.8701\n",
      "Epoch 1482/1500\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2116 - accuracy: 0.9283 - val_loss: 0.3454 - val_accuracy: 0.8882\n",
      "Epoch 1483/1500\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2025 - accuracy: 0.9234 - val_loss: 0.3768 - val_accuracy: 0.8882\n",
      "Epoch 1484/1500\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1787 - accuracy: 0.9381 - val_loss: 0.3529 - val_accuracy: 0.8882\n",
      "Epoch 1485/1500\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1996 - accuracy: 0.9315 - val_loss: 0.4179 - val_accuracy: 0.8792\n",
      "Epoch 1486/1500\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1709 - accuracy: 0.9307 - val_loss: 0.3385 - val_accuracy: 0.9033\n",
      "Epoch 1487/1500\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2002 - accuracy: 0.9405 - val_loss: 0.3657 - val_accuracy: 0.8882\n",
      "Epoch 1488/1500\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1938 - accuracy: 0.9324 - val_loss: 0.4084 - val_accuracy: 0.8731\n",
      "Epoch 1489/1500\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1903 - accuracy: 0.9283 - val_loss: 0.4194 - val_accuracy: 0.8701\n",
      "Epoch 1490/1500\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1607 - accuracy: 0.9364 - val_loss: 0.4762 - val_accuracy: 0.8640\n",
      "Epoch 1491/1500\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2022 - accuracy: 0.9332 - val_loss: 0.3801 - val_accuracy: 0.8792\n",
      "Epoch 1492/1500\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1913 - accuracy: 0.9291 - val_loss: 0.4114 - val_accuracy: 0.8701\n",
      "Epoch 1493/1500\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1582 - accuracy: 0.9454 - val_loss: 0.3817 - val_accuracy: 0.8852\n",
      "Epoch 1494/1500\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2101 - accuracy: 0.9348 - val_loss: 0.4419 - val_accuracy: 0.8761\n",
      "Epoch 1495/1500\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2082 - accuracy: 0.9275 - val_loss: 0.3821 - val_accuracy: 0.8731\n",
      "Epoch 1496/1500\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1827 - accuracy: 0.9356 - val_loss: 0.4052 - val_accuracy: 0.8882\n",
      "Epoch 1497/1500\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2046 - accuracy: 0.9193 - val_loss: 0.4040 - val_accuracy: 0.8731\n",
      "Epoch 1498/1500\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.1911 - accuracy: 0.9307 - val_loss: 0.3621 - val_accuracy: 0.9033\n",
      "Epoch 1499/1500\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2220 - accuracy: 0.9144 - val_loss: 0.3562 - val_accuracy: 0.8912\n",
      "Epoch 1500/1500\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 0.30269\n",
      "39/38 - 8s - loss: 0.2125 - accuracy: 0.9283 - val_loss: 0.3587 - val_accuracy: 0.8973\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='piece_model1.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "piece_hist = piece_model.fit_generator(generator=piece_train_iter, \n",
    "                          steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                          validation_data=piece_valid_iter, \n",
    "                          validation_steps=STEP_SIZE_VALID,\n",
    "                          epochs=1500, \n",
    "                          callbacks=[checkpointer], \n",
    "                          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.8835821\n"
     ]
    }
   ],
   "source": [
    "#Reset test iterator\n",
    "STEP_SIZE_TEST = piece_test_iter.n/piece_test_iter.batch_size\n",
    "piece_test_iter.reset()\n",
    "# load the weights that yielded the best validation accuracy\n",
    "piece_model.load_weights('piece_model1.weights.best.hdf5')\n",
    "# evaluate and print test accuracy\n",
    "score = piece_model.evaluate_generator(generator=piece_test_iter,steps=STEP_SIZE_TEST)\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 1s 68ms/step\n"
     ]
    }
   ],
   "source": [
    "piece_test_iter.reset()\n",
    "piece_pred = piece_model.predict_generator(piece_test_iter,steps=STEP_SIZE_TEST,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 4 4 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 1 0 0 2 0 0 0 0\n",
      " 0 0 0 0 0 0 3 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 1 1 4 1 4 1 4 1 1\n",
      " 1 1 1 1 1 1 2 2 2 2 2 2 2 5 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 0 2 4 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3\n",
      " 3 3 3 2 3 0 3 3 3 3 3 2 0 0 3 3 2 3 2 0 3 3 0 5 3 3 3 0 3 0 3 2 3 3 3 3 3\n",
      " 3 3 3 2 0 3 3 4 4 4 4 4 4 4 4 4 5 4 4 4 4 1 4 4 1 5 4 4 4 4 4 4 4 1 4 4 4\n",
      " 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 0 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 1 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6]\n"
     ]
    }
   ],
   "source": [
    "predicted_class_indices=np.argmax(piece_pred,axis=1)\n",
    "print(predicted_class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (piece_test_iter.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "truth =  [labels[k] for k in piece_test_iter.classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[labels[k] for k in piece_test_iter.classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bishop/1538777310.1624303.jpg',\n",
       " 'bishop/1538777556.4695792.jpg',\n",
       " 'bishop/1538777675.2653856.jpg',\n",
       " 'bishop/1538777891.4931645.jpg',\n",
       " 'bishop/1538778091.9695802.jpg',\n",
       " 'bishop/1538778093.2397656.jpg',\n",
       " 'bishop/1538778214.3746974.jpg',\n",
       " 'bishop/1538778215.8878355.jpg',\n",
       " 'bishop/1538778410.6081142.jpg',\n",
       " 'bishop/1538778427.8668559.jpg',\n",
       " 'bishop/1538778537.1458335.jpg',\n",
       " 'bishop/1538778538.3017461.jpg',\n",
       " 'bishop/1538778643.4925923.jpg',\n",
       " 'bishop/1538778653.1963263.jpg',\n",
       " 'bishop/1538778656.6147914.jpg',\n",
       " 'bishop/1538778729.1405196.jpg',\n",
       " 'bishop/1538778752.467536.jpg',\n",
       " 'bishop/1538778832.1937056.jpg',\n",
       " 'bishop/1538778833.610855.jpg',\n",
       " 'bishop/1538778878.3252861.jpg',\n",
       " 'bishop/1538778958.324379.jpg',\n",
       " 'bishop/1538779144.9958382.jpg',\n",
       " 'bishop/1538779155.2682924.jpg',\n",
       " 'bishop/1538779417.7487714.jpg',\n",
       " 'bishop/1538779519.5865016.jpg',\n",
       " 'bishop/1539017656.2202125_22.jpg',\n",
       " 'bishop/1539017656.6888525_5.jpg',\n",
       " 'bishop/1539023140.397658.jpg',\n",
       " 'bishop/1539023363.830272.jpg',\n",
       " 'bishop/1539023732.561326.jpg',\n",
       " 'bishop/1539024039.3749897.jpg',\n",
       " 'bishop/1539024424.9014952.jpg',\n",
       " 'bishop/1539024593.521976.jpg',\n",
       " 'bishop/1539024725.794323.jpg',\n",
       " 'bishop/1539024760.3120384.jpg',\n",
       " 'bishop/1539025334.2429533.jpg',\n",
       " 'bishop/1539025355.3753638.jpg',\n",
       " 'bishop/1539025373.0113628.jpg',\n",
       " 'bishop/1539025415.635129.jpg',\n",
       " 'bishop/1539025434.58347.jpg',\n",
       " 'bishop/1539025474.8601475.jpg',\n",
       " 'bishop/1539025486.6533895.jpg',\n",
       " 'bishop/1539025736.4874806.jpg',\n",
       " 'bishop/1539025801.510389.jpg',\n",
       " 'bishop/1539025832.837544.jpg',\n",
       " 'bishop/1539025959.458598.jpg',\n",
       " 'bishop/1539026035.275597.jpg',\n",
       " 'king/1538774883.9670901.jpg',\n",
       " 'king/1538775155.1271548.jpg',\n",
       " 'king/1538775504.1042295.jpg',\n",
       " 'king/1538776272.1332486.jpg',\n",
       " 'king/1538776522.496111.jpg',\n",
       " 'king/1538777095.0775242.jpg',\n",
       " 'king/1538777418.4191957.jpg',\n",
       " 'king/1538777707.301926.jpg',\n",
       " 'king/1538778011.0228102.jpg',\n",
       " 'king/1538778406.86247.jpg',\n",
       " 'king/1538778972.6889744.jpg',\n",
       " 'king/1538779020.8452945.jpg',\n",
       " 'king/1538779136.6977031.jpg',\n",
       " 'king/1538779354.1704876.jpg',\n",
       " 'king/1538779455.0068257.jpg',\n",
       " 'king/1538779987.557536.jpg',\n",
       " 'king/1538780110.995072.jpg',\n",
       " 'king/1539017656.0015128_29.jpg',\n",
       " 'king/1539017656.4857748_24.jpg',\n",
       " 'king/1539017656.5013957_31.jpg',\n",
       " 'king/1539017657.7939315_15.jpg',\n",
       " 'king/1539017657.8720484_58.jpg',\n",
       " 'king/1539017658.0594935_23.jpg',\n",
       " 'king/1539022987.3141916.jpg',\n",
       " 'king/1539023157.5381975.jpg',\n",
       " 'king/1539023185.6079886.jpg',\n",
       " 'king/1539023430.39102.jpg',\n",
       " 'king/1539023823.1830983.jpg',\n",
       " 'king/1539024597.23388.jpg',\n",
       " 'king/1539024744.5240357.jpg',\n",
       " 'king/1539025742.555007.jpg',\n",
       " 'king/1539026833.4767778.jpg',\n",
       " 'king/1539027947.6755114.jpg',\n",
       " 'knight/1538775492.0805464.jpg',\n",
       " 'knight/1538776219.5658782.jpg',\n",
       " 'knight/1538777112.8554816.jpg',\n",
       " 'knight/1538777213.003978.jpg',\n",
       " 'knight/1538778117.6826968.jpg',\n",
       " 'knight/1538778197.602519.jpg',\n",
       " 'knight/1538778212.992307.jpg',\n",
       " 'knight/1538778356.0347438.jpg',\n",
       " 'knight/1538778478.4419494.jpg',\n",
       " 'knight/1538778511.7367067.jpg',\n",
       " 'knight/1538778628.9804418.jpg',\n",
       " 'knight/1538778670.1169665.jpg',\n",
       " 'knight/1538778725.7980158.jpg',\n",
       " 'knight/1538778798.7536736.jpg',\n",
       " 'knight/1538779127.0457888.jpg',\n",
       " 'knight/1538779157.0249271.jpg',\n",
       " 'knight/1538779329.0645812.jpg',\n",
       " 'knight/1538779362.8107798.jpg',\n",
       " 'knight/1538779439.9036374.jpg',\n",
       " 'knight/1538779459.4036417.jpg',\n",
       " 'knight/1538779559.3820636.jpg',\n",
       " 'knight/1538779701.3311424.jpg',\n",
       " 'knight/1538779739.058273.jpg',\n",
       " 'knight/1538779919.6389072.jpg',\n",
       " 'knight/1539017656.048383_53.jpg',\n",
       " 'knight/1539017656.2045898_14.jpg',\n",
       " 'knight/1539017656.532639_55.jpg',\n",
       " 'knight/1539017656.6732304_1.jpg',\n",
       " 'knight/1539017656.7825797_62.jpg',\n",
       " 'knight/1539017657.0012786_23.jpg',\n",
       " 'knight/1539017657.0643072_43.jpg',\n",
       " 'knight/1539017657.0682952_45.jpg',\n",
       " 'knight/1539017657.2756457_13.jpg',\n",
       " 'knight/1539017657.2756457_18.jpg',\n",
       " 'knight/1539017657.291264_26.jpg',\n",
       " 'knight/1539017657.5752366_26.jpg',\n",
       " 'knight/1539017657.5752366_28.jpg',\n",
       " 'knight/1539017657.7939315_13.jpg',\n",
       " 'knight/1539017657.8251727_32.jpg',\n",
       " 'knight/1539017657.8251727_33.jpg',\n",
       " 'knight/1539017657.8407946_39.jpg',\n",
       " 'knight/1539017658.0438766_13.jpg',\n",
       " 'knight/1539017658.090735_39.jpg',\n",
       " 'knight/1539017658.2938118_13.jpg',\n",
       " 'knight/1539017658.3094337_22.jpg',\n",
       " 'knight/1539017658.3406758_39.jpg',\n",
       " 'knight/1539017658.3562984_41.jpg',\n",
       " 'knight/1539017658.5749974_27.jpg',\n",
       " 'knight/1539017658.5749974_28.jpg',\n",
       " 'knight/1539017658.809318_29.jpg',\n",
       " 'knight/1539023028.0835974.jpg',\n",
       " 'knight/1539023081.4048553.jpg',\n",
       " 'knight/1539023160.9380662.jpg',\n",
       " 'knight/1539023190.5311208.jpg',\n",
       " 'knight/1539023209.5838358.jpg',\n",
       " 'knight/1539023214.4817233.jpg',\n",
       " 'knight/1539023817.175047.jpg',\n",
       " 'knight/1539026872.2657654.jpg',\n",
       " 'knight/1539027337.492997.jpg',\n",
       " 'knight/1539028085.6649253.jpg',\n",
       " 'knight/1539028221.7775393.jpg',\n",
       " 'pawn/1538776446.2195349.jpg',\n",
       " 'pawn/1538778988.6561909.jpg',\n",
       " 'pawn/1538778996.169403.jpg',\n",
       " 'pawn/1538778999.7679274.jpg',\n",
       " 'pawn/1538779001.1267478.jpg',\n",
       " 'pawn/1538779112.965819.jpg',\n",
       " 'pawn/1538779143.7233405.jpg',\n",
       " 'pawn/1538779184.9161654.jpg',\n",
       " 'pawn/1538779186.269667.jpg',\n",
       " 'pawn/1539017659.0342584_2.jpg',\n",
       " 'pawn/1539017659.0342584_5.jpg',\n",
       " 'pawn/1539017659.273254_5.jpg',\n",
       " 'pawn/1539017659.320111_30.jpg',\n",
       " 'pawn/1539017659.9293435_51.jpg',\n",
       " 'pawn/1539017660.148043_49.jpg',\n",
       " 'pawn/1539017660.3198786_9.jpg',\n",
       " 'pawn/1539017660.3511202_27.jpg',\n",
       " 'pawn/1539017660.5854392_17.jpg',\n",
       " 'pawn/1539017660.804138_1.jpg',\n",
       " 'pawn/1539017660.9134872_58.jpg',\n",
       " 'pawn/1539017661.6291142_49.jpg',\n",
       " 'pawn/1539017661.8009472_9.jpg',\n",
       " 'pawn/1539017661.8321903_20.jpg',\n",
       " 'pawn/1539017662.0429847_1.jpg',\n",
       " 'pawn/1539017662.058607_12.jpg',\n",
       " 'pawn/1539017664.3632076_13.jpg',\n",
       " 'pawn/1539017664.4256973_53.jpg',\n",
       " 'pawn/1539022975.1625128.jpg',\n",
       " 'pawn/1539023017.1607347.jpg',\n",
       " 'pawn/1539023228.1371589.jpg',\n",
       " 'pawn/1539023368.838083.jpg',\n",
       " 'pawn/1539023428.3806713.jpg',\n",
       " 'pawn/1539023466.5992458.jpg',\n",
       " 'pawn/1539023766.4123085.jpg',\n",
       " 'pawn/1539024357.4180336.jpg',\n",
       " 'pawn/1539024430.756946.jpg',\n",
       " 'pawn/1539024541.9791014.jpg',\n",
       " 'pawn/1539024583.6953154.jpg',\n",
       " 'pawn/1539025313.0497591.jpg',\n",
       " 'pawn/1539025362.0886774.jpg',\n",
       " 'pawn/1539025391.7579608.jpg',\n",
       " 'pawn/1539025607.6992056.jpg',\n",
       " 'pawn/1539025673.4654312.jpg',\n",
       " 'pawn/1539025757.220354.jpg',\n",
       " 'pawn/1539025921.7744703.jpg',\n",
       " 'pawn/1539026038.6849546.jpg',\n",
       " 'pawn/1539026837.0018582.jpg',\n",
       " 'pawn/1539026875.391828.jpg',\n",
       " 'pawn/1539026923.465744.jpg',\n",
       " 'pawn/1539026980.8596466.jpg',\n",
       " 'pawn/1539027155.227193.jpg',\n",
       " 'queen/1538774886.4350865.jpg',\n",
       " 'queen/1538774903.7820106.jpg',\n",
       " 'queen/1538775023.0136087.jpg',\n",
       " 'queen/1538775387.0960882.jpg',\n",
       " 'queen/1538775521.7555773.jpg',\n",
       " 'queen/1538776153.1161563.jpg',\n",
       " 'queen/1538776241.9984956.jpg',\n",
       " 'queen/1538776666.2012632.jpg',\n",
       " 'queen/1538777233.7800925.jpg',\n",
       " 'queen/1538777276.2401419.jpg',\n",
       " 'queen/1538777951.5345926.jpg',\n",
       " 'queen/1538778211.4124253.jpg',\n",
       " 'queen/1538778645.0375922.jpg',\n",
       " 'queen/1538779089.730211.jpg',\n",
       " 'queen/1538779104.4953988.jpg',\n",
       " 'queen/1538779711.7096522.jpg',\n",
       " 'queen/1539017656.0171356_34.jpg',\n",
       " 'queen/1539017656.0171356_37.jpg',\n",
       " 'queen/1539017656.9544163_8.jpg',\n",
       " 'queen/1539017657.5596101_20.jpg',\n",
       " 'queen/1539017658.0751133_28.jpg',\n",
       " 'queen/1539017658.809318_21.jpg',\n",
       " 'queen/1539023282.7923298.jpg',\n",
       " 'queen/1539023360.027172.jpg',\n",
       " 'queen/1539024135.86198.jpg',\n",
       " 'queen/1539024330.4476821.jpg',\n",
       " 'queen/1539025459.6575894.jpg',\n",
       " 'queen/1539025544.3600874.jpg',\n",
       " 'queen/1539026802.3034804.jpg',\n",
       " 'queen/1539027143.1900225.jpg',\n",
       " 'queen/1539027242.9633646.jpg',\n",
       " 'queen/1539028286.9873247.jpg',\n",
       " 'rook/1538776436.5497537.jpg',\n",
       " 'rook/1538778101.779042.jpg',\n",
       " 'rook/1538778196.1506379.jpg',\n",
       " 'rook/1538778246.5786252.jpg',\n",
       " 'rook/1538778420.713808.jpg',\n",
       " 'rook/1538778508.6299925.jpg',\n",
       " 'rook/1538778565.9025004.jpg',\n",
       " 'rook/1538778590.4654863.jpg',\n",
       " 'rook/1538778808.3629324.jpg',\n",
       " 'rook/1538778830.4086065.jpg',\n",
       " 'rook/1538778848.9860363.jpg',\n",
       " 'rook/1538778901.7553732.jpg',\n",
       " 'rook/1538778949.280067.jpg',\n",
       " 'rook/1538779051.3667924.jpg',\n",
       " 'rook/1538779142.3172784.jpg',\n",
       " 'rook/1538779147.5203454.jpg',\n",
       " 'rook/1538779183.5230417.jpg',\n",
       " 'rook/1538779355.8192723.jpg',\n",
       " 'rook/1538779366.148124.jpg',\n",
       " 'rook/1538779421.4983451.jpg',\n",
       " 'rook/1538779431.4099524.jpg',\n",
       " 'rook/1538779446.819697.jpg',\n",
       " 'rook/1538779524.2512803.jpg',\n",
       " 'rook/1538779688.6545618.jpg',\n",
       " 'rook/1538779718.4556568.jpg',\n",
       " 'rook/1538780007.7006266.jpg',\n",
       " 'rook/1538780044.5343263.jpg',\n",
       " 'rook/1538780084.217775.jpg',\n",
       " 'rook/1538780193.7644806.jpg',\n",
       " 'rook/1539017656.1889713_1.jpg',\n",
       " 'rook/1539017656.2983181_57.jpg',\n",
       " 'rook/1539017656.4545326_0.jpg',\n",
       " 'rook/1539017656.5482595_56.jpg',\n",
       " 'rook/1539017656.6732304_0.jpg',\n",
       " 'rook/1539017656.7825797_56.jpg',\n",
       " 'rook/1539017656.7825797_63.jpg',\n",
       " 'rook/1539017657.0325217_32.jpg',\n",
       " 'rook/1539017657.1038074_61.jpg',\n",
       " 'rook/1539017657.3537493_58.jpg',\n",
       " 'rook/1539017657.3537493_60.jpg',\n",
       " 'rook/1539017657.6064732_44.jpg',\n",
       " 'rook/1539017657.6220949_52.jpg',\n",
       " 'rook/1539017657.8720484_52.jpg',\n",
       " 'rook/1539017657.8876572_60.jpg',\n",
       " 'rook/1539017658.121979_59.jpg',\n",
       " 'rook/1539017658.1376002_63.jpg',\n",
       " 'rook/1539017658.2781916_2.jpg',\n",
       " 'rook/1539017658.387541_63.jpg',\n",
       " 'rook/1539017658.5906172_32.jpg',\n",
       " 'rook/1539023040.9695048.jpg',\n",
       " 'rook/1539025256.5206127.jpg',\n",
       " 'rook/1539025399.1357424.jpg',\n",
       " 'rook/1539025545.9052472.jpg',\n",
       " 'rook/1539025595.0919125.jpg',\n",
       " 'rook/1539025702.8716216.jpg',\n",
       " 'rook/1539025785.8488226.jpg',\n",
       " 'rook/1539026043.774472.jpg',\n",
       " 'rook/1539026846.958911.jpg',\n",
       " 'rook/1539026987.1509364.jpg',\n",
       " 'rook/1539027229.5479693.jpg',\n",
       " 'rook/1539027241.4846253.jpg',\n",
       " 'square/1538775356.6463027.jpg',\n",
       " 'square/1538775359.6263092.jpg',\n",
       " 'square/1538775377.837528.jpg',\n",
       " 'square/1538775380.3348455.jpg',\n",
       " 'square/1538775381.8908746.jpg',\n",
       " 'square/1538775383.3700583.jpg',\n",
       " 'square/1538775392.500712.jpg',\n",
       " 'square/1538775393.9093816.jpg',\n",
       " 'square/1538775395.379885.jpg',\n",
       " 'square/1538775397.2511947.jpg',\n",
       " 'square/1538775410.2587154.jpg',\n",
       " 'square/1538775411.657127.jpg',\n",
       " 'square/1538775413.002367.jpg',\n",
       " 'square/1538775414.3379526.jpg',\n",
       " 'square/1538775428.0259957.jpg',\n",
       " 'square/1538775429.9237733.jpg',\n",
       " 'square/1538775431.6329722.jpg',\n",
       " 'square/1538775433.0617445.jpg',\n",
       " 'square/1538775459.9201775.jpg',\n",
       " 'square/1538775463.3285487.jpg',\n",
       " 'square/1538775464.8436766.jpg',\n",
       " 'square/1538775472.3628545.jpg',\n",
       " 'square/1538775473.8751059.jpg',\n",
       " 'square/1538775477.4368722.jpg',\n",
       " 'square/1538775478.8303273.jpg',\n",
       " 'square/1538775482.290001.jpg',\n",
       " 'square/1538775490.1613836.jpg',\n",
       " 'square/1538775496.149376.jpg',\n",
       " 'square/1538775497.691954.jpg',\n",
       " 'square/1538775501.9954052.jpg',\n",
       " 'square/1538775507.261289.jpg',\n",
       " 'square/1538775508.9165573.jpg',\n",
       " 'square/1538775518.2826645.jpg',\n",
       " 'square/1538775525.8732312.jpg',\n",
       " 'square/1538775527.3804672.jpg',\n",
       " 'square/1538775528.933519.jpg',\n",
       " 'square/1538775534.8773243.jpg',\n",
       " 'square/1538775541.8031077.jpg',\n",
       " 'square/1538775546.3549168.jpg',\n",
       " 'square/1538775553.2582464.jpg',\n",
       " 'square/1538775554.7329156.jpg',\n",
       " 'square/1538775556.1137612.jpg',\n",
       " 'square/1538775557.4647563.jpg',\n",
       " 'square/1538775563.8003812.jpg',\n",
       " 'square/1538775575.8332133.jpg',\n",
       " 'square/1538775578.4874399.jpg',\n",
       " 'square/1538775583.9372883.jpg',\n",
       " 'square/1538775585.983797.jpg',\n",
       " 'square/1538775589.5684185.jpg',\n",
       " 'square/1538775593.2157.jpg']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piece_test_iter.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=piece_test_iter.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Truth\": truth,\n",
    "                      \"Predictions\":predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bishop/1538777310.1624303.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bishop/1538777556.4695792.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bishop/1538777675.2653856.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bishop/1538777891.4931645.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bishop/1538778091.9695802.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bishop/1538778093.2397656.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bishop/1538778214.3746974.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bishop/1538778215.8878355.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bishop/1538778410.6081142.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bishop/1538778427.8668559.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bishop/1538778537.1458335.jpg</td>\n",
       "      <td>queen</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bishop/1538778538.3017461.jpg</td>\n",
       "      <td>queen</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bishop/1538778643.4925923.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bishop/1538778653.1963263.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bishop/1538778656.6147914.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bishop/1538778729.1405196.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bishop/1538778752.467536.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bishop/1538778832.1937056.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bishop/1538778833.610855.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bishop/1538778878.3252861.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bishop/1538778958.324379.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bishop/1538779144.9958382.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bishop/1538779155.2682924.jpg</td>\n",
       "      <td>knight</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bishop/1538779417.7487714.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bishop/1538779519.5865016.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bishop/1539017656.2202125_22.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bishop/1539017656.6888525_5.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bishop/1539023140.397658.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bishop/1539023363.830272.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bishop/1539023732.561326.jpg</td>\n",
       "      <td>king</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>square/1538775464.8436766.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>square/1538775472.3628545.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>square/1538775473.8751059.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>square/1538775477.4368722.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>square/1538775478.8303273.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>square/1538775482.290001.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>square/1538775490.1613836.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>square/1538775496.149376.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>square/1538775497.691954.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>square/1538775501.9954052.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>square/1538775507.261289.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>square/1538775508.9165573.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>square/1538775518.2826645.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>square/1538775525.8732312.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>square/1538775527.3804672.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>square/1538775528.933519.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>square/1538775534.8773243.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>square/1538775541.8031077.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>square/1538775546.3549168.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>square/1538775553.2582464.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>square/1538775554.7329156.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>square/1538775556.1137612.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>square/1538775557.4647563.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>square/1538775563.8003812.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>square/1538775575.8332133.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>square/1538775578.4874399.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>square/1538775583.9372883.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>square/1538775585.983797.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>square/1538775589.5684185.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>square/1538775593.2157.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Filename Predictions   Truth\n",
       "0       bishop/1538777310.1624303.jpg      bishop  bishop\n",
       "1       bishop/1538777556.4695792.jpg      bishop  bishop\n",
       "2       bishop/1538777675.2653856.jpg      bishop  bishop\n",
       "3       bishop/1538777891.4931645.jpg      bishop  bishop\n",
       "4       bishop/1538778091.9695802.jpg      bishop  bishop\n",
       "5       bishop/1538778093.2397656.jpg      bishop  bishop\n",
       "6       bishop/1538778214.3746974.jpg      bishop  bishop\n",
       "7       bishop/1538778215.8878355.jpg      bishop  bishop\n",
       "8       bishop/1538778410.6081142.jpg      bishop  bishop\n",
       "9       bishop/1538778427.8668559.jpg      bishop  bishop\n",
       "10      bishop/1538778537.1458335.jpg       queen  bishop\n",
       "11      bishop/1538778538.3017461.jpg       queen  bishop\n",
       "12      bishop/1538778643.4925923.jpg      bishop  bishop\n",
       "13      bishop/1538778653.1963263.jpg      bishop  bishop\n",
       "14      bishop/1538778656.6147914.jpg      bishop  bishop\n",
       "15      bishop/1538778729.1405196.jpg      bishop  bishop\n",
       "16       bishop/1538778752.467536.jpg      bishop  bishop\n",
       "17      bishop/1538778832.1937056.jpg      bishop  bishop\n",
       "18       bishop/1538778833.610855.jpg      knight  bishop\n",
       "19      bishop/1538778878.3252861.jpg      bishop  bishop\n",
       "20       bishop/1538778958.324379.jpg      bishop  bishop\n",
       "21      bishop/1538779144.9958382.jpg      bishop  bishop\n",
       "22      bishop/1538779155.2682924.jpg      knight  bishop\n",
       "23      bishop/1538779417.7487714.jpg      bishop  bishop\n",
       "24      bishop/1538779519.5865016.jpg      bishop  bishop\n",
       "25   bishop/1539017656.2202125_22.jpg      bishop  bishop\n",
       "26    bishop/1539017656.6888525_5.jpg      bishop  bishop\n",
       "27       bishop/1539023140.397658.jpg      bishop  bishop\n",
       "28       bishop/1539023363.830272.jpg      bishop  bishop\n",
       "29       bishop/1539023732.561326.jpg        king  bishop\n",
       "..                                ...         ...     ...\n",
       "305     square/1538775464.8436766.jpg      square  square\n",
       "306     square/1538775472.3628545.jpg      square  square\n",
       "307     square/1538775473.8751059.jpg      square  square\n",
       "308     square/1538775477.4368722.jpg      square  square\n",
       "309     square/1538775478.8303273.jpg      square  square\n",
       "310      square/1538775482.290001.jpg      square  square\n",
       "311     square/1538775490.1613836.jpg      square  square\n",
       "312      square/1538775496.149376.jpg      square  square\n",
       "313      square/1538775497.691954.jpg      square  square\n",
       "314     square/1538775501.9954052.jpg      square  square\n",
       "315      square/1538775507.261289.jpg      square  square\n",
       "316     square/1538775508.9165573.jpg      square  square\n",
       "317     square/1538775518.2826645.jpg      square  square\n",
       "318     square/1538775525.8732312.jpg      square  square\n",
       "319     square/1538775527.3804672.jpg      square  square\n",
       "320      square/1538775528.933519.jpg      square  square\n",
       "321     square/1538775534.8773243.jpg      square  square\n",
       "322     square/1538775541.8031077.jpg      square  square\n",
       "323     square/1538775546.3549168.jpg      square  square\n",
       "324     square/1538775553.2582464.jpg      square  square\n",
       "325     square/1538775554.7329156.jpg      square  square\n",
       "326     square/1538775556.1137612.jpg      square  square\n",
       "327     square/1538775557.4647563.jpg      square  square\n",
       "328     square/1538775563.8003812.jpg      square  square\n",
       "329     square/1538775575.8332133.jpg      square  square\n",
       "330     square/1538775578.4874399.jpg      square  square\n",
       "331     square/1538775583.9372883.jpg      square  square\n",
       "332      square/1538775585.983797.jpg      square  square\n",
       "333     square/1538775589.5684185.jpg      square  square\n",
       "334        square/1538775593.2157.jpg      square  square\n",
       "\n",
       "[335 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  1  3  1  2  0  0]\n",
      " [ 0 29  0  0  4  0  0]\n",
      " [ 2  0 56  0  2  1  0]\n",
      " [ 8  0  6 36  0  1  0]\n",
      " [ 0  3  0  0 27  2  0]\n",
      " [ 1  1  0  1  0 58  0]\n",
      " [ 0  0  0  0  0  0 50]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "import sklearn.metrics as metrics\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=piece_test_iter.classes, y_pred=predicted_class_indices)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'bishop', 1: 'king', 2: 'knight', 3: 'pawn', 4: 'queen', 5: 'rook', 6: 'square'}\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(piece_hist.history['accuracy'])\n",
    "plt.plot(piece_hist.history['val_accuracy'])\n",
    "plt.title('Piece Model accuracy w/ Augmentation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(piece_hist.history['loss'])\n",
    "plt.plot(piece_hist.history['val_loss'])\n",
    "plt.title('Piece Model Log Loss w/ Augmentation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  1  3  1  2  0  0]\n",
      " [ 0 29  0  0  4  0  0]\n",
      " [ 2  0 56  0  2  1  0]\n",
      " [ 8  0  6 36  0  1  0]\n",
      " [ 0  3  0  0 27  2  0]\n",
      " [ 1  1  0  1  0 58  0]\n",
      " [ 0  0  0  0  0  0 50]]\n",
      "{0: 'bishop', 1: 'king', 2: 'knight', 3: 'pawn', 4: 'queen', 5: 'rook', 6: 'square'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmatrix = confusion_matrix(piece_test_iter.classes,predicted_class_indices)\n",
    "print(cmatrix)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(cmatrix, annot=True, xticklabels=['bishop','king','pawn','knight','queen','rook','square'],yticklabels=['bishop','king','pawn','knight','queen','rook','square'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34240595798594936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss_result = log_loss(piece_test_iter.classes,piece_pred)\n",
    "print(log_loss_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
